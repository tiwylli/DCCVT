{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:41:49.369498Z",
     "start_time": "2024-06-28T06:41:49.367052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gradientUtils as gu\n",
    "# Define a threshold for early stopping\n",
    "threshold = 1e-16\n"
   ],
   "id": "d9bd2cd3ba2cdece",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AUTO GRAD",
   "id": "f1d0d1304549ae68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:41:49.744717Z",
     "start_time": "2024-06-28T06:41:49.397666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def autograd():    \n",
    "    # Define the ground truth vertex\n",
    "    X_g = torch.tensor([1.0], requires_grad=False)\n",
    "    Y_g = torch.tensor([1.0], requires_grad=False)\n",
    "    # Initialize the sites\n",
    "    s_i = gu.Site(1.0, 1.0)\n",
    "    s_j = gu.Site(2.0, 2.0)\n",
    "    s_k = gu.Site(1.50, 2.750)\n",
    "    # Define an optimizer\n",
    "    optimizer = torch.optim.Adam([s_i.x, s_i.y, s_j.x, s_j.y, s_k.x, s_k.y], lr=0.01)\n",
    "    # Previous loss\n",
    "    prev_loss = float('inf')\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the vertex\n",
    "        X, Y = gu.compute_vertex(s_i, s_j, s_k)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = (X - X_g)**2 + (Y - Y_g)**2\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss.item()) < threshold:\n",
    "            print(f\"Stopping early at epoch {epoch} due to minimal loss change\")\n",
    "            gu.plot_and_save(epoch, s_i, s_j, s_k, X_g, Y_g,\"images/autograd/\")\n",
    "            break\n",
    "    \n",
    "        prev_loss = loss.item()\n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "            gu.plot_and_save(epoch, s_i, s_j, s_k, X_g, Y_g,\"images/autograd/\")\n",
    "    \n",
    "    # Print the final site coordinates\n",
    "    print(f'Final site coordinates:')\n",
    "    print(f's_i: ({s_i.x.item()}, {s_i.y.item()})')\n",
    "    print(f's_j: ({s_j.x.item()}, {s_j.y.item()})')\n",
    "    print(f's_k: ({s_k.x.item()}, {s_k.y.item()})')\n",
    "    \n",
    "    # Compute the final vertex\n",
    "    final_X, final_Y = gu.compute_vertex(s_i, s_j, s_k)\n",
    "    \n",
    "    # Print the final vertex coordinates\n",
    "    print(f'Final vertex coordinates: ({final_X.item()}, {final_Y.item()})')\n",
    "    return s_i, s_j, s_k\n",
    "\n",
    "s_i, s_j, s_k = autograd()"
   ],
   "id": "46b82dee2f33b8ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8612499237060547\n",
      "Epoch 100, Loss: 0.0021477900445461273\n",
      "Epoch 200, Loss: 5.845098627332845e-09\n",
      "Stopping early at epoch 279 due to minimal loss change\n",
      "Final site coordinates:\n",
      "s_i: (0.10342850536108017, 0.35356053709983826)\n",
      "s_j: (1.8965137004852295, 1.6465104818344116)\n",
      "s_k: (1.1535061597824097, 2.094599962234497)\n",
      "Final vertex coordinates: (0.9999982714653015, 0.9999975562095642)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom gradient",
   "id": "d79d867f3270cdc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:41:50.251755Z",
     "start_time": "2024-06-28T06:41:49.745661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def customgrad():    \n",
    "    # Define the ground truth vertex\n",
    "    X_g = torch.tensor([1.0], requires_grad=False)\n",
    "    Y_g = torch.tensor([1.0], requires_grad=False)\n",
    "    # Initialize the sites\n",
    "    s_i = gu.Site(1.0, 1.0)\n",
    "    s_j = gu.Site(2.0, 2.0)\n",
    "    s_k = gu.Site(1.50, 2.750)\n",
    "    # Define an optimizer\n",
    "    optimizer = torch.optim.Adam([s_i.x, s_i.y, s_j.x, s_j.y, s_k.x, s_k.y], lr=0.01)\n",
    "    prev_loss = float('inf')\n",
    "    \n",
    "    class CustomVertexFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x_i, y_i, x_j, y_j, x_k, y_k):\n",
    "            N_X = x_i**2 * (y_j - y_k) - x_j**2 * (y_i - y_k) + (x_k**2 + (y_i - y_k) * (y_j - y_k)) * (y_i - y_j)\n",
    "            N_Y = -(x_i**2 * (x_j - x_k) - x_i * (x_j**2 - x_k**2 + y_j**2 - y_k**2) + x_j**2 * x_k - x_j * (x_k**2 - y_i**2 + y_k**2) - x_k * (y_i**2 - y_j**2))\n",
    "            D = 2 * (x_i * (y_j - y_k) - x_j * (y_i - y_k) + x_k * (y_i - y_j))\n",
    "            \n",
    "            X = N_X / D\n",
    "            Y = N_Y / D\n",
    "            \n",
    "            ctx.save_for_backward(x_i, y_i, x_j, y_j, x_k, y_k, X, Y, N_X, N_Y, D)\n",
    "            return X, Y\n",
    "    \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output_X, grad_output_Y):\n",
    "            x_i, y_i, x_j, y_j, x_k, y_k, X, Y, N_X, N_Y, D = ctx.saved_tensors\n",
    "            \n",
    "            # Compute partial derivatives of X and Y wrt each site coordinate\n",
    "            dN_X_dx_i = 2 * x_i * (y_j - y_k)\n",
    "            dN_X_dx_j = -2 * x_j * (y_i - y_k)\n",
    "            dN_X_dx_k = 2 * x_k * (y_i - y_j)\n",
    "            \n",
    "            dN_Y_dx_i = -2 * x_i * (x_j - x_k) - x_j**2 + x_k**2 - y_j**2 + y_k**2\n",
    "            dN_Y_dx_j = -x_i**2 + x_k**2\n",
    "            dN_Y_dx_k = -x_i**2 + x_j**2\n",
    "            \n",
    "            dD_dx_i = 2 * (y_j - y_k)\n",
    "            dD_dx_j = 2 * (y_i - y_k)\n",
    "            dD_dx_k = 2 * (y_i - y_j)\n",
    "            \n",
    "            dX_dx_i = (dN_X_dx_i * D - N_X * dD_dx_i) / (D**2)\n",
    "            dX_dx_j = (dN_X_dx_j * D - N_X * dD_dx_j) / (D**2)\n",
    "            dX_dx_k = (dN_X_dx_k * D - N_X * dD_dx_k) / (D**2)\n",
    "            \n",
    "            dY_dx_i = (dN_Y_dx_i * D - N_Y * dD_dx_i) / (D**2)\n",
    "            dY_dx_j = (dN_Y_dx_j * D - N_Y * dD_dx_j) / (D**2)\n",
    "            dY_dx_k = (dN_Y_dx_k * D - N_Y * dD_dx_k) / (D**2)\n",
    "            \n",
    "            # Similarly for the y coordinates\n",
    "            dN_X_dy_i = 2 * (x_j - x_k) * x_i - x_j**2 + x_k**2 + y_j**2 - y_k**2\n",
    "            dN_X_dy_j = -2 * (x_i - x_k) * x_j + x_i**2 - x_k**2 - y_i**2 + y_k**2\n",
    "            dN_X_dy_k = 2 * (x_i - x_j) * x_k - x_i**2 + x_j**2 + y_i**2 - y_j**2\n",
    "            \n",
    "            dN_Y_dy_i = 0\n",
    "            dN_Y_dy_j = 0\n",
    "            dN_Y_dy_k = 0\n",
    "            \n",
    "            dD_dy_i = 2 * (x_j - x_k)\n",
    "            dD_dy_j = 2 * (x_i - x_k)\n",
    "            dD_dy_k = 2 * (x_i - x_j)\n",
    "            \n",
    "            dX_dy_i = (dN_X_dy_i * D - N_X * dD_dy_i) / (D**2)\n",
    "            dX_dy_j = (dN_X_dy_j * D - N_X * dD_dy_j) / (D**2)\n",
    "            dX_dy_k = (dN_X_dy_k * D - N_X * dD_dy_k) / (D**2)\n",
    "            \n",
    "            dY_dy_i = (dN_Y_dy_i * D - N_Y * dD_dy_i) / (D**2)\n",
    "            dY_dy_j = (dN_Y_dy_j * D - N_Y * dD_dy_j) / (D**2)\n",
    "            dY_dy_k = (dN_Y_dy_k * D - N_Y * dD_dy_k) / (D**2)\n",
    "            \n",
    "            grad_x_i = grad_output_X * dX_dx_i + grad_output_Y * dY_dx_i\n",
    "            grad_y_i = grad_output_X * dX_dy_i + grad_output_Y * dY_dy_i\n",
    "            grad_x_j = grad_output_X * dX_dx_j + grad_output_Y * dY_dx_j\n",
    "            grad_y_j = grad_output_X * dX_dy_j + grad_output_Y * dY_dy_j\n",
    "            grad_x_k = grad_output_X * dX_dx_k + grad_output_Y * dY_dx_k\n",
    "            grad_y_k = grad_output_X * dX_dy_k + grad_output_Y * dY_dy_k\n",
    "            \n",
    "            return grad_x_i, grad_y_i, grad_x_j, grad_y_j, grad_x_k, grad_y_k\n",
    "    \n",
    "    def compute_vertex(s_i, s_j, s_k):\n",
    "        return CustomVertexFunction.apply(s_i.x, s_i.y, s_j.x, s_j.y, s_k.x, s_k.y)\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the vertex\n",
    "        X, Y = compute_vertex(s_i, s_j, s_k)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = (X - X_g)**2 + (Y - Y_g)**2\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss.item()) < threshold:\n",
    "            print(f\"Stopping early at epoch {epoch} due to minimal loss change\")\n",
    "            gu.plot_and_save(epoch, s_i, s_j, s_k, X_g, Y_g,\"images/customgrad/\")\n",
    "            break\n",
    "    \n",
    "        prev_loss = loss.item()\n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "            gu.plot_and_save(epoch, s_i, s_j, s_k, X_g, Y_g,\"images/customgrad/\")\n",
    "    \n",
    "    # Print the final site coordinates\n",
    "    print(f'Final site coordinates:')\n",
    "    print(f's_i: ({s_i.x.item()}, {s_i.y.item()})')\n",
    "    print(f's_j: ({s_j.x.item()}, {s_j.y.item()})')\n",
    "    print(f's_k: ({s_k.x.item()}, {s_k.y.item()})')\n",
    "    \n",
    "    # Compute the final vertex\n",
    "    final_X, final_Y = compute_vertex(s_i, s_j, s_k)\n",
    "    \n",
    "    # Print the final vertex coordinates\n",
    "    print(f'Final vertex coordinates: ({final_X.item()}, {final_Y.item()})')\n",
    "    return s_i, s_j, s_k\n",
    "    \n",
    "s_i, s_j, s_k = customgrad()\n"
   ],
   "id": "8b10a84d21844100",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8612499237060547\n",
      "Epoch 100, Loss: 0.16594450175762177\n",
      "Epoch 200, Loss: 0.0030409719329327345\n",
      "Epoch 300, Loss: 4.169791282038204e-06\n",
      "Stopping early at epoch 381 due to minimal loss change\n",
      "Final site coordinates:\n",
      "s_i: (-0.010905168950557709, 1.029019832611084)\n",
      "s_j: (1.496026635169983, 1.881323218345642)\n",
      "s_k: (0.9567925333976746, 2.0104000568389893)\n",
      "Final vertex coordinates: (0.9999986290931702, 1.0000028610229492)\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
