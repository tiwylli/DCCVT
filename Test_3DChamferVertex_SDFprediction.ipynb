{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import trimesh\n",
    "from scipy.spatial import Delaunay, Voronoi\n",
    "\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.03\n",
    "lr_model = 0.0003\n",
    "iterations = 5000\n",
    "save_every = 100\n",
    "max_iter = 100\n",
    "#learning_rate = 0.03\n",
    "destination = \"./images/autograd/3D/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites loaded: torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "#currently sites are between -5 and 5 in all 3 dimensions\n",
    "# check if sites exists\n",
    "#num_centroids = 16*16*16\n",
    "num_centroids =16*16*16*8\n",
    "site_fp = f'sites_{num_centroids}_{input_dims}.pt'\n",
    "\n",
    "if os.path.exists(site_fp):\n",
    "    sites = torch.load(site_fp)\n",
    "    print(\"Sites loaded:\", sites.shape)\n",
    "else:\n",
    "    print(\"Creating new sites\")\n",
    "    sites = su.createCVTgrid(num_centroids=num_centroids, dimensionality=input_dims)\n",
    "    #save the initial sites torch tensor\n",
    "    torch.save(sites, site_fp)\n",
    "\n",
    "\n",
    "def plot_voronoi_3d(sites, xlim=5, ylim=5, zlim=5):\n",
    "    import numpy as np\n",
    "    import pyvoro\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "    # initialize random number generator\n",
    "    rng = np.random.default_rng(11)\n",
    "    # create a set of points in 3D\n",
    "    points = sites.detach().cpu().numpy()\n",
    "\n",
    "    # use pyvoro to compute the Voronoi tessellation\n",
    "    # the second argument gives the the axis limits in x,y and z direction\n",
    "    # in this case all between 0 and 1.\n",
    "    # the third argument gives \"dispersion = max distance between two points\n",
    "    # that might be adjacent\" (not sure how exactly this works)\n",
    "    voronoi = pyvoro.compute_voronoi(points,[[-xlim,xlim],[-ylim,ylim],[-zlim,zlim]],1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # for each Voronoi cell, plot all the faces of the corresponding polygon\n",
    "    for vnoicell in voronoi:\n",
    "        faces = []\n",
    "        # the vertices are the corner points of the Voronoi cell\n",
    "        vertices = np.array(vnoicell['vertices'])\n",
    "        # cycle through all faces of the polygon\n",
    "        for face in vnoicell['faces']:\n",
    "            faces.append(vertices[np.array(face['vertices'])])\n",
    "            \n",
    "        # join the faces into a 3D polygon\n",
    "        polygon = Poly3DCollection(faces, alpha=0.5, \n",
    "                                facecolors=rng.uniform(0,1,3),\n",
    "                                linewidths=0.5,edgecolors='black')\n",
    "        ax.add_collection3d(polygon)\n",
    "    \n",
    "    ax.set_xlim([-xlim,xlim])\n",
    "    ax.set_ylim([-ylim,ylim])\n",
    "    ax.set_zlim([-zlim,zlim])\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "#plot_voronoi_3d(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.init()\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target points: torch.Size([262144, 3])\n",
      "min_target tensor([-1.9998, -1.9769, -1.5499])\n",
      "max_target tensor([2.0000, 1.9810, 1.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<polyscope.point_cloud.PointCloud at 0x7c9623abe690>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the mesh\n",
    "mesh = [\"bunny\", \"Resources/stanford-bunny.obj\"]\n",
    "#mesh = [\"staryu\", \"Resources/staryu.obj\"]\n",
    "#mesh = [\"chair\", \"Resources/chair_low.obj\"]\n",
    "\n",
    "bunny = trimesh.load(mesh[1])\n",
    "\n",
    "# Get current bounding box\n",
    "min_bound = bunny.bounds[0]  # Min (x, y, z)\n",
    "max_bound = bunny.bounds[1]  # Max (x, y, z)\n",
    "\n",
    "# Compute scale factor\n",
    "current_size = max_bound - min_bound  # Size in each dimension\n",
    "target_size = 4  # Because we want [-5, 5], the total size is 10\n",
    "\n",
    "scale_factor = target_size / np.max(current_size)  # Scale based on the largest dimension\n",
    "\n",
    "# Compute new center after scaling\n",
    "new_vertices = bunny.vertices * scale_factor  # Scale the vertices\n",
    "new_min = np.min(new_vertices, axis=0)\n",
    "new_max = np.max(new_vertices, axis=0)\n",
    "new_center = (new_min + new_max) / 2  # New center after scaling\n",
    "\n",
    "# Compute translation to center the bunny at (0,0,0)\n",
    "translation = -new_center  # Move to the origin\n",
    "\n",
    "# Apply transformation (scaling + translation)\n",
    "bunny.vertices = new_vertices + translation\n",
    "\n",
    "#target_points = bunny.sample(16*16*16)\n",
    "target_points = bunny.sample(num_centroids*8)\n",
    "target_points = torch.tensor(target_points, device=device)\n",
    "print(\"Target points:\", target_points.shape)\n",
    "min_target = target_points.min(0)[0]\n",
    "max_target = target_points.max(0)[0]\n",
    "print(\"min_target\", min_target)\n",
    "print(\"max_target\", max_target)\n",
    "\n",
    "ps.register_point_cloud(\"Target_points\",target_points.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.show()\n",
    "\n",
    "def polyscope_sdf(model,i):\n",
    "    # Render the SDF as an implicit surface (zero-level set)\n",
    "    def model_sdf(pts):\n",
    "        pts_tensor = torch.tensor(pts, dtype=torch.float64, device=device)\n",
    "        sdf_values = model(pts_tensor)\n",
    "        sdf_values_np = sdf_values.detach().cpu().numpy().flatten()  # Convert to NumPy\n",
    "        \n",
    "        return sdf_values_np\n",
    "\n",
    "    ps.render_implicit_surface(f\"SDF Surface {i}\", model_sdf, mode=\"sphere_march\", enabled=True, subsample_factor=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "\n",
    "model = mlp.Decoder(multires=multires, input_dims=input_dims).to(device)\n",
    "radius = 1.0\n",
    "#model_path = 'models_resources/pretrained_sphere_small.pth'\n",
    "#model_path = f'models_resources/pretrained_sphere_{radius}.pth'\n",
    "model_path = 'models_resources/trained_bunny_GT.pth'\n",
    "\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model')\n",
    "else:\n",
    "    print(\"no model found, pretraining\")\n",
    "    #model.pre_train_sphere(int(radius*1000),radius)\n",
    "   # model.train_GT_mesh(3000, mesh, target_points)\n",
    "    #model.cleanup(10)\n",
    "    model.pre_train_target_pc(300, target_points)\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_sdf tensor(0.0527, grad_fn=<MinBackward1>)\n",
      "max_sdf tensor(3.5235, grad_fn=<MaxBackward1>)\n",
      "filtered_sites torch.Size([24, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create a camera view from parameters\n",
    "intrinsics = ps.CameraIntrinsics(fov_vertical_deg=60, aspect=2)\n",
    "extrinsics = ps.CameraExtrinsics(root=(20., 20., 20.), look_dir=(-1., -1.,-1.), up_dir=(0.,1.,0.))\n",
    "params = ps.CameraParameters(intrinsics, extrinsics)\n",
    "cam = ps.register_camera_view(\"cam\", params)\n",
    "#polyscope_sdf(model,1)\n",
    "\n",
    "sdf_values = model(sites)\n",
    "#print min max sdf\n",
    "print(\"min_sdf\",sdf_values.min())\n",
    "print(\"max_sdf\",sdf_values.max())\n",
    "\n",
    "sdf_values_np = sdf_values.detach().cpu().numpy().flatten()  # Convert to NumPy\n",
    "ps_cloud.add_scalar_quantity(\"SDF Values\", sdf_values_np, enabled=True)\n",
    "\n",
    "#filter sites with sdf lower than 0.1\n",
    "filtered_sites = sites[sdf_values[:, 0] < 0.1]\n",
    "print(\"filtered_sites\",filtered_sites.shape)\n",
    "ps.register_point_cloud(\"filtered_sites\",filtered_sites.detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as nps\n",
    "# import trimesh\n",
    "# import pyrender\n",
    "# from skimage.measure import marching_cubes\n",
    "\n",
    "\n",
    "# # Create a grid of points\n",
    "# grid_size = 64  # Resolution of the grid\n",
    "# x = np.linspace(-4, 4, grid_size)\n",
    "# y = np.linspace(-4, 4, grid_size)\n",
    "# z = np.linspace(-4, 4, grid_size)\n",
    "# X, Y, Z = np.meshgrid(x, y, z, indexing=\"ij\")\n",
    "\n",
    "# grid_x = X.flatten()\n",
    "# grid_y = Y.flatten()\n",
    "# grid_z = Z.flatten()\n",
    "\n",
    "# grid = np.stack([grid_x, grid_y, grid_z], axis=1)\n",
    "# grid = torch.tensor(grid, dtype=torch.float64, device=device)\n",
    "\n",
    "# # Compute SDF values on the grid\n",
    "# sdf_values = model(grid)\n",
    "\n",
    "# #sdf_values should be a 3d numpy array\n",
    "# sdf_values = sdf_values.detach().cpu().numpy().reshape(grid_size, grid_size, grid_size)\n",
    "\n",
    "# # Extract mesh using Marching Cubes\n",
    "# vertices, faces, _, _ = marching_cubes(sdf_values, level=0)\n",
    "\n",
    "# # Create a mesh\n",
    "# mesh = trimesh.Trimesh(vertices, faces)\n",
    "\n",
    "# # Render with Pyrender\n",
    "# scene = pyrender.Scene()\n",
    "# mesh_pyrender = pyrender.Mesh.from_trimesh(mesh)\n",
    "# scene.add(mesh_pyrender)\n",
    "\n",
    "# # Set up the renderer\n",
    "# viewer = pyrender.Viewer(scene, use_raymond_lighting=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_vectorized(sites, model):\n",
    "    sdf_values = model(sites)\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    # Compute Voronoi diagram\n",
    "    vor = Voronoi(sites_np)\n",
    "    \n",
    "    neighbors = torch.tensor(np.array(vor.ridge_points), device=device)\n",
    "    \n",
    "    # Extract the SDF values for each site in the pair\n",
    "    sdf_i = sdf_values[neighbors[:, 0]]  # First site in each pair\n",
    "    sdf_j = sdf_values[neighbors[:, 1]]  # Second site in each pair\n",
    "    # Find the indices where SDF values have opposing signs or one is zero\n",
    "    mask_zero_crossing_sites = (sdf_i * sdf_j <= 0).squeeze()\n",
    "    sites_to_upsample = torch.unique(neighbors[mask_zero_crossing_sites].view(-1))\n",
    "    \n",
    "    print(\"Sites to upsample \",sites_to_upsample.shape)\n",
    "    \n",
    "    tet_centroids = sites[sites_to_upsample]\n",
    "\n",
    "    # Tetrahedron relative positions (unit tetrahedron)\n",
    "    basic_tet_1 = torch.tensor([[1, 1, 1]], device=device, dtype=torch.float64)\n",
    "    basic_tet_1 = basic_tet_1.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_2 = torch.tensor([-1, -1, 1], device=device, dtype=torch.float64)    \n",
    "    basic_tet_2 = basic_tet_2.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_3 = torch.tensor([-1, 1, -1], device=device, dtype=torch.float64)    \n",
    "    basic_tet_3 = basic_tet_3.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_4 = torch.tensor([1, -1, -1], device=device, dtype=torch.float64)\n",
    "    basic_tet_4 = basic_tet_4.repeat(len(tet_centroids), 1)\n",
    "\n",
    "\n",
    "    #compute scale based on cell volume\n",
    "    centroids = torch.tensor(np.array([vor.vertices[vor.regions[vor.point_region[i]]].mean(axis=0) for i in range(len(sites_np))]), device=device)\n",
    "    #centroids = torch.tensor(np.array(centroids), device=sites.device, dtype=sites.dtype)\n",
    "    cells_vertices = [vor.vertices[vor.regions[vor.point_region[i]]] for i in range(len(sites_np))]\n",
    "\n",
    "    #compute the distance between each centroid  and each vertex in cells_vertices row\n",
    "    distances = []\n",
    "    for i in range(len(cells_vertices)):\n",
    "        min_dist = 100000000000\n",
    "        for j in range(len(cells_vertices[i])):\n",
    "            dist = torch.norm(centroids[i] - torch.tensor(cells_vertices[i][j], device=device), p=2)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        distances.append(min_dist)\n",
    "    distances = torch.tensor(distances, device=device)\n",
    " \n",
    "    \n",
    "    scale = distances[sites_to_upsample] / 2\n",
    "    \n",
    "    scale = scale.unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    new_sites = torch.cat((tet_centroids + basic_tet_1 * scale, tet_centroids + basic_tet_2 * scale, tet_centroids + basic_tet_3 * scale, tet_centroids + basic_tet_4 * scale), dim=0)\n",
    "\n",
    "    updated_sites = torch.cat((sites, new_sites), dim=0)\n",
    "\n",
    "    return updated_sites\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "edge_smoothing_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "zero_target_points_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.5, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_pc = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    lambda_laplace = lambda_weights[3]\n",
    "    lamda_chamfer = lambda_weights[4]\n",
    "    lamda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    lambda_target_points = lambda_weights[7]\n",
    "    \n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, model)\n",
    "        vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        #combine vertices and bisectors to one tensor for chamfer\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "\n",
    "\n",
    "        # Compute losses       \n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized(sites, model)\n",
    "        #min_distance_loss = min_distance_regularization_for_op_sites(edges,sites)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        #edge_smoothing_loss = compute_edge_smoothing_loss(edges, sites, model)\n",
    "        chamfer_loss = lf.chamfer_distance(target_points, points)\n",
    "        eikonal_loss = lf.eikonal(model, input_dimensions=input_dims)\n",
    "        pc_loss = lf.point_cloud_loss(target_points, model)\n",
    "        #domain_restriction_loss = lf.domain_restriction(target_points, model)\n",
    "        \n",
    "        sdf_values_target_points = model(target_points)[:,0]\n",
    "        zero_target_points_loss_L2 = torch.mean(sdf_values_target_points**2)\n",
    "        zero_target_points_loss_L1 = torch.mean(torch.abs(model(target_points)[:, 0]))\n",
    "        lambda_1, lambda_2 = 0 , 0.99  # Adjust weights as needed\n",
    "        zero_target_points_loss = lambda_1 * zero_target_points_loss_L1 + lambda_2 * zero_target_points_loss_L2\n",
    "\n",
    "               \n",
    "        # Track raw losses (unweighted)\n",
    "        cvt_loss_values.append(cvt_loss.item())\n",
    "        \n",
    "        #min_distance_loss_values.append(min_distance_loss.item())\n",
    "        #edge_smoothing_loss_values.append(edge_smoothing_loss.item())\n",
    "        chamfer_distance_loss_values.append(chamfer_loss.item())\n",
    "        eikonal_loss_values.append(eikonal_loss.item())\n",
    "        #domain_restriction_loss_values.append(domain_restriction_loss.item())\n",
    "        zero_target_points_loss_values.append(zero_target_points_loss.item())\n",
    "  \n",
    "        loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            lambda_pc * pc_loss +\n",
    "            #lambda_min_distance * min_distance_loss + \n",
    "            #lambda_laplace * edge_smoothing_loss +\n",
    "            lamda_chamfer * chamfer_loss +\n",
    "            lamda_eikonal * eikonal_loss +\n",
    "            #lambda_domain_restriction * domain_restriction_loss +\n",
    "            lambda_target_points * zero_target_points_loss\n",
    "        )\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        #print losses\n",
    "        print(f\"Chamfer loss: {lamda_chamfer * chamfer_loss.item()}\")\n",
    "        print(f\"Eikonal loss: {lamda_eikonal * eikonal_loss.item()}\")\n",
    "        print(f\"PC loss: {lambda_pc * pc_loss.item()}\")\n",
    "        print(f\"Zero target points loss: {lambda_target_points * zero_target_points_loss.item()}\")\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            if upsampled > 0:\n",
    "                print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            \n",
    "            #new_sites = su.upsampling_inside(best_sites, model)\n",
    "            #new_sites = su.adaptive_density_upsampling(best_sites, model)\n",
    "            \n",
    "            #sites = su.add_upsampled_sites(best_sites, new_sites)\n",
    "            \n",
    "            sites = upsampling_vectorized(sites, model)\n",
    "            \n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            #print(\"upsampled sites length: \",len(sites))\n",
    "            \n",
    "            #best_sites = sites.clone()\n",
    "            #best_sites.best_loss = best_loss\n",
    "            \n",
    "            optimizer = torch.optim.Adam([{'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "                                          {'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        \n",
    "        epoch += 1           \n",
    "        \n",
    "    return best_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvt_loss:  tensor(0.0724, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/tinycuda/lib/python3.13/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.078722975101485\n",
      "Chamfer loss: 4.084212115192147\n",
      "Eikonal loss: 0.002784846065759128\n",
      "PC loss: 1.1038923258797975\n",
      "Zero target points loss: 53.88710977804986\n",
      "Epoch 0: loss = 59.078722975101485\n",
      "Best Epoch 0: Best loss = 59.078722975101485\n",
      "cvt_loss:  tensor(0.0377, grad_fn=<MeanBackward0>)\n",
      "Epoch 1: loss = 41.34165839768739\n",
      "Chamfer loss: 2.7004202638455945\n",
      "Eikonal loss: 0.004663689666368282\n",
      "PC loss: 0.9331497975722588\n",
      "Zero target points loss: 37.70304724665805\n",
      "cvt_loss:  tensor(0.0244, grad_fn=<MeanBackward0>)\n",
      "Epoch 2: loss = 27.66067890228898\n",
      "Chamfer loss: 1.7143243367831917\n",
      "Eikonal loss: 0.00895379853886849\n",
      "PC loss: 0.7816349449776848\n",
      "Zero target points loss: 25.1555220286745\n",
      "cvt_loss:  tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "Epoch 3: loss = 18.69477789203793\n",
      "Chamfer loss: 1.2377687647345463\n",
      "Eikonal loss: 0.0148964596378389\n",
      "PC loss: 0.6500898312844671\n",
      "Zero target points loss: 16.79183907284591\n",
      "cvt_loss:  tensor(0.0145, grad_fn=<MeanBackward0>)\n",
      "Epoch 4: loss = 13.862941858583492\n",
      "Chamfer loss: 0.9514803121205581\n",
      "Eikonal loss: 0.020109546584041894\n",
      "PC loss: 0.5405975276458219\n",
      "Zero target points loss: 12.350609623286893\n",
      "cvt_loss:  tensor(0.0159, grad_fn=<MeanBackward0>)\n",
      "Epoch 5: loss = 11.632918917969462\n",
      "Chamfer loss: 0.8564341868253286\n",
      "Eikonal loss: 0.021502753021743305\n",
      "PC loss: 0.4523242871260613\n",
      "Zero target points loss: 10.302499050411315\n",
      "cvt_loss:  tensor(0.0166, grad_fn=<MeanBackward0>)\n",
      "Epoch 6: loss = 10.182370242706376\n",
      "Chamfer loss: 0.7963694138052627\n",
      "Eikonal loss: 0.019170668814540187\n",
      "PC loss: 0.38635866480949166\n",
      "Zero target points loss: 8.980305496501254\n",
      "cvt_loss:  tensor(0.0138, grad_fn=<MeanBackward0>)\n",
      "Epoch 7: loss = 8.849475866460764\n",
      "Chamfer loss: 0.7620101364141862\n",
      "Eikonal loss: 0.014597732663253185\n",
      "PC loss: 0.337348882181748\n",
      "Zero target points loss: 7.735381108301514\n",
      "cvt_loss:  tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "Epoch 8: loss = 7.5981362718153616\n",
      "Chamfer loss: 0.7314294520132639\n",
      "Eikonal loss: 0.009212477775525472\n",
      "PC loss: 0.2987155853612583\n",
      "Zero target points loss: 6.558583253821335\n",
      "cvt_loss:  tensor(0.0197, grad_fn=<MeanBackward0>)\n",
      "Epoch 9: loss = 6.524011903383743\n",
      "Chamfer loss: 0.7144293638703525\n",
      "Eikonal loss: 0.003993405931812033\n",
      "PC loss: 0.2675870192025382\n",
      "Zero target points loss: 5.537804866257343\n",
      "cvt_loss:  tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch 10: loss = 5.603241273768688\n",
      "Chamfer loss: 0.6740590117892813\n",
      "Eikonal loss: -0.0005163950913124902\n",
      "PC loss: 0.23739802367785226\n",
      "Zero target points loss: 4.6920857880797255\n",
      "cvt_loss:  tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "Epoch 11: loss = 4.836819663212322\n",
      "Chamfer loss: 0.6377948314883369\n",
      "Eikonal loss: -0.0041556173761931004\n",
      "PC loss: 0.2122820783721668\n",
      "Zero target points loss: 3.9907311806767276\n",
      "cvt_loss:  tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "Epoch 12: loss = 4.1739723380696825\n",
      "Chamfer loss: 0.5951068746804432\n",
      "Eikonal loss: -0.007030271375761935\n",
      "PC loss: 0.1870825917479338\n",
      "Zero target points loss: 3.398614044822742\n",
      "cvt_loss:  tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "Epoch 13: loss = 3.6178446369939983\n",
      "Chamfer loss: 0.5713609889157419\n",
      "Eikonal loss: -0.009360485469437725\n",
      "PC loss: 0.16156747943069316\n",
      "Zero target points loss: 2.8940734607888037\n",
      "cvt_loss:  tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "Epoch 14: loss = 3.1342413434147334\n",
      "Chamfer loss: 0.5364351402509396\n",
      "Eikonal loss: -0.011345008639494003\n",
      "PC loss: 0.1426405918394594\n",
      "Zero target points loss: 2.4662897585100114\n",
      "cvt_loss:  tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "Epoch 15: loss = 2.73407821595299\n",
      "Chamfer loss: 0.515550742787036\n",
      "Eikonal loss: -0.013142893355486383\n",
      "PC loss: 0.12466628775973404\n",
      "Zero target points loss: 2.1068077493681154\n",
      "cvt_loss:  tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "Epoch 16: loss = 2.3924978064862747\n",
      "Chamfer loss: 0.49206086917088765\n",
      "Eikonal loss: -0.014862232327917075\n",
      "PC loss: 0.10832765026142495\n",
      "Zero target points loss: 1.806767352491815\n",
      "cvt_loss:  tensor(0.0195, grad_fn=<MeanBackward0>)\n",
      "Epoch 17: loss = 2.1204343882479932\n",
      "Chamfer loss: 0.4821165338841724\n",
      "Eikonal loss: -0.016526836137613066\n",
      "PC loss: 0.09609745467198305\n",
      "Zero target points loss: 1.558552101640486\n",
      "cvt_loss:  tensor(0.0194, grad_fn=<MeanBackward0>)\n",
      "Epoch 18: loss = 1.8832489447593228\n",
      "Chamfer loss: 0.4638869826153661\n",
      "Eikonal loss: -0.018163348638077052\n",
      "PC loss: 0.08392947934526725\n",
      "Zero target points loss: 1.3534016370095523\n",
      "cvt_loss:  tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "Epoch 19: loss = 1.7030873799137223\n",
      "Chamfer loss: 0.4627442714547499\n",
      "Eikonal loss: -0.019719170920379583\n",
      "PC loss: 0.07688874190461673\n",
      "Zero target points loss: 1.1829696120866444\n",
      "cvt_loss:  tensor(0.0198, grad_fn=<MeanBackward0>)\n",
      "Epoch 20: loss = 1.530606845755223\n",
      "Chamfer loss: 0.442135024149135\n",
      "Eikonal loss: -0.02119647055948355\n",
      "PC loss: 0.0690872108623451\n",
      "Zero target points loss: 1.0403826951982054\n",
      "cvt_loss:  tensor(0.0218, grad_fn=<MeanBackward0>)\n",
      "Epoch 21: loss = 1.3856338243487625\n",
      "Chamfer loss: 0.4251199344471655\n",
      "Eikonal loss: -0.022615986970660848\n",
      "PC loss: 0.06231669451665916\n",
      "Zero target points loss: 0.9205950484871819\n",
      "cvt_loss:  tensor(0.0241, grad_fn=<MeanBackward0>)\n",
      "Epoch 22: loss = 1.278439664669289\n",
      "Chamfer loss: 0.4271919199381223\n",
      "Eikonal loss: -0.02399334115265542\n",
      "PC loss: 0.05717313941114882\n",
      "Zero target points loss: 0.8178267978867495\n",
      "cvt_loss:  tensor(0.0182, grad_fn=<MeanBackward0>)\n",
      "Epoch 23: loss = 1.1585103414619191\n",
      "Chamfer loss: 0.4035243881231878\n",
      "Eikonal loss: -0.025344793841332414\n",
      "PC loss: 0.051955668833226665\n",
      "Zero target points loss: 0.7281930259718911\n",
      "cvt_loss:  tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "Epoch 24: loss = 1.0874767977057656\n",
      "Chamfer loss: 0.415510907203328\n",
      "Eikonal loss: -0.02669020938501241\n",
      "PC loss: 0.04821985687876115\n",
      "Zero target points loss: 0.6502292543182737\n",
      "cvt_loss:  tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "Epoch 25: loss = 0.9960730487311502\n",
      "Chamfer loss: 0.3964821279967477\n",
      "Eikonal loss: -0.02800801394981417\n",
      "PC loss: 0.04480220232271234\n",
      "Zero target points loss: 0.5825754776979837\n",
      "cvt_loss:  tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "Epoch 26: loss = 0.915814600291413\n",
      "Chamfer loss: 0.3786494158817714\n",
      "Eikonal loss: -0.029284822833796877\n",
      "PC loss: 0.041492953320777126\n",
      "Zero target points loss: 0.5247381239100474\n",
      "cvt_loss:  tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "Epoch 27: loss = 0.8615693248476003\n",
      "Chamfer loss: 0.37704076396771913\n",
      "Eikonal loss: -0.030448050156899182\n",
      "PC loss: 0.03917763315136102\n",
      "Zero target points loss: 0.4755959973878622\n",
      "cvt_loss:  tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "Epoch 28: loss = 0.8259279468206782\n",
      "Chamfer loss: 0.38700927928256873\n",
      "Eikonal loss: -0.031480252658876016\n",
      "PC loss: 0.03699693521735778\n",
      "Zero target points loss: 0.4331948550435449\n",
      "cvt_loss:  tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "Epoch 29: loss = 0.7885387544256997\n",
      "Chamfer loss: 0.39003254771024726\n",
      "Eikonal loss: -0.03241799823087543\n",
      "PC loss: 0.034775475620695016\n",
      "Zero target points loss: 0.39598578269974816\n",
      "cvt_loss:  tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "Epoch 30: loss = 0.7560349959702881\n",
      "Chamfer loss: 0.39351321872944667\n",
      "Eikonal loss: -0.03332286071836837\n",
      "PC loss: 0.032716421672277965\n",
      "Zero target points loss: 0.3629654410602385\n",
      "cvt_loss:  tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "Epoch 31: loss = 0.7243057120585317\n",
      "Chamfer loss: 0.3935237485911072\n",
      "Eikonal loss: -0.034193295663826076\n",
      "PC loss: 0.031054538689634967\n",
      "Zero target points loss: 0.33370904946418894\n",
      "cvt_loss:  tensor(0.0189, grad_fn=<MeanBackward0>)\n",
      "Epoch 32: loss = 0.6729887846336265\n",
      "Chamfer loss: 0.3704312794020763\n",
      "Eikonal loss: -0.03500694639147681\n",
      "PC loss: 0.02957932481287866\n",
      "Zero target points loss: 0.307796051052828\n",
      "cvt_loss:  tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "Epoch 33: loss = 0.6345547833007298\n",
      "Chamfer loss: 0.35680520909207764\n",
      "Eikonal loss: -0.03577314385814999\n",
      "PC loss: 0.02866421616940539\n",
      "Zero target points loss: 0.2846584155904088\n",
      "cvt_loss:  tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "Epoch 34: loss = 0.6202927875423443\n",
      "Chamfer loss: 0.36526360200976266\n",
      "Eikonal loss: -0.03650613656153448\n",
      "PC loss: 0.02755035097470253\n",
      "Zero target points loss: 0.26378588232033534\n",
      "cvt_loss:  tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "Epoch 35: loss = 0.5727792349822056\n",
      "Chamfer loss: 0.3386887330473851\n",
      "Eikonal loss: -0.0372514433593989\n",
      "PC loss: 0.026413369402085687\n",
      "Zero target points loss: 0.2447616927849261\n",
      "cvt_loss:  tensor(0.0151, grad_fn=<MeanBackward0>)\n",
      "Epoch 36: loss = 0.551541501531673\n",
      "Chamfer loss: 0.3363729715815279\n",
      "Eikonal loss: -0.037986695760490075\n",
      "PC loss: 0.025342641774912612\n",
      "Zero target points loss: 0.2276620814870323\n",
      "cvt_loss:  tensor(0.0191, grad_fn=<MeanBackward0>)\n",
      "Epoch 37: loss = 0.5313428178190567\n",
      "Chamfer loss: 0.33295096635017274\n",
      "Eikonal loss: -0.03869258611398079\n",
      "PC loss: 0.02467439873850425\n",
      "Zero target points loss: 0.2122187845857157\n",
      "cvt_loss:  tensor(0.0165, grad_fn=<MeanBackward0>)\n",
      "Epoch 38: loss = 0.5377416914760387\n",
      "Chamfer loss: 0.355136855370197\n",
      "Eikonal loss: -0.03936539324726328\n",
      "PC loss: 0.023575581080782945\n",
      "Zero target points loss: 0.19822971237048892\n",
      "cvt_loss:  tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "Epoch 39: loss = 0.5033589517924388\n",
      "Chamfer loss: 0.3347729446570571\n",
      "Eikonal loss: -0.0400002266984197\n",
      "PC loss: 0.02287202117594595\n",
      "Zero target points loss: 0.18554353691030837\n",
      "cvt_loss:  tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "Epoch 40: loss = 0.5104245035179475\n",
      "Chamfer loss: 0.3545183417051177\n",
      "Eikonal loss: -0.04060211881378013\n",
      "PC loss: 0.022305696890393467\n",
      "Zero target points loss: 0.17402903872431563\n",
      "cvt_loss:  tensor(0.0139, grad_fn=<MeanBackward0>)\n",
      "Epoch 41: loss = 0.5269093233413756\n",
      "Chamfer loss: 0.3828388808226256\n",
      "Eikonal loss: -0.041177383912955945\n",
      "PC loss: 0.021551592767527494\n",
      "Zero target points loss: 0.1635571092335476\n",
      "cvt_loss:  tensor(0.0190, grad_fn=<MeanBackward0>)\n",
      "Epoch 42: loss = 0.5001766580453694\n",
      "Chamfer loss: 0.3669314569861305\n",
      "Eikonal loss: -0.04173705535475841\n",
      "PC loss: 0.020779346455784387\n",
      "Zero target points loss: 0.15401258336425347\n",
      "cvt_loss:  tensor(0.0175, grad_fn=<MeanBackward0>)\n",
      "Epoch 43: loss = 0.4850966310812707\n",
      "Chamfer loss: 0.3613257779127736\n",
      "Eikonal loss: -0.042276162591159655\n",
      "PC loss: 0.020577903693795085\n",
      "Zero target points loss: 0.14529381119793688\n",
      "cvt_loss:  tensor(0.0187, grad_fn=<MeanBackward0>)\n",
      "Epoch 44: loss = 0.4419857947982305\n",
      "Chamfer loss: 0.3272800863144233\n",
      "Eikonal loss: -0.04280528159775789\n",
      "PC loss: 0.020011311659808708\n",
      "Zero target points loss: 0.13731285826562725\n",
      "cvt_loss:  tensor(0.0165, grad_fn=<MeanBackward0>)\n",
      "Epoch 45: loss = 0.4290622736744143\n",
      "Chamfer loss: 0.32268687120456035\n",
      "Eikonal loss: -0.04332404154008788\n",
      "PC loss: 0.019540363737559796\n",
      "Zero target points loss: 0.12999453704442102\n",
      "cvt_loss:  tensor(0.0168, grad_fn=<MeanBackward0>)\n",
      "Epoch 46: loss = 0.43002030471073055\n",
      "Chamfer loss: 0.331309403053248\n",
      "Eikonal loss: -0.0438294458442656\n",
      "PC loss: 0.019094206466127285\n",
      "Zero target points loss: 0.12327858366179874\n",
      "cvt_loss:  tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "Epoch 47: loss = 0.39580643679600525\n",
      "Chamfer loss: 0.3039191725549965\n",
      "Eikonal loss: -0.04431933666491638\n",
      "PC loss: 0.018915086561812074\n",
      "Zero target points loss: 0.11709571846751157\n",
      "cvt_loss:  tensor(0.0183, grad_fn=<MeanBackward0>)\n",
      "Epoch 48: loss = 0.4107494031136524\n",
      "Chamfer loss: 0.32571396722452955\n",
      "Eikonal loss: -0.04479540048757891\n",
      "PC loss: 0.018254063144374524\n",
      "Zero target points loss: 0.11139389639211852\n",
      "cvt_loss:  tensor(0.0158, grad_fn=<MeanBackward0>)\n",
      "Epoch 49: loss = 0.4046937936774594\n",
      "Chamfer loss: 0.3254124308368638\n",
      "Eikonal loss: -0.045254258426242464\n",
      "PC loss: 0.018244757218982024\n",
      "Zero target points loss: 0.10613304358333583\n",
      "cvt_loss:  tensor(0.0170, grad_fn=<MeanBackward0>)\n",
      "Epoch 50: loss = 0.38028381940473177\n",
      "Chamfer loss: 0.3068196280182517\n",
      "Eikonal loss: -0.04570039686324585\n",
      "PC loss: 0.01772902848340316\n",
      "Zero target points loss: 0.10126589514587071\n",
      "Epoch 50: loss = 0.38028381940473177\n",
      "Best Epoch 50: Best loss = 0.38028381940473177\n",
      "cvt_loss:  tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch 51: loss = 0.39726192416820383\n",
      "Chamfer loss: 0.3288747773570258\n",
      "Eikonal loss: -0.04613947466365784\n",
      "PC loss: 0.01756523956428427\n",
      "Zero target points loss: 0.09674685381846801\n",
      "cvt_loss:  tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "Epoch 52: loss = 0.385863832249138\n",
      "Chamfer loss: 0.32244911895122963\n",
      "Eikonal loss: -0.04656601843935607\n",
      "PC loss: 0.017229504744605374\n",
      "Zero target points loss: 0.09254262161001019\n",
      "cvt_loss:  tensor(0.0213, grad_fn=<MeanBackward0>)\n",
      "Epoch 53: loss = 0.41315660070313154\n",
      "Chamfer loss: 0.35421303688059624\n",
      "Eikonal loss: -0.046980946724007905\n",
      "PC loss: 0.01708791007847797\n",
      "Zero target points loss: 0.0886236652180383\n",
      "cvt_loss:  tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "Epoch 54: loss = 0.392565492760346\n",
      "Chamfer loss: 0.3382359209095086\n",
      "Eikonal loss: -0.04738675059557082\n",
      "PC loss: 0.01654303904952359\n",
      "Zero target points loss: 0.08496311513144816\n",
      "cvt_loss:  tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "Epoch 55: loss = 0.38641895888947964\n",
      "Chamfer loss: 0.3360004658448969\n",
      "Eikonal loss: -0.04778456705615115\n",
      "PC loss: 0.0165050696768803\n",
      "Zero target points loss: 0.08153485449967204\n",
      "cvt_loss:  tensor(0.0220, grad_fn=<MeanBackward0>)\n",
      "Epoch 56: loss = 0.37713131369418273\n",
      "Chamfer loss: 0.33051244828011744\n",
      "Eikonal loss: -0.048173213706384624\n",
      "PC loss: 0.016255951839750146\n",
      "Zero target points loss: 0.07831639351863479\n",
      "cvt_loss:  tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "Epoch 57: loss = 0.36966664043808173\n",
      "Chamfer loss: 0.32689979184885215\n",
      "Eikonal loss: -0.04855185679420772\n",
      "PC loss: 0.015818370980573145\n",
      "Zero target points loss: 0.07529369426923263\n",
      "cvt_loss:  tensor(0.0178, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m profiler \u001b[38;5;241m=\u001b[39m cProfile\u001b[38;5;241m.\u001b[39mProfile()\n\u001b[1;32m     25\u001b[0m profiler\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 27\u001b[0m sites \u001b[38;5;241m=\u001b[39m \u001b[43mautograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m profiler\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m     30\u001b[0m stats \u001b[38;5;241m=\u001b[39m pstats\u001b[38;5;241m.\u001b[39mStats(profiler)\u001b[38;5;241m.\u001b[39msort_stats(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m, in \u001b[0;36mautograd\u001b[0;34m(sites, model, max_iter, stop_train_threshold, upsampling, lambda_weights)\u001b[0m\n\u001b[1;32m     56\u001b[0m zero_target_points_loss \u001b[38;5;241m=\u001b[39m lambda_1 \u001b[38;5;241m*\u001b[39m zero_target_points_loss_L1 \u001b[38;5;241m+\u001b[39m lambda_2 \u001b[38;5;241m*\u001b[39m zero_target_points_loss_L2\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Track raw losses (unweighted)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m cvt_loss_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcvt_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#min_distance_loss_values.append(min_distance_loss.item())\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#edge_smoothing_loss_values.append(edge_smoothing_loss.item())\u001b[39;00m\n\u001b[1;32m     64\u001b[0m chamfer_distance_loss_values\u001b[38;5;241m.\u001b[39mappend(chamfer_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_weights = [0.01,0.1,0.2,0,1.000111111111101101000101,0.01,0,0.1]\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lamda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_target_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 500\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}3d_sites_{num_centroids}_chamfer{lamda_chamfer}.npy'\n",
    "#check if optimized sites file exists\n",
    "if os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)    \n",
    "else:\n",
    "    import cProfile, pstats\n",
    "    import time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "    \n",
    "    profiler.disable()\n",
    "    stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    stats.print_stats()\n",
    "    stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lamda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "#plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/3D/bunny500_50_3d_model_4096_chamfer1.0001111111111012.pth\n",
      "sites ./images/autograd/3D/bunny500_50_3d_sites_4096_chamfer1.0001111111111012.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "sites = torch.load(site_file_path)\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces\", final_mesh[0], final_mesh[1])\n",
    "ps.register_point_cloud(\"Mesh vertices\", final_mesh[0])\n",
    "polyscope_sdf(model,2)\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_visualisation_3d():\n",
    "    import imageio\n",
    "    img_buffer_mesh = []\n",
    "    img_buffer_model = []\n",
    "    for i in range(int(max_iter/10)+1):\n",
    "        epoch = i*int(max_iter/10)\n",
    "        \n",
    "        site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "        model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "        if os.path.exists(site_file_path) and os.path.exists(model_file_path):\n",
    "            print(\"importing sites and model\")\n",
    "        else:\n",
    "            print(\"files not found\")\n",
    "            continue\n",
    "        print(\"mesh of epoch: \", epoch)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_file_path))\n",
    "    \n",
    "        current_mesh = su.get_zero_crossing_mesh_3d(torch.load(site_file_path), model)\n",
    "        ps.remove_all_structures()\n",
    "        ps.register_surface_mesh(\"Zero-Crossing faces\", current_mesh[0], current_mesh[1])\n",
    "        ps.register_point_cloud(\"Mesh vertices\", current_mesh[0])\n",
    "        img_buffer_mesh.append(ps.screenshot_to_buffer(transparent_bg=False))\n",
    "        \n",
    "        ps.remove_all_structures()\n",
    "        polyscope_sdf(model)\n",
    "        img_buffer_model.append(ps.screenshot_to_buffer(transparent_bg=False))\n",
    "\n",
    "\n",
    "    imageio.mimsave(f'{destination}{max_iter}_3d_{num_centroids}_optimization_mesh.gif',img_buffer_mesh, fps=1, duration=1, loop=0)\n",
    "    imageio.mimsave(f'{destination}{max_iter}_3d_{num_centroids}_optimization_sdf.gif', img_buffer_model, fps=1, duration=1, loop=0)\n",
    "\n",
    "#export_visualisation_3d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinycuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
