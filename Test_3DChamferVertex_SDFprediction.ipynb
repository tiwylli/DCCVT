{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import interactive_polyscope\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import trimesh\n",
    "from scipy.spatial import Delaunay, Voronoi\n",
    "\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.03\n",
    "lr_model = 0.0003\n",
    "iterations = 5000\n",
    "save_every = 100\n",
    "max_iter = 100\n",
    "#learning_rate = 0.03\n",
    "destination = \"./images/autograd/3D/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently sites are between -5 and 5 in all 3 dimensions\n",
    "# check if sites exists\n",
    "#num_centroids = 16*16*16\n",
    "num_centroids = 8*8*8\n",
    "site_fp = f'sites_{num_centroids}.pt'\n",
    "\n",
    "if os.path.exists(site_fp):\n",
    "    sites = torch.load(site_fp)\n",
    "else:\n",
    "    sites = su.createCVTgrid(num_centroids=num_centroids, dimensionality=3)\n",
    "    #save the initial sites torch tensor\n",
    "    torch.save(sites, site_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.init()\n",
    "# ps_cloud = ps.register_point_cloud(\"a\",sites.detach().cpu().numpy())\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mesh\n",
    "bunny = trimesh.load(\"Resources/stanford-bunny.obj\")\n",
    "\n",
    "# Step 1: Get current bounding box\n",
    "min_bound = bunny.bounds[0]  # Min (x, y, z)\n",
    "max_bound = bunny.bounds[1]  # Max (x, y, z)\n",
    "\n",
    "# Step 2: Compute scale factor\n",
    "current_size = max_bound - min_bound  # Size in each dimension\n",
    "target_size = 8  # Because we want [-5, 5], the total size is 10\n",
    "\n",
    "scale_factor = target_size / np.max(current_size)  # Scale based on the largest dimension\n",
    "\n",
    "# Step 3: Compute new center after scaling\n",
    "new_vertices = bunny.vertices * scale_factor  # Scale the vertices\n",
    "new_min = np.min(new_vertices, axis=0)\n",
    "new_max = np.max(new_vertices, axis=0)\n",
    "new_center = (new_min + new_max) / 2  # New center after scaling\n",
    "\n",
    "# Step 4: Compute translation to center the bunny at (0,0,0)\n",
    "translation = -new_center  # Move to the origin\n",
    "\n",
    "# Step 5: Apply transformation (scaling + translation)\n",
    "bunny.vertices = new_vertices + translation\n",
    "\n",
    "target_points = bunny.sample(16*16*16)\n",
    "target_points = torch.tensor(target_points, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.init()\n",
    "# ps_cloud = ps.register_point_cloud(\"b\",target_points.detach().cpu().numpy())\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "\n",
    "model = mlp.Decoder(multires=multires, input_dims=input_dims).to(device)\n",
    "model_path = 'models_resources/pretrained_sphere_small.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model')\n",
    "else:\n",
    "    print(\"no model found, pretraining\")\n",
    "    model.pre_train_sphere(3000)\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "    \n",
    "def polyscope_sdf(model):\n",
    "    # Render the SDF as an implicit surface (zero-level set)\n",
    "    def model_sdf(pts):\n",
    "        pts_tensor = torch.tensor(pts, dtype=torch.float64, device=device)\n",
    "        sdf_values = model(pts_tensor)\n",
    "        sdf_values_np = sdf_values.detach().cpu().numpy().flatten()  # Convert to NumPy\n",
    "        \n",
    "        return sdf_values_np\n",
    "\n",
    "    ps.render_implicit_surface(\"SDF Surface\", model_sdf, mode=\"sphere_march\", enabled=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.cm as cm\n",
    "# ps.init()\n",
    "\n",
    "# #genereate random 3d points tensor\n",
    "# points = torch.rand(10000,3, device=device)*10-5\n",
    "# #predict sdf values for the points\n",
    "# sdf_values = model(points)\n",
    "# #give them a color to plot with polyscope\n",
    "# sdf_values_np = sdf_values.detach().cpu().numpy().flatten()  # Convert to NumPy\n",
    "# sdf_norm = (sdf_values_np - sdf_values_np.min()) / (sdf_values_np.max() - sdf_values_np.min())  # Normalize to [0,1]\n",
    "# colors = cm.coolwarm(sdf_norm)[:, :3]  # Get RGB colors from colormap\n",
    "\n",
    "# # Step 4: Convert points to NumPy for Polyscope\n",
    "# points_np = points.detach().cpu().numpy()\n",
    "\n",
    "# # Step 5: Visualize with Polyscope\n",
    "# ps_cloud = ps.register_point_cloud(\"SDF Points\", points_np, radius=0.01)\n",
    "# ps_cloud.add_color_quantity(\"SDF Colors\", colors)\n",
    "\n",
    "\n",
    "# grid_res = 50  # Resolution of the grid\n",
    "# x = torch.linspace(-5.0, 5.0, grid_res)\n",
    "# y = torch.linspace(-5.0, 5.0, grid_res)\n",
    "# z = torch.linspace(-5.0, 5.0, grid_res)\n",
    "\n",
    "# X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')  # Create a 3D grid\n",
    "# grid_points = torch.stack([X, Y, Z], dim=-1).reshape(-1, 3).to(device)  # Flatten into Nx3 tensor\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     sdf_values = model(grid_points)  # Predict SDF values\n",
    "# sdf_values_np = sdf_values.cpu().numpy().flatten()  # Convert to NumPy\n",
    "\n",
    "# # Step 3: Normalize SDF values for colormap\n",
    "# sdf_norm = (sdf_values_np - sdf_values_np.min()) / (sdf_values_np.max() - sdf_values_np.min())\n",
    "# colors = cm.coolwarm(sdf_norm)[:, :3]  # Map colors from colormap\n",
    "\n",
    "# # Step 4: Convert points to NumPy for Polyscope\n",
    "# points_np = grid_points.cpu().numpy()\n",
    "\n",
    "# # Step 5: Visualize with Polyscope\n",
    "# ps_cloud = ps.register_point_cloud(\"SDF Grid\", points_np, radius=0.01)\n",
    "# ps_cloud.add_color_quantity(\"SDF Values\", colors)\n",
    "\n",
    "# # Show the visualization\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo this should also work in 2d, might want to adjust the simplex iteration\n",
    "def get_delaunay_neighbors_list_3d(sites):\n",
    "    # Detach and convert to NumPy for Delaunay triangulation\n",
    "    points_np = sites.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute the Delaunay tessellation\n",
    "    tri = Delaunay(points_np)\n",
    "\n",
    "    # Find the neighbors of each point\n",
    "    neighbors = {i: set() for i in range(len(points_np))}\n",
    "    for simplex in tri.simplices:  # Each simplex is a tetrahedron (4 points)\n",
    "        for i in range(4):  # Iterate over each of the 4 vertices\n",
    "            for j in range(i + 1, 4):  # Connect each pair of points\n",
    "                neighbors[simplex[i]].add(simplex[j])\n",
    "                neighbors[simplex[j]].add(simplex[i])\n",
    "\n",
    "    # Convert sets to lists for easier use\n",
    "    neighbors = {key: list(value) for key, value in neighbors.items()}\n",
    "    return neighbors\n",
    "\n",
    "def compute_vertices_index_3d(neighbors):\n",
    "    vertices_index_to_compute = []\n",
    "    \n",
    "    for site, adjacents in neighbors.items():\n",
    "        for i in adjacents:\n",
    "            for n in adjacents:\n",
    "                if n != site and n != i and n in neighbors[i]:  # Ensuring a triangle exists\n",
    "                    for m in adjacents:\n",
    "                        if m != site and m != i and m != n and m in neighbors[i] and m in neighbors[n]:\n",
    "                            # We now have 4 mutually connected points forming a tetrahedron\n",
    "                            vertices_index_to_compute.append([i, site, n, m])\n",
    "    \n",
    "    # Deduplicate tetrahedra (avoid different orderings of the same set)\n",
    "    seen_tetrahedra = set()\n",
    "    filtered_tetrahedra = []\n",
    "\n",
    "    for tetrahedron in vertices_index_to_compute:\n",
    "        canonical_tetrahedron = tuple(sorted(tetrahedron, key=str))  # Sort for uniqueness\n",
    "        if canonical_tetrahedron not in seen_tetrahedra:\n",
    "            seen_tetrahedra.add(canonical_tetrahedron)\n",
    "            filtered_tetrahedra.append(tetrahedron)\n",
    "    \n",
    "    return filtered_tetrahedra\n",
    "\n",
    "def compute_zero_crossing_vertices_3d(sites, model):\n",
    "    \"\"\"\n",
    "    Computes the indices of the sites composing vertices where neighboring sites have opposite or zero SDF values.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, D) tensor of site positions.\n",
    "        model (callable): Function or neural network that computes SDF values.\n",
    "\n",
    "    Returns:\n",
    "        zero_crossing_vertices_index (list of triplets): List of sites indices (si, sj, sk) where atleast 2 sites have opposing SDF signs.\n",
    "    \"\"\"\n",
    "    # Compute Delaunay neighbors\n",
    "    neighbors = get_delaunay_neighbors_list_3d(sites)\n",
    "\n",
    "    # Compute SDF values for all sites\n",
    "    sdf_values = model(sites)  # Assuming model outputs (N, 1) or (N,) tensor\n",
    "\n",
    "    # Find pairs of neighbors with opposing SDF values\n",
    "    zero_crossing_pairs = set()\n",
    "    for i, adjacents in neighbors.items():\n",
    "        for j in adjacents:\n",
    "            if i < j:  # Avoid duplicates\n",
    "                sdf_i, sdf_j = sdf_values[i].item(), sdf_values[j].item()\n",
    "                if sdf_i * sdf_j <= 0:  # Opposing signs or one is zero\n",
    "                    zero_crossing_pairs.add((i, j))\n",
    "\n",
    "    # Compute tetrahedra (quadruplets) and filter only those involving zero-crossing pairs\n",
    "    all_tetrahedra = compute_vertices_index_3d(neighbors)  # Now returns quadruplets\n",
    "    zero_crossing_vertices_index = []\n",
    "    \n",
    "    for tetrahedron in all_tetrahedra:\n",
    "        tetrahedron_pairs = {\n",
    "            (tetrahedron[0], tetrahedron[1]), (tetrahedron[0], tetrahedron[2]), (tetrahedron[0], tetrahedron[3]),\n",
    "            (tetrahedron[1], tetrahedron[2]), (tetrahedron[1], tetrahedron[3]), (tetrahedron[2], tetrahedron[3])\n",
    "        }\n",
    "        if any(pair in zero_crossing_pairs for pair in tetrahedron_pairs):\n",
    "            zero_crossing_vertices_index.append(tetrahedron)\n",
    "\n",
    "    return zero_crossing_vertices_index, zero_crossing_pairs\n",
    "\n",
    "\n",
    "# def compute_vertex(s_i, s_j, s_k):\n",
    "#     # Unpack coordinates for each site\n",
    "#     x_i, y_i = s_i[0], s_i[1]\n",
    "#     x_j, y_j = s_j[0], s_j[1]\n",
    "#     x_k, y_k = s_k[0], s_k[1]\n",
    "    \n",
    "#     # Calculate numerator and  for x coordinate\n",
    "#     n_x = (\n",
    "#         x_i**2 * (y_j - y_k)\n",
    "#         - x_j**2 * (y_i - y_k)\n",
    "#         + (x_k**2 + (y_i - y_k) * (y_j - y_k)) * (y_i - y_j)\n",
    "#     )\n",
    "\n",
    "#     # Calculate numerator for y coordinate\n",
    "#     n_y = -(\n",
    "#         x_i**2 * (x_j - x_k)\n",
    "#         - x_i * (x_j**2 - x_k**2 + y_j**2 - y_k**2)\n",
    "#         + x_j**2 * x_k\n",
    "#         - x_j * (x_k**2 - y_i**2 + y_k**2)\n",
    "#         - x_k * (y_i**2 - y_j**2)\n",
    "#     )\n",
    "    \n",
    "#     # Calculate denominator \n",
    "#     d = 2 * (x_i * (y_j - y_k) - x_j * (y_i - y_k) + x_k * (y_i - y_j))\n",
    "    \n",
    "#     # Calculate x and y coordinates\n",
    "#     x = n_x / d\n",
    "#     y = n_y / d\n",
    "\n",
    "#     # Return x, y as a tensor to maintain the computational graph\n",
    "#     return torch.stack([x, y])\n",
    "\n",
    "def compute_vertex_3d(s_i, s_j, s_k, s_l):\n",
    "    \"\"\"\n",
    "    Computes the circumcenter of a tetrahedron given four sites in 3D.\n",
    "\n",
    "    Args:\n",
    "        s_i, s_j, s_k, s_l (torch.Tensor): 3D coordinates of four sites (shape: (3,)).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The circumcenter (x, y, z).\n",
    "    \"\"\"\n",
    "    # Convert to coordinate form\n",
    "    x_i, y_i, z_i = s_i\n",
    "    x_j, y_j, z_j = s_j\n",
    "    x_k, y_k, z_k = s_k\n",
    "    x_l, y_l, z_l = s_l\n",
    "\n",
    "    # Compute squared norms\n",
    "    s_i2 = x_i**2 + y_i**2 + z_i**2\n",
    "    s_j2 = x_j**2 + y_j**2 + z_j**2\n",
    "    s_k2 = x_k**2 + y_k**2 + z_k**2\n",
    "    s_l2 = x_l**2 + y_l**2 + z_l**2\n",
    "\n",
    "    # Construct matrix system\n",
    "    A = torch.tensor([\n",
    "        [x_i, y_i, z_i, 1],\n",
    "        [x_j, y_j, z_j, 1],\n",
    "        [x_k, y_k, z_k, 1],\n",
    "        [x_l, y_l, z_l, 1]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    Dx = torch.tensor([\n",
    "        [s_i2, y_i, z_i, 1],\n",
    "        [s_j2, y_j, z_j, 1],\n",
    "        [s_k2, y_k, z_k, 1],\n",
    "        [s_l2, y_l, z_l, 1]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    Dy = torch.tensor([\n",
    "        [x_i, s_i2, z_i, 1],\n",
    "        [x_j, s_j2, z_j, 1],\n",
    "        [x_k, s_k2, z_k, 1],\n",
    "        [x_l, s_l2, z_l, 1]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    Dz = torch.tensor([\n",
    "        [x_i, y_i, s_i2, 1],\n",
    "        [x_j, y_j, s_j2, 1],\n",
    "        [x_k, y_k, s_k2, 1],\n",
    "        [x_l, y_l, s_l2, 1]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    # Compute determinants\n",
    "    detA = torch.det(A)\n",
    "    detDx = torch.det(Dx)\n",
    "    detDy = -torch.det(Dy)  # Negative due to orientation\n",
    "    detDz = torch.det(Dz)\n",
    "\n",
    "    # Compute circumcenter coordinates\n",
    "    x = 0.5 * (detDx / detA)\n",
    "    y = 0.5 * (detDy / detA)\n",
    "    z = 0.5 * (detDz / detA)\n",
    "\n",
    "    return torch.stack([x, y, z])\n",
    "\n",
    "def compute_all_vertices_3d(sites, vertices_to_compute):\n",
    "    \"\"\"\n",
    "    Computes all Voronoi vertices for a given set of tetrahedra.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "        vertices_to_compute (list of quadruplets): List of indices forming tetrahedra.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Computed Voronoi vertices.\n",
    "    \"\"\"\n",
    "    vertices = []\n",
    "    \n",
    "    for quadruplet in vertices_to_compute:\n",
    "        si = sites[quadruplet[0]]\n",
    "        sj = sites[quadruplet[1]]\n",
    "        sk = sites[quadruplet[2]]\n",
    "        sl = sites[quadruplet[3]]\n",
    "\n",
    "        # Compute circumcenter for the tetrahedron\n",
    "        v = compute_vertex_3d(si, sj, sk, sl)\n",
    "        vertices.append(v)\n",
    "\n",
    "    # Stack all vertices into a single tensor\n",
    "    vertices = torch.stack(vertices)\n",
    "    return vertices\n",
    "\n",
    "#Todo its the same in 3d if we only want a point. Do we want the plane ?\n",
    "def compute_all_bisectors_3d(sites, bisectors_to_compute):\n",
    "    # Initialize an empty tensor for storing bisectors\n",
    "    bisectors = []\n",
    "    \n",
    "    for pairs in bisectors_to_compute:\n",
    "        si = sites[pairs[0]]\n",
    "        sj = sites[pairs[1]]\n",
    "        b = (si + sj) / 2\n",
    "        bisectors.append(b)\n",
    "\n",
    "    # Stack the list of bisectors into a single tensor for easier gradient tracking\n",
    "    bisectors = torch.stack(bisectors)\n",
    "    return bisectors\n",
    "\n",
    "#Todo see above\n",
    "# def compute_all_bisector_planes_3d(sites, bisectors_to_compute):\n",
    "#     \"\"\"\n",
    "#     Computes bisector planes for given site pairs in 3D.\n",
    "\n",
    "#     Args:\n",
    "#         sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "#         bisectors_to_compute (list of pairs): List of index pairs (i, j).\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Plane normals (N, 3) and midpoints (N, 3).\n",
    "#     \"\"\"\n",
    "#     midpoints = []\n",
    "#     normals = []\n",
    "    \n",
    "#     for pair in bisectors_to_compute:\n",
    "#         si = sites[pair[0]]\n",
    "#         sj = sites[pair[1]]\n",
    "        \n",
    "#         midpoint = (si + sj) / 2  # Midpoint\n",
    "#         normal = sj - si  # Normal direction\n",
    "        \n",
    "#         midpoints.append(midpoint)\n",
    "#         normals.append(normal)\n",
    "\n",
    "#     # Stack into tensors\n",
    "#     midpoints = torch.stack(midpoints)\n",
    "#     normals = torch.stack(normals)\n",
    "    \n",
    "#     return normals, midpoints\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "edge_smoothing_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "zero_target_points_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.5, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_sdf = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    lambda_laplace = lambda_weights[3]\n",
    "    lamda_chamfer = lambda_weights[4]\n",
    "    lamda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    lambda_target_points = lambda_weights[7]\n",
    "    \n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = compute_zero_crossing_vertices_3d(sites, model)\n",
    "        vertices = compute_all_vertices_3d(sites, vertices_to_compute)\n",
    "        bisectors = compute_all_bisectors_3d(sites, bisectors_to_compute)\n",
    "\n",
    "        \n",
    "        #combine vertices and bisectors to one tensor for chamfer\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "\n",
    "        print(\"points length: \",len(points))\n",
    "  \n",
    "\n",
    "        # Compute losses       \n",
    "        #cvt_loss = lf.compute_cvt_loss(sites)\n",
    "        #min_distance_loss = min_distance_regularization_for_op_sites(edges,sites)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        #edge_smoothing_loss = compute_edge_smoothing_loss(edges, sites, model)\n",
    "        chamfer_loss = lf.chamfer_distance(target_points, points)\n",
    "        eikonal_loss = lf.eikonal(model, input_dimensions=input_dims)\n",
    "        #domain_restriction_loss = lf.domain_restriction(target_points, model)\n",
    "        \n",
    "        sdf_values_target_points = model(target_points)[:,0]\n",
    "        zero_target_points_loss_L2 = torch.mean(sdf_values_target_points**2)\n",
    "        zero_target_points_loss_L1 = torch.mean(torch.abs(model(target_points)[:, 0]))\n",
    "        lambda_1, lambda_2 = 0 , 0.99  # Adjust weights as needed\n",
    "        zero_target_points_loss = lambda_1 * zero_target_points_loss_L1 + lambda_2 * zero_target_points_loss_L2\n",
    "\n",
    "               \n",
    "        # Track raw losses (unweighted)\n",
    "        #cvt_loss_values.append(cvt_loss.item())\n",
    "        #min_distance_loss_values.append(min_distance_loss.item())\n",
    "        #edge_smoothing_loss_values.append(edge_smoothing_loss.item())\n",
    "        chamfer_distance_loss_values.append(chamfer_loss.item())\n",
    "        eikonal_loss_values.append(eikonal_loss.item())\n",
    "        #domain_restriction_loss_values.append(domain_restriction_loss.item())\n",
    "        zero_target_points_loss_values.append(zero_target_points_loss.item())\n",
    "  \n",
    "        loss = (\n",
    "            #lambda_cvt * cvt_loss +\n",
    "            #lambda_min_distance * min_distance_loss + \n",
    "            #lambda_laplace * edge_smoothing_loss +\n",
    "            lamda_chamfer * chamfer_loss +\n",
    "            lamda_eikonal * eikonal_loss +\n",
    "            #lambda_domain_restriction * domain_restriction_loss +\n",
    "            lambda_target_points * zero_target_points_loss\n",
    "        )\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            if upsampled > 0:\n",
    "                print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "            print(\"sites length: \",len(sites))\n",
    "            \n",
    "            new_sites = su.upsampling_inside(best_sites, model)\n",
    "            #new_sites = su.adaptive_density_upsampling(best_sites, model)\n",
    "            print(new_sites)\n",
    "            sites = su.add_upsampled_sites(best_sites, new_sites)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            print(\"upsampled sites length: \",len(sites))\n",
    "            \n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            \n",
    "            optimizer = torch.optim.Adam([{'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "                                          {'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{max_iter}_{epoch}_3d_sites _{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{max_iter}_{epoch}_3d_model_chamfer{lamda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        \n",
    "        epoch += 1           \n",
    "        \n",
    "    return best_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points length:  371\n",
      "Epoch 0: loss = 7.688159672442257\n",
      "Epoch 0: loss = 7.688159672442257\n",
      "Best Epoch 0: Best loss = 7.688159672442257\n",
      "points length:  559\n",
      "Epoch 1: loss = 4.395391065328132\n",
      "points length:  770\n",
      "Epoch 2: loss = 2.7098271640569798\n",
      "points length:  1076\n",
      "Epoch 3: loss = 2.1473397734139987\n",
      "points length:  1432\n",
      "Epoch 4: loss = 2.3141590160678303\n",
      "points length:  1712\n",
      "Epoch 5: loss = 2.746247669737153\n",
      "points length:  1710\n",
      "Epoch 6: loss = 2.3976830223500984\n",
      "points length:  1456\n",
      "Epoch 7: loss = 2.0722810206452347\n",
      "points length:  1285\n",
      "Epoch 8: loss = 2.1328699115120813\n",
      "points length:  1287\n",
      "Epoch 9: loss = 1.910777518501353\n",
      "points length:  1278\n",
      "Epoch 10: loss = 1.820337211157398\n",
      "points length:  1301\n",
      "Epoch 11: loss = 1.7655163214624598\n",
      "points length:  1341\n",
      "Epoch 12: loss = 1.7215139634464247\n",
      "points length:  1383\n",
      "Epoch 13: loss = 1.658152926526236\n",
      "points length:  1335\n",
      "Epoch 14: loss = 1.6730027161055054\n",
      "points length:  1392\n",
      "Epoch 15: loss = 1.6179409488475096\n",
      "points length:  1399\n",
      "Epoch 16: loss = 1.5816533591739468\n",
      "points length:  1422\n",
      "Epoch 17: loss = 1.5910697935589808\n",
      "points length:  1427\n",
      "Epoch 18: loss = 1.5437631368129674\n",
      "points length:  1520\n",
      "Epoch 19: loss = 1.5099807409709065\n",
      "points length:  1563\n",
      "Epoch 20: loss = 1.5164713278825688\n",
      "points length:  1546\n",
      "Epoch 21: loss = 1.4942002191079304\n",
      "points length:  1558\n",
      "Epoch 22: loss = 1.4757728948901636\n",
      "points length:  1652\n",
      "Epoch 23: loss = 1.4209545771226249\n",
      "points length:  1604\n",
      "Epoch 24: loss = 1.4498638260625512\n",
      "points length:  1635\n",
      "Epoch 25: loss = 1.4804413677822856\n",
      "points length:  1758\n",
      "Epoch 26: loss = 1.4522827231614257\n",
      "points length:  1779\n",
      "Epoch 27: loss = 1.4456322398191652\n",
      "points length:  1862\n",
      "Epoch 28: loss = 1.4512501821733574\n",
      "points length:  1800\n",
      "Epoch 29: loss = 1.41258036246295\n",
      "points length:  1896\n",
      "Epoch 30: loss = 1.4131221243811045\n",
      "points length:  1817\n",
      "Epoch 31: loss = 1.414673771468193\n",
      "points length:  1853\n",
      "Epoch 32: loss = 1.401708576567795\n",
      "points length:  1761\n",
      "Epoch 33: loss = 1.422808801629141\n",
      "points length:  1825\n",
      "Epoch 34: loss = 1.4409797978766758\n",
      "points length:  1801\n",
      "Epoch 35: loss = 1.4117713984510885\n",
      "points length:  1860\n",
      "Epoch 36: loss = 1.3860578238562555\n",
      "points length:  1788\n",
      "Epoch 37: loss = 1.4339149010209435\n",
      "points length:  1765\n",
      "Epoch 38: loss = 1.3631634049118675\n",
      "points length:  1912\n",
      "Epoch 39: loss = 1.3824706068218278\n",
      "points length:  1930\n",
      "Epoch 40: loss = 1.3691180910906242\n",
      "points length:  1891\n",
      "Epoch 41: loss = 1.3691525145907177\n",
      "points length:  1944\n",
      "Epoch 42: loss = 1.3837384158330612\n",
      "points length:  1898\n",
      "Epoch 43: loss = 1.409735982869676\n",
      "points length:  1833\n",
      "Epoch 44: loss = 1.3968399890042953\n",
      "points length:  1995\n",
      "Epoch 45: loss = 1.3305824424306862\n",
      "points length:  1925\n",
      "Epoch 46: loss = 1.3201557886692634\n",
      "points length:  1838\n",
      "Epoch 47: loss = 1.369548262504594\n",
      "points length:  1958\n",
      "Epoch 48: loss = 1.4012556213430125\n",
      "points length:  2110\n",
      "Epoch 49: loss = 1.325918508730154\n",
      "points length:  2020\n",
      "Epoch 50: loss = 1.3092480283482764\n",
      "Epoch 50: loss = 1.3092480283482764\n",
      "Best Epoch 50: Best loss = 1.3092480283482764\n",
      "points length:  2031\n",
      "Epoch 51: loss = 1.330488237076805\n",
      "points length:  2090\n",
      "Epoch 52: loss = 1.3121044785241207\n",
      "points length:  2161\n",
      "Epoch 53: loss = 1.3221633761175748\n",
      "points length:  2127\n",
      "Epoch 54: loss = 1.3332929287606285\n",
      "points length:  2160\n",
      "Epoch 55: loss = 1.3242398277057403\n",
      "points length:  2122\n",
      "Epoch 56: loss = 1.2961500229305785\n",
      "points length:  2108\n",
      "Epoch 57: loss = 1.3692737160991852\n",
      "points length:  2315\n",
      "Epoch 58: loss = 1.3498750623871187\n",
      "points length:  2239\n",
      "Epoch 59: loss = 1.3710331890176253\n",
      "points length:  2431\n",
      "Epoch 60: loss = 1.3372788388400008\n",
      "points length:  2328\n",
      "Epoch 61: loss = 1.3071612632299767\n",
      "points length:  2352\n",
      "Epoch 62: loss = 1.3145780879859141\n",
      "points length:  2386\n",
      "Epoch 63: loss = 1.513047978843257\n",
      "points length:  2547\n",
      "Epoch 64: loss = 1.464150910397472\n",
      "points length:  2567\n",
      "Epoch 65: loss = 1.3260403588432004\n",
      "points length:  2500\n",
      "Epoch 66: loss = 1.4452238732328742\n",
      "points length:  2454\n",
      "Epoch 67: loss = 1.2347546259863798\n",
      "points length:  2374\n",
      "Epoch 68: loss = 1.1878529002739615\n",
      "points length:  2477\n",
      "Epoch 69: loss = 1.296359637684318\n",
      "points length:  2547\n",
      "Epoch 70: loss = 1.3141887172607347\n",
      "points length:  2556\n",
      "Epoch 71: loss = 1.5771899221062915\n",
      "points length:  2573\n",
      "Epoch 72: loss = 1.4197686140797479\n",
      "points length:  2571\n",
      "Epoch 73: loss = 1.5270250619532186\n",
      "points length:  2517\n",
      "Epoch 74: loss = 1.599603074516945\n",
      "points length:  2479\n",
      "Epoch 75: loss = 1.4675401136086303\n",
      "points length:  2644\n",
      "Epoch 76: loss = 1.4056238954415325\n",
      "points length:  2572\n",
      "Epoch 77: loss = 1.4181643140612488\n",
      "points length:  2671\n",
      "Epoch 78: loss = 1.3129811100274111\n",
      "points length:  2742\n",
      "Epoch 79: loss = 1.4716542900075178\n",
      "points length:  2659\n",
      "Epoch 80: loss = 1.3684262971698726\n",
      "points length:  2632\n",
      "Epoch 81: loss = 1.8691771025165818\n",
      "points length:  2805\n",
      "Epoch 82: loss = 1.4933953988595134\n",
      "points length:  2751\n",
      "Epoch 83: loss = 1.5046411476456965\n",
      "points length:  2722\n",
      "Epoch 84: loss = 1.4743616950308727\n",
      "points length:  2719\n",
      "Epoch 85: loss = 1.4507138343421537\n",
      "points length:  2760\n",
      "Epoch 86: loss = 1.8848198522924187\n",
      "points length:  2848\n",
      "Epoch 87: loss = 2.017533319837706\n",
      "points length:  2849\n",
      "Epoch 88: loss = 1.533613348018904\n",
      "points length:  2768\n",
      "Epoch 89: loss = 1.487618879522298\n",
      "points length:  2759\n",
      "Epoch 90: loss = 1.4817592815895666\n",
      "points length:  2930\n",
      "Epoch 91: loss = 1.6372028934995508\n",
      "points length:  2761\n",
      "Epoch 92: loss = 1.5144365364009886\n",
      "points length:  2779\n",
      "Epoch 93: loss = 1.5232893876713096\n",
      "points length:  2792\n",
      "Epoch 94: loss = 2.6854514656234816\n",
      "points length:  2855\n",
      "Epoch 95: loss = 2.336335630649702\n",
      "points length:  3007\n",
      "Epoch 96: loss = 2.545286720944231\n",
      "points length:  2927\n",
      "Epoch 97: loss = 1.595053152400729\n",
      "points length:  2937\n",
      "Epoch 98: loss = 2.367563666258248\n",
      "points length:  2812\n",
      "Epoch 99: loss = 2.336843973854808\n",
      "points length:  2998\n",
      "Epoch 100: loss = 1.907416413091486\n",
      "Epoch 100: loss = 1.907416413091486\n",
      "Best Epoch 68: Best loss = 1.1878529002739615\n",
      "points length:  2854\n",
      "Epoch 101: loss = 1.4289402848939676\n",
      "points length:  2971\n",
      "Epoch 102: loss = 1.6068249591188966\n",
      "points length:  3070\n",
      "Epoch 103: loss = 1.6625450168742297\n",
      "points length:  3219\n",
      "Epoch 104: loss = 1.5441295919473996\n",
      "points length:  3023\n",
      "Epoch 105: loss = 1.5417198109229862\n",
      "points length:  3177\n",
      "Epoch 106: loss = 1.861279104519895\n",
      "points length:  3038\n",
      "Epoch 107: loss = 1.5671395412987166\n",
      "points length:  3002\n",
      "Epoch 108: loss = 1.512690708866938\n",
      "points length:  3036\n",
      "Epoch 109: loss = 1.3827442062269393\n",
      "points length:  3008\n",
      "Epoch 110: loss = 1.421961434947835\n",
      "points length:  2969\n",
      "Epoch 111: loss = 1.4476799058148717\n",
      "points length:  3052\n",
      "Epoch 112: loss = 1.4275725315944723\n",
      "points length:  2864\n",
      "Epoch 113: loss = 1.263670823461153\n",
      "points length:  3235\n",
      "Epoch 114: loss = 1.4338491565961737\n",
      "points length:  3115\n",
      "Epoch 115: loss = 1.994714227705356\n",
      "points length:  3487\n",
      "Epoch 116: loss = 2.0858038288573835\n",
      "points length:  3296\n",
      "Epoch 117: loss = 1.9149185275711929\n",
      "points length:  3607\n",
      "Epoch 118: loss = 3.1623180000650697\n",
      "points length:  3236\n",
      "Epoch 119: loss = 1.3991147002875928\n",
      "points length:  3533\n",
      "Epoch 120: loss = 1.671047094621913\n",
      "points length:  3468\n",
      "Epoch 121: loss = 1.6346457334696913\n",
      "points length:  3580\n",
      "Epoch 122: loss = 1.470689809736705\n",
      "points length:  3398\n",
      "Epoch 123: loss = 1.9105084004847779\n",
      "points length:  3274\n",
      "Epoch 124: loss = 1.608244642971372\n",
      "points length:  3513\n",
      "Epoch 125: loss = 2.5459874469003063\n",
      "points length:  3418\n",
      "Epoch 126: loss = 2.904292128299129\n",
      "points length:  3555\n",
      "Epoch 127: loss = 1.7391699142396184\n",
      "points length:  3373\n",
      "Epoch 128: loss = 2.195491648403893\n",
      "points length:  3487\n",
      "Epoch 129: loss = 2.3375562909468584\n",
      "points length:  3473\n",
      "Epoch 130: loss = 2.226618400934288\n",
      "points length:  3502\n",
      "Epoch 131: loss = 2.07219560949222\n",
      "points length:  3537\n",
      "Epoch 132: loss = 1.7528600821319933\n",
      "points length:  3596\n",
      "Epoch 133: loss = 2.2973408315643633\n",
      "points length:  3741\n",
      "Epoch 134: loss = 1.7631142443870917\n",
      "points length:  3604\n",
      "Epoch 135: loss = 1.6220255149088596\n",
      "points length:  3751\n",
      "Epoch 136: loss = 1.763400869946863\n",
      "points length:  3820\n",
      "Epoch 137: loss = 1.6294628294371785\n",
      "points length:  3831\n",
      "Epoch 138: loss = 7.279403544478424\n",
      "points length:  3641\n",
      "Epoch 139: loss = 1.7480882145051075\n",
      "points length:  3698\n",
      "Epoch 140: loss = 1.7586801019892386\n",
      "points length:  3309\n",
      "Epoch 141: loss = 1.7327748377189052\n",
      "points length:  3945\n",
      "Epoch 142: loss = 1.9718218993158767\n",
      "points length:  3089\n",
      "Epoch 143: loss = 1.4021981907091485\n",
      "points length:  4285\n",
      "Epoch 144: loss = 3.7964593597234466\n",
      "points length:  3161\n",
      "Epoch 145: loss = 1.888650385291409\n",
      "points length:  4060\n",
      "Epoch 146: loss = 4.538360032261662\n",
      "points length:  3401\n",
      "Epoch 147: loss = 2.4841114959730044\n",
      "points length:  3642\n",
      "Epoch 148: loss = 2.497604776443081\n",
      "points length:  3686\n",
      "Epoch 149: loss = 2.0320766372145984\n",
      "points length:  3715\n",
      "Epoch 150: loss = 2.1361262210789183\n",
      "Epoch 150: loss = 2.1361262210789183\n",
      "Best Epoch 68: Best loss = 1.1878529002739615\n",
      "points length:  3587\n",
      "Epoch 151: loss = 3.9535014240605593\n",
      "points length:  3566\n",
      "Epoch 152: loss = 2.1921092757491105\n",
      "points length:  3646\n",
      "Epoch 153: loss = 2.0512293706790317\n",
      "points length:  3657\n",
      "Epoch 154: loss = 3.1570538047426453\n",
      "points length:  3567\n",
      "Epoch 155: loss = 2.3185172783998507\n",
      "points length:  3519\n",
      "Epoch 156: loss = 1.6655121111467968\n",
      "points length:  3777\n",
      "Epoch 157: loss = 1.723901170295739\n",
      "points length:  3713\n",
      "Epoch 158: loss = 2.964687123348024\n",
      "points length:  3661\n",
      "Epoch 159: loss = 2.132294765452123\n",
      "points length:  3508\n",
      "Epoch 160: loss = 2.5701702298770925\n",
      "points length:  3606\n",
      "Epoch 161: loss = 2.009453335059623\n",
      "points length:  3657\n",
      "Epoch 162: loss = 2.1004867497223496\n",
      "points length:  3514\n",
      "Epoch 163: loss = 2.6640200034017147\n",
      "points length:  3494\n",
      "Epoch 164: loss = 2.3768278475170606\n",
      "points length:  3608\n",
      "Epoch 165: loss = 21.863832308461987\n",
      "points length:  3375\n",
      "Epoch 166: loss = 1.8126094200077805\n",
      "points length:  3408\n",
      "Epoch 167: loss = 2.348200312449373\n",
      "points length:  3470\n",
      "Epoch 168: loss = 1.9021492712760766\n",
      "points length:  3691\n",
      "Epoch 169: loss = 2.124046966245262\n",
      "points length:  3561\n",
      "Epoch 170: loss = 2.9620298610003863\n",
      "points length:  3891\n",
      "Epoch 171: loss = 2.0131501884661094\n",
      "points length:  3375\n",
      "Epoch 172: loss = 2.1165909079828737\n",
      "points length:  3874\n",
      "Epoch 173: loss = 2.487231387774869\n",
      "points length:  3121\n",
      "Epoch 174: loss = 2.0193823481753284\n",
      "points length:  4225\n",
      "Epoch 175: loss = 2.1672871253725234\n",
      "points length:  3179\n",
      "Epoch 176: loss = 2.0292407168731854\n",
      "points length:  4107\n",
      "Epoch 177: loss = 3.5560216453723976\n",
      "points length:  3485\n",
      "Epoch 178: loss = 2.222800240610357\n",
      "points length:  3913\n",
      "Epoch 179: loss = 2.635012532747749\n",
      "points length:  3607\n",
      "Epoch 180: loss = 4.669125272610146\n",
      "points length:  3881\n",
      "Epoch 181: loss = 2.6043585231850837\n",
      "points length:  3794\n",
      "Epoch 182: loss = 3.426493898976554\n",
      "points length:  4019\n",
      "Epoch 183: loss = 2.7491740489114997\n",
      "points length:  3919\n",
      "Epoch 184: loss = 2.0352941884185394\n",
      "points length:  3823\n",
      "Epoch 185: loss = 3.0309578626961886\n",
      "points length:  3751\n",
      "Epoch 186: loss = 1.7086503030053253\n",
      "points length:  3762\n",
      "Epoch 187: loss = 2.145169520208277\n",
      "points length:  3726\n",
      "Epoch 188: loss = 2.773362916220356\n",
      "points length:  3745\n",
      "Epoch 189: loss = 1.742455575372056\n",
      "points length:  3745\n",
      "Epoch 190: loss = 2.3444704585645515\n",
      "points length:  3560\n",
      "Epoch 191: loss = 2.7801583461482404\n",
      "points length:  3640\n",
      "Epoch 192: loss = 3.236712148841217\n",
      "points length:  3714\n",
      "Epoch 193: loss = 2.249175784292085\n",
      "points length:  3530\n",
      "Epoch 194: loss = 2.357748416788785\n",
      "points length:  3767\n",
      "Epoch 195: loss = 2.585313879026316\n",
      "points length:  3772\n",
      "Epoch 196: loss = 2.714506619654644\n",
      "points length:  3756\n",
      "Epoch 197: loss = 4.0238439806850845\n",
      "points length:  3595\n",
      "Epoch 198: loss = 1.8306967737811428\n",
      "points length:  3508\n",
      "Epoch 199: loss = 2.1844612158578975\n",
      "points length:  3245\n",
      "Epoch 200: loss = 1.672431703649153\n",
      "Epoch 200: loss = 1.672431703649153\n",
      "Best Epoch 68: Best loss = 1.1878529002739615\n",
      "points length:  3128\n",
      "Epoch 201: loss = 14.779697933806077\n",
      "points length:  3397\n",
      "Epoch 202: loss = 1.8769094699859312\n",
      "points length:  3541\n",
      "Epoch 203: loss = 1.81070023257269\n",
      "points length:  3539\n",
      "Epoch 204: loss = 1.782394246822625\n",
      "points length:  3595\n",
      "Epoch 205: loss = 1.8275140126658935\n",
      "points length:  3502\n",
      "Epoch 206: loss = 1.8671027062770211\n",
      "points length:  3745\n",
      "Epoch 207: loss = 1.9251665153500461\n",
      "points length:  3292\n",
      "Epoch 208: loss = 3.1047129692645377\n",
      "points length:  3607\n",
      "Epoch 209: loss = 3.3680043261863704\n",
      "points length:  2921\n",
      "Epoch 210: loss = 4.352193871565773\n",
      "points length:  3918\n",
      "Epoch 211: loss = 12.951656965465379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     sites \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(sites)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     sites \u001b[38;5;241m=\u001b[39m \u001b[43mautograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     sites_np \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(site_file_path, sites_np)\n",
      "Cell \u001b[0;32mIn[22], line 83\u001b[0m, in \u001b[0;36mautograd\u001b[0;34m(sites, model, max_iter, stop_train_threshold, upsampling, lambda_weights)\u001b[0m\n\u001b[1;32m     80\u001b[0m loss_values\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_weights = [0.2,0,0.1,0,1.10,0.1,0,2]\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lamda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_target_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 500\n",
    "\n",
    "site_file_path = f'{destination}{max_iter}3d_sites_chamfer{lamda_chamfer}.npy'\n",
    "#check if optimized sites file exists\n",
    "if os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)    \n",
    "else:\n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.init()\n",
    "#for i in range(int(100/10)):\n",
    "epoch = 200\n",
    "\n",
    "model_file_path = f'{destination}{max_iter}_{epoch}_3d_model_chamfer{lamda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{max_iter}_{epoch}_3d_sites _{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "sites = torch.load(site_file_path)\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#^\n",
    "ps_cloud = ps.register_point_cloud(\"a\",sites_np)\n",
    "\n",
    "ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
