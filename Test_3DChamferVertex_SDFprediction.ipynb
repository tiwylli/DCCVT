{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import interactive_polyscope\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import trimesh\n",
    "from scipy.spatial import Delaunay, Voronoi\n",
    "\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.03\n",
    "lr_model = 0.0003\n",
    "iterations = 5000\n",
    "save_every = 100\n",
    "max_iter = 100\n",
    "#learning_rate = 0.03\n",
    "destination = \"./images/autograd/3D/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently sites are between -5 and 5 in all 3 dimensions\n",
    "# check if sites exists\n",
    "#num_centroids = 16*16*16\n",
    "num_centroids = 8*8*8\n",
    "site_fp = f'sites_{num_centroids}.pt'\n",
    "\n",
    "if os.path.exists(site_fp):\n",
    "    sites = torch.load(site_fp)\n",
    "else:\n",
    "    sites = su.createCVTgrid(num_centroids=num_centroids, dimensionality=input_dims)\n",
    "    #save the initial sites torch tensor\n",
    "    torch.save(sites, site_fp)\n",
    "\n",
    "\n",
    "def plot_voronoi_3d(sites, xlim=5, ylim=5, zlim=5):\n",
    "    import numpy as np\n",
    "    import pyvoro\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "    # initialize random number generator\n",
    "    rng = np.random.default_rng(11)\n",
    "    # create a set of points in 3D\n",
    "    points = sites.detach().cpu().numpy()\n",
    "\n",
    "    # use pyvoro to compute the Voronoi tessellation\n",
    "    # the second argument gives the the axis limits in x,y and z direction\n",
    "    # in this case all between 0 and 1.\n",
    "    # the third argument gives \"dispersion = max distance between two points\n",
    "    # that might be adjacent\" (not sure how exactly this works)\n",
    "    voronoi = pyvoro.compute_voronoi(points,[[-xlim,xlim],[-ylim,ylim],[-zlim,zlim]],1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # for each Voronoi cell, plot all the faces of the corresponding polygon\n",
    "    for vnoicell in voronoi:\n",
    "        faces = []\n",
    "        # the vertices are the corner points of the Voronoi cell\n",
    "        vertices = np.array(vnoicell['vertices'])\n",
    "        # cycle through all faces of the polygon\n",
    "        for face in vnoicell['faces']:\n",
    "            faces.append(vertices[np.array(face['vertices'])])\n",
    "            \n",
    "        # join the faces into a 3D polygon\n",
    "        polygon = Poly3DCollection(faces, alpha=0.5, \n",
    "                                facecolors=rng.uniform(0,1,3),\n",
    "                                linewidths=0.5,edgecolors='black')\n",
    "        ax.add_collection3d(polygon)\n",
    "    \n",
    "    ax.set_xlim([-xlim,xlim])\n",
    "    ax.set_ylim([-ylim,ylim])\n",
    "    ax.set_zlim([-zlim,zlim])\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "#plot_voronoi_3d(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.init()\n",
    "#ps_cloud = ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the mesh\n",
    "mesh = [\"bunny\", \"Resources/stanford-bunny.obj\"]\n",
    "bunny = trimesh.load(mesh[1])\n",
    "\n",
    "# Step 1: Get current bounding box\n",
    "min_bound = bunny.bounds[0]  # Min (x, y, z)\n",
    "max_bound = bunny.bounds[1]  # Max (x, y, z)\n",
    "\n",
    "# Step 2: Compute scale factor\n",
    "current_size = max_bound - min_bound  # Size in each dimension\n",
    "target_size = 8  # Because we want [-5, 5], the total size is 10\n",
    "\n",
    "scale_factor = target_size / np.max(current_size)  # Scale based on the largest dimension\n",
    "\n",
    "# Step 3: Compute new center after scaling\n",
    "new_vertices = bunny.vertices * scale_factor  # Scale the vertices\n",
    "new_min = np.min(new_vertices, axis=0)\n",
    "new_max = np.max(new_vertices, axis=0)\n",
    "new_center = (new_min + new_max) / 2  # New center after scaling\n",
    "\n",
    "# Step 4: Compute translation to center the bunny at (0,0,0)\n",
    "translation = -new_center  # Move to the origin\n",
    "\n",
    "# Step 5: Apply transformation (scaling + translation)\n",
    "bunny.vertices = new_vertices + translation\n",
    "\n",
    "target_points = bunny.sample(16*16*16)\n",
    "target_points = torch.tensor(target_points, device=device)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"Target_points\",target_points.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "\n",
    "model = mlp.Decoder(multires=multires, input_dims=input_dims).to(device)\n",
    "model_path = 'models_resources/pretrained_sphere_small.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model')\n",
    "else:\n",
    "    print(\"no model found, pretraining\")\n",
    "    model.pre_train_sphere(3000)\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "    \n",
    "def polyscope_sdf(model):\n",
    "    # Render the SDF as an implicit surface (zero-level set)\n",
    "    def model_sdf(pts):\n",
    "        pts_tensor = torch.tensor(pts, dtype=torch.float64, device=device)\n",
    "        sdf_values = model(pts_tensor)\n",
    "        sdf_values_np = sdf_values.detach().cpu().numpy().flatten()  # Convert to NumPy\n",
    "        \n",
    "        return sdf_values_np\n",
    "\n",
    "    ps.render_implicit_surface(\"SDF Surface\", model_sdf, mode=\"sphere_march\", enabled=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo this should also work in 2d, might want to adjust the simplex iteration\n",
    "def get_delaunay_neighbors_list_3d_and_all_voronoi_vertices_index(sites):\n",
    "    # Detach and convert to NumPy for Delaunay triangulation\n",
    "    points_np = sites.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute the Delaunay tessellation\n",
    "    tri = Delaunay(points_np)\n",
    "    vor = Voronoi(points_np)\n",
    "    \n",
    "    # # Find the neighbors of each point\n",
    "    neighbors = {i: set() for i in range(len(points_np))}\n",
    "    for simplex in tri.simplices:  # Each simplex is a tetrahedron (4 points)\n",
    "        for i in range(4):  # Iterate over each of the 4 vertices\n",
    "            for j in range(i + 1, 4):  # Connect each pair of points\n",
    "                neighbors[simplex[i]].add(simplex[j])\n",
    "                neighbors[simplex[j]].add(simplex[i])\n",
    "\n",
    "    # Convert sets to lists for easier use\n",
    "    neighbors = {key: list(value) for key, value in neighbors.items()}\n",
    "    \n",
    "    n2 = vor.ridge_points    \n",
    "    voronoi_vertices_sites = tri.simplices\n",
    "    \n",
    "    #todo remove neighbors\n",
    "    \n",
    "    return neighbors, n2, voronoi_vertices_sites\n",
    "\n",
    "#todo deprecate because im dumb\n",
    "def compute_vertices_index_3d(neighbors):\n",
    "    vertices_index_to_compute = []\n",
    "    for site, adjacents in neighbors.items():\n",
    "        for i in adjacents:\n",
    "            for n in adjacents:\n",
    "                if n != site and n != i and n in neighbors[i]:  # Ensuring a triangle exists\n",
    "                    for m in adjacents:\n",
    "                        if m != site and m != i and m != n and m in neighbors[i] and m in neighbors[n]:\n",
    "                            # We now have 4 mutually connected points forming a tetrahedron\n",
    "                            vertices_index_to_compute.append([i, site, n, m])\n",
    "    \n",
    "    # Deduplicate tetrahedra (avoid different orderings of the same set)\n",
    "    seen_tetrahedra = set()\n",
    "    filtered_tetrahedra = []\n",
    "\n",
    "    for tetrahedron in vertices_index_to_compute:\n",
    "        canonical_tetrahedron = tuple(sorted(tetrahedron, key=str))  # Sort for uniqueness\n",
    "        if canonical_tetrahedron not in seen_tetrahedra:\n",
    "            seen_tetrahedra.add(canonical_tetrahedron)\n",
    "            filtered_tetrahedra.append(tetrahedron)\n",
    "    \n",
    "    return filtered_tetrahedra\n",
    "\n",
    "\n",
    "def compute_zero_crossing_vertices_3d(sites, model):\n",
    "    \"\"\"\n",
    "    Computes the indices of the sites composing vertices where neighboring sites have opposite or zero SDF values.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, D) tensor of site positions.\n",
    "        model (callable): Function or neural network that computes SDF values.\n",
    "\n",
    "    Returns:\n",
    "        zero_crossing_vertices_index (list of triplets): List of sites indices (si, sj, sk) where atleast 2 sites have opposing SDF signs.\n",
    "    \"\"\"\n",
    "    # Compute Delaunay neighbors\n",
    "    neighbors, n2, all_tetrahedra = get_delaunay_neighbors_list_3d_and_all_voronoi_vertices_index(sites)\n",
    "\n",
    "    # Compute SDF values for all sites\n",
    "    sdf_values = model(sites)  # Assuming model outputs (N, 1) or (N,) tensor\n",
    "\n",
    "\n",
    "    #todo, probably using voronoi.ridgeopints is faster\n",
    "    # Find pairs of neighbors with opposing SDF values\n",
    "    zero_crossing_pairs = set()\n",
    "    for i, adjacents in neighbors.items():\n",
    "        for j in adjacents:\n",
    "            if i < j:  # Avoid duplicates\n",
    "                sdf_i, sdf_j = sdf_values[i].item(), sdf_values[j].item()\n",
    "                if sdf_i * sdf_j <= 0:  # Opposing signs or one is zero\n",
    "                    zero_crossing_pairs.add((i, j))\n",
    "   \n",
    "    zero_crossing_vertices_index = []\n",
    "    \n",
    "    for tetrahedron in all_tetrahedra:\n",
    "        tetrahedron_pairs = {\n",
    "            (tetrahedron[0], tetrahedron[1]), (tetrahedron[0], tetrahedron[2]), (tetrahedron[0], tetrahedron[3]),\n",
    "            (tetrahedron[1], tetrahedron[2]), (tetrahedron[1], tetrahedron[3]), (tetrahedron[2], tetrahedron[3])\n",
    "        }\n",
    "        if any(pair in zero_crossing_pairs for pair in tetrahedron_pairs):\n",
    "            zero_crossing_vertices_index.append(tetrahedron)\n",
    "\n",
    "\n",
    "    print(\"zero_crossing_vertices_index_LIST\", len(zero_crossing_vertices_index))\n",
    "    print(\"zero_crossing_pairs_bisectors_LIST\", len(zero_crossing_pairs))\n",
    "    zero_crossing_pairs = list(zero_crossing_pairs)\n",
    "    zero_crossing_pairs = torch.tensor(np.array(zero_crossing_pairs), device=device)\n",
    "    \n",
    "\n",
    "\n",
    "    #todo fix this as its the good way to do it\n",
    "    # neighbors = torch.tensor(np.array(n2), device=device)\n",
    "    # all_tetrahedra = torch.tensor(np.array(all_tetrahedra), device=device)\n",
    "    \n",
    "    # # Extract the SDF values for each site in the pair\n",
    "    # sdf_i = sdf_values[neighbors[:, 0]]  # First site in each pair\n",
    "    # sdf_j = sdf_values[neighbors[:, 1]]  # Second site in each pair\n",
    "    # # Find the indices where SDF values have opposing signs or one is zero\n",
    "    # mask = (sdf_i * sdf_j <= 0).squeeze()\n",
    "    # zero_crossing_pairs = neighbors[mask]\n",
    "\n",
    "######################################### Todo HERE #########################################\n",
    "    # # Generate all unique edges for each tetrahedron\n",
    "    # tetrahedron_edges = torch.stack([\n",
    "    #     all_tetrahedra[:, [0, 1]], all_tetrahedra[:, [0, 2]], all_tetrahedra[:, [0, 3]],\n",
    "    #     all_tetrahedra[:, [1, 2]], all_tetrahedra[:, [1, 3]], all_tetrahedra[:, [2, 3]]\n",
    "    # ], dim=1)  # Shape: (T, 6, 2)\n",
    "\n",
    "    # # Sort each edge to ensure consistent ordering\n",
    "    # tetrahedron_edges = torch.sort(tetrahedron_edges, dim=2)[0]  # Shape: (T, 6, 2)\n",
    "    # zero_crossing_pairs = torch.sort(zero_crossing_pairs, dim=1)[0]  # Shape: (Z, 2)\n",
    "\n",
    "    # # Efficient membership check using `torch.isin`\n",
    "    # tetrahedron_edges_flat = tetrahedron_edges.view(-1, 2)  # Flatten to (6T, 2)\n",
    "    # matches = torch.isin(tetrahedron_edges_flat, zero_crossing_pairs).all(dim=1)  # (6T,)\n",
    "\n",
    "    # # Reshape back to (T, 6) and check if any edge in a tetrahedron has a match\n",
    "    # match_per_tetrahedron = matches.view(-1, 6).any(dim=1)  # (T,)\n",
    "\n",
    "    # # Select tetrahedra that contain at least one zero-crossing edge\n",
    "    # zero_crossing_vertices_index = all_tetrahedra[match_per_tetrahedron]\n",
    "\n",
    "    ################################################################################\n",
    "    \n",
    "    print(\"zero_crossing_vertices_index_TORCH\",zero_crossing_vertices_index.shape)\n",
    "\n",
    "\n",
    "    #print(\"zero_crossing_pairs_TORCH\", zero_crossing_pairs.shape)\n",
    "    \n",
    "    \n",
    "    return zero_crossing_vertices_index, zero_crossing_pairs\n",
    "\n",
    "# def compute_vertex_3d(s_i, s_j, s_k, s_l):\n",
    "#     \"\"\"\n",
    "#     Computes the circumcenter of a tetrahedron given four sites in 3D.\n",
    "\n",
    "#     Args:\n",
    "#         s_i, s_j, s_k, s_l (torch.Tensor): 3D coordinates of four sites (shape: (3,)).\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: The circumcenter (x, y, z).\n",
    "#     \"\"\"\n",
    "#     # Convert to coordinate form\n",
    "#     x_i, y_i, z_i = s_i\n",
    "#     x_j, y_j, z_j = s_j\n",
    "#     x_k, y_k, z_k = s_k\n",
    "#     x_l, y_l, z_l = s_l\n",
    "\n",
    "#     # Compute squared norms\n",
    "#     s_i2 = x_i**2 + y_i**2 + z_i**2\n",
    "#     s_j2 = x_j**2 + y_j**2 + z_j**2\n",
    "#     s_k2 = x_k**2 + y_k**2 + z_k**2\n",
    "#     s_l2 = x_l**2 + y_l**2 + z_l**2\n",
    "\n",
    "#     # Construct matrix system\n",
    "#     A = torch.tensor([\n",
    "#         [x_i, y_i, z_i, 1],\n",
    "#         [x_j, y_j, z_j, 1],\n",
    "#         [x_k, y_k, z_k, 1],\n",
    "#         [x_l, y_l, z_l, 1]\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "#     Dx = torch.tensor([\n",
    "#         [s_i2, y_i, z_i, 1],\n",
    "#         [s_j2, y_j, z_j, 1],\n",
    "#         [s_k2, y_k, z_k, 1],\n",
    "#         [s_l2, y_l, z_l, 1]\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "#     Dy = torch.tensor([\n",
    "#         [x_i, s_i2, z_i, 1],\n",
    "#         [x_j, s_j2, z_j, 1],\n",
    "#         [x_k, s_k2, z_k, 1],\n",
    "#         [x_l, s_l2, z_l, 1]\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "#     Dz = torch.tensor([\n",
    "#         [x_i, y_i, s_i2, 1],\n",
    "#         [x_j, y_j, s_j2, 1],\n",
    "#         [x_k, y_k, s_k2, 1],\n",
    "#         [x_l, y_l, s_l2, 1]\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "#     # Compute determinants\n",
    "#     detA = torch.det(A)\n",
    "#     detDx = torch.det(Dx)\n",
    "#     detDy = torch.det(Dy)  # todo, removed Negative due to orientation\n",
    "#     detDz = torch.det(Dz)\n",
    "\n",
    "#     # Compute circumcenter coordinates\n",
    "#     x = 0.5 * (detDx / detA)\n",
    "#     y = 0.5 * (detDy / detA)\n",
    "#     z = 0.5 * (detDz / detA)\n",
    "\n",
    "#     #todo -y is weird but always gives the right answer when comparing with vor.vertices\n",
    "#     #maybe error in matrix computation\n",
    "#     return torch.stack([x, y, z])\n",
    "\n",
    "# def compute_all_vertices_3d(sites, vertices_to_compute):\n",
    "#     \"\"\"\n",
    "#     Computes all Voronoi vertices for a given set of tetrahedra.\n",
    "\n",
    "#     Args:\n",
    "#         sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "#         vertices_to_compute (list of quadruplets): List of indices forming tetrahedra.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Computed Voronoi vertices.\n",
    "#     \"\"\"\n",
    "#     vertices = []\n",
    "    \n",
    "#     for quadruplet in vertices_to_compute:\n",
    "#         si = sites[quadruplet[0]]\n",
    "#         sj = sites[quadruplet[1]]\n",
    "#         sk = sites[quadruplet[2]]\n",
    "#         sl = sites[quadruplet[3]]\n",
    "\n",
    "#         # Compute circumcenter for the tetrahedron\n",
    "#         v = compute_vertex_3d(si, sj, sk, sl)\n",
    "#         vertices.append(v)\n",
    "\n",
    "#     # Stack all vertices into a single tensor\n",
    "#     vertices = torch.stack(vertices)\n",
    "#     return vertices\n",
    "\n",
    "def compute_vertices_3d_vectorized(sites, vertices_to_compute):\n",
    "    \"\"\"\n",
    "    Computes the circumcenters of multiple tetrahedra in a vectorized manner.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "        vertices_to_compute (torch.Tensor): (M, 4) tensor of indices forming tetrahedra.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (M, 3) tensor of computed Voronoi vertices.\n",
    "    \"\"\"\n",
    "    # Extract tetrahedra site coordinates in a batched manner\n",
    "    tetrahedra = sites[vertices_to_compute]  # Shape: (M, 4, 3)\n",
    "\n",
    "    # Compute squared norms of each point\n",
    "    squared_norms = (tetrahedra ** 2).sum(dim=2, keepdim=True)  # Shape: (M, 4, 1)\n",
    "\n",
    "    # Construct the 4x4 matrices in batch\n",
    "    ones_col = torch.ones_like(squared_norms)  # Column of ones for homogeneous coordinates\n",
    "\n",
    "    A = torch.cat([tetrahedra, ones_col], dim=2)  # Shape: (M, 4, 4)\n",
    "    Dx = torch.cat([squared_norms, tetrahedra[:, :, 1:], ones_col], dim=2)\n",
    "    Dy = torch.cat([tetrahedra[:, :, :1], squared_norms, tetrahedra[:, :, 2:], ones_col], dim=2)\n",
    "    Dz = torch.cat([tetrahedra[:, :, :2], squared_norms, ones_col], dim=2)\n",
    "\n",
    "    # Compute determinants in batch\n",
    "    detA = torch.linalg.det(A)  # Shape: (M,)\n",
    "    detDx = torch.linalg.det(Dx)\n",
    "    detDy = torch.linalg.det(Dy)  # todo, removed Negative due to orientation\n",
    "    detDz = torch.linalg.det(Dz)\n",
    "\n",
    "    # Compute circumcenters\n",
    "    circumcenters = 0.5 * torch.stack([detDx / detA, detDy / detA, detDz / detA], dim=1)\n",
    "\n",
    "    return circumcenters  # Shape: (M, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Todo its the same in 3d if we only want a point. Do we want the plane ?\n",
    "# def compute_all_bisectors_3d(sites, bisectors_to_compute):\n",
    "#     # Initialize an empty tensor for storing bisectors\n",
    "#     bisectors = []\n",
    "    \n",
    "#     for pairs in bisectors_to_compute:\n",
    "#         si = sites[pairs[0]]\n",
    "#         sj = sites[pairs[1]]\n",
    "#         b = (si + sj) / 2\n",
    "#         bisectors.append(b)\n",
    "\n",
    "#     # Stack the list of bisectors into a single tensor for easier gradient tracking\n",
    "#     bisectors = torch.stack(bisectors)\n",
    "#     return bisectors\n",
    "\n",
    "\n",
    "def compute_all_bisectors_3d_vectorized(sites, bisectors_to_compute):\n",
    "    \"\"\"\n",
    "    Computes the bisector points for given pairs of sites in 3D.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "        bisectors_to_compute (torch.Tensor): (M, 2) tensor of index pairs.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (M, 3) tensor of computed bisector points.\n",
    "    \"\"\"\n",
    "    # Extract all site pairs at once\n",
    "    si = sites[bisectors_to_compute[:, 0]]  # Shape: (M, 3)\n",
    "    sj = sites[bisectors_to_compute[:, 1]]  # Shape: (M, 3)\n",
    "\n",
    "    # Compute bisectors in a single vectorized operation\n",
    "    bisectors = (si + sj) / 2  # Shape: (M, 3)\n",
    "\n",
    "    return bisectors\n",
    "\n",
    "\n",
    "#Todo see above\n",
    "# def compute_all_bisector_planes_3d(sites, bisectors_to_compute):\n",
    "#     \"\"\"\n",
    "#     Computes bisector planes for given site pairs in 3D.\n",
    "\n",
    "#     Args:\n",
    "#         sites (torch.Tensor): (N, 3) tensor of site positions.\n",
    "#         bisectors_to_compute (list of pairs): List of index pairs (i, j).\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Plane normals (N, 3) and midpoints (N, 3).\n",
    "#     \"\"\"\n",
    "#     midpoints = []\n",
    "#     normals = []\n",
    "    \n",
    "#     for pair in bisectors_to_compute:\n",
    "#         si = sites[pair[0]]\n",
    "#         sj = sites[pair[1]]\n",
    "        \n",
    "#         midpoint = (si + sj) / 2  # Midpoint\n",
    "#         normal = sj - si  # Normal direction\n",
    "        \n",
    "#         midpoints.append(midpoint)\n",
    "#         normals.append(normal)\n",
    "\n",
    "#     # Stack into tensors\n",
    "#     midpoints = torch.stack(midpoints)\n",
    "#     normals = torch.stack(normals)\n",
    "    \n",
    "#     return normals, midpoints\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "edge_smoothing_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "zero_target_points_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.5, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_sdf = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    lambda_laplace = lambda_weights[3]\n",
    "    lamda_chamfer = lambda_weights[4]\n",
    "    lamda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    lambda_target_points = lambda_weights[7]\n",
    "    \n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = compute_zero_crossing_vertices_3d(sites, model)\n",
    "        #vertices_to_compute to array before tensor because extremely slow\n",
    "        \n",
    "        #vertices = compute_all_vertices_3d(sites, vertices_to_compute)\n",
    "        vertices = compute_vertices_3d_vectorized(sites, vertices_to_compute)\n",
    "        print(\"vertices length: \",len(vertices))\n",
    "        \n",
    "        bisectors = compute_all_bisectors_3d_vectorized(sites, bisectors_to_compute)\n",
    "\n",
    "        \n",
    "        #combine vertices and bisectors to one tensor for chamfer\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "\n",
    "        print(\"points length: \",len(points))\n",
    "  \n",
    "\n",
    "        # Compute losses       \n",
    "        cvt_loss = lf.compute_cvt_loss(sites)\n",
    "        #min_distance_loss = min_distance_regularization_for_op_sites(edges,sites)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        #edge_smoothing_loss = compute_edge_smoothing_loss(edges, sites, model)\n",
    "        chamfer_loss = lf.chamfer_distance(target_points, points)\n",
    "        eikonal_loss = lf.eikonal(model, input_dimensions=input_dims)\n",
    "        #domain_restriction_loss = lf.domain_restriction(target_points, model)\n",
    "        \n",
    "        sdf_values_target_points = model(target_points)[:,0]\n",
    "        zero_target_points_loss_L2 = torch.mean(sdf_values_target_points**2)\n",
    "        zero_target_points_loss_L1 = torch.mean(torch.abs(model(target_points)[:, 0]))\n",
    "        lambda_1, lambda_2 = 0 , 0.99  # Adjust weights as needed\n",
    "        zero_target_points_loss = lambda_1 * zero_target_points_loss_L1 + lambda_2 * zero_target_points_loss_L2\n",
    "\n",
    "               \n",
    "        # Track raw losses (unweighted)\n",
    "        #cvt_loss_values.append(cvt_loss.item())\n",
    "        #min_distance_loss_values.append(min_distance_loss.item())\n",
    "        #edge_smoothing_loss_values.append(edge_smoothing_loss.item())\n",
    "        chamfer_distance_loss_values.append(chamfer_loss.item())\n",
    "        eikonal_loss_values.append(eikonal_loss.item())\n",
    "        #domain_restriction_loss_values.append(domain_restriction_loss.item())\n",
    "        zero_target_points_loss_values.append(zero_target_points_loss.item())\n",
    "  \n",
    "        loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            #lambda_min_distance * min_distance_loss + \n",
    "            #lambda_laplace * edge_smoothing_loss +\n",
    "            lamda_chamfer * chamfer_loss +\n",
    "            lamda_eikonal * eikonal_loss +\n",
    "            #lambda_domain_restriction * domain_restriction_loss +\n",
    "            lambda_target_points * zero_target_points_loss\n",
    "        )\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            if upsampled > 0:\n",
    "                print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "            print(\"sites length: \",len(sites))\n",
    "            \n",
    "            new_sites = su.upsampling_inside(best_sites, model)\n",
    "            #new_sites = su.adaptive_density_upsampling(best_sites, model)\n",
    "            print(new_sites)\n",
    "            sites = su.add_upsampled_sites(best_sites, new_sites)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            print(\"upsampled sites length: \",len(sites))\n",
    "            \n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            \n",
    "            optimizer = torch.optim.Adam([{'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "                                          {'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        \n",
    "        epoch += 1           \n",
    "        \n",
    "    return best_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_crossing_vertices_index_LIST 181\n",
      "zero_crossing_pairs_bisectors_LIST 137\n",
      "zero_crossing_vertices_index_TORCH torch.Size([565, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([137, 2])\n",
      "vertices length:  565\n",
      "points length:  702\n",
      "Epoch 0: loss = 42.82148415630446\n",
      "Epoch 0: loss = 42.82148415630446\n",
      "Best Epoch 0: Best loss = 42.82148415630446\n",
      "zero_crossing_vertices_index_LIST 282\n",
      "zero_crossing_pairs_bisectors_LIST 206\n",
      "zero_crossing_vertices_index_TORCH torch.Size([820, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([206, 2])\n",
      "vertices length:  820\n",
      "points length:  1026\n",
      "Epoch 1: loss = 42.215307985152194\n",
      "zero_crossing_vertices_index_LIST 385\n",
      "zero_crossing_pairs_bisectors_LIST 283\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1086, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([283, 2])\n",
      "vertices length:  1086\n",
      "points length:  1369\n",
      "Epoch 2: loss = 385.785971327791\n",
      "zero_crossing_vertices_index_LIST 482\n",
      "zero_crossing_pairs_bisectors_LIST 390\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1490, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([390, 2])\n",
      "vertices length:  1490\n",
      "points length:  1880\n",
      "Epoch 3: loss = 32.24956237648711\n",
      "zero_crossing_vertices_index_LIST 631\n",
      "zero_crossing_pairs_bisectors_LIST 511\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1866, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([511, 2])\n",
      "vertices length:  1866\n",
      "points length:  2377\n",
      "Epoch 4: loss = 23.891925752588197\n",
      "zero_crossing_vertices_index_LIST 753\n",
      "zero_crossing_pairs_bisectors_LIST 575\n",
      "zero_crossing_vertices_index_TORCH torch.Size([2064, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([575, 2])\n",
      "vertices length:  2064\n",
      "points length:  2639\n",
      "Epoch 5: loss = 155.43625839930633\n",
      "zero_crossing_vertices_index_LIST 745\n",
      "zero_crossing_pairs_bisectors_LIST 573\n",
      "zero_crossing_vertices_index_TORCH torch.Size([2039, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([573, 2])\n",
      "vertices length:  2039\n",
      "points length:  2612\n",
      "Epoch 6: loss = 7.624691463040298\n",
      "zero_crossing_vertices_index_LIST 615\n",
      "zero_crossing_pairs_bisectors_LIST 505\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1833, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([505, 2])\n",
      "vertices length:  1833\n",
      "points length:  2338\n",
      "Epoch 7: loss = 5.004830008370408\n",
      "zero_crossing_vertices_index_LIST 670\n",
      "zero_crossing_pairs_bisectors_LIST 477\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1737, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([477, 2])\n",
      "vertices length:  1737\n",
      "points length:  2214\n",
      "Epoch 8: loss = 3.9931751561482423\n",
      "zero_crossing_vertices_index_LIST 577\n",
      "zero_crossing_pairs_bisectors_LIST 442\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1611, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([442, 2])\n",
      "vertices length:  1611\n",
      "points length:  2053\n",
      "Epoch 9: loss = 759.3501184183206\n",
      "zero_crossing_vertices_index_LIST 570\n",
      "zero_crossing_pairs_bisectors_LIST 435\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1592, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([435, 2])\n",
      "vertices length:  1592\n",
      "points length:  2027\n",
      "Epoch 10: loss = 11.27040788731903\n",
      "Epoch 10: loss = 11.27040788731903\n",
      "Best Epoch 8: Best loss = 3.9931751561482423\n",
      "zero_crossing_vertices_index_LIST 583\n",
      "zero_crossing_pairs_bisectors_LIST 442\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1611, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([442, 2])\n",
      "vertices length:  1611\n",
      "points length:  2053\n",
      "Epoch 11: loss = 49.20407427942786\n",
      "zero_crossing_vertices_index_LIST 596\n",
      "zero_crossing_pairs_bisectors_LIST 458\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1649, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([458, 2])\n",
      "vertices length:  1649\n",
      "points length:  2107\n",
      "Epoch 12: loss = 115281.97090718056\n",
      "zero_crossing_vertices_index_LIST 617\n",
      "zero_crossing_pairs_bisectors_LIST 476\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1719, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([476, 2])\n",
      "vertices length:  1719\n",
      "points length:  2195\n",
      "Epoch 13: loss = 124.79332727667497\n",
      "zero_crossing_vertices_index_LIST 639\n",
      "zero_crossing_pairs_bisectors_LIST 475\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1705, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([475, 2])\n",
      "vertices length:  1705\n",
      "points length:  2180\n",
      "Epoch 14: loss = 36.65228653228104\n",
      "zero_crossing_vertices_index_LIST 616\n",
      "zero_crossing_pairs_bisectors_LIST 477\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1704, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([477, 2])\n",
      "vertices length:  1704\n",
      "points length:  2181\n",
      "Epoch 15: loss = 34.25067834217057\n",
      "zero_crossing_vertices_index_LIST 613\n",
      "zero_crossing_pairs_bisectors_LIST 481\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1728, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([481, 2])\n",
      "vertices length:  1728\n",
      "points length:  2209\n",
      "Epoch 16: loss = 7.33514423583151\n",
      "zero_crossing_vertices_index_LIST 661\n",
      "zero_crossing_pairs_bisectors_LIST 480\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1741, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([480, 2])\n",
      "vertices length:  1741\n",
      "points length:  2221\n",
      "Epoch 17: loss = 47.418137046590374\n",
      "zero_crossing_vertices_index_LIST 642\n",
      "zero_crossing_pairs_bisectors_LIST 482\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1767, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([482, 2])\n",
      "vertices length:  1767\n",
      "points length:  2249\n",
      "Epoch 18: loss = 34.43902688806487\n",
      "zero_crossing_vertices_index_LIST 658\n",
      "zero_crossing_pairs_bisectors_LIST 490\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1772, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([490, 2])\n",
      "vertices length:  1772\n",
      "points length:  2262\n",
      "Epoch 19: loss = 67787.0064556198\n",
      "zero_crossing_vertices_index_LIST 647\n",
      "zero_crossing_pairs_bisectors_LIST 489\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1770, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([489, 2])\n",
      "vertices length:  1770\n",
      "points length:  2259\n",
      "Epoch 20: loss = 39.550926907397596\n",
      "Epoch 20: loss = 39.550926907397596\n",
      "Best Epoch 8: Best loss = 3.9931751561482423\n",
      "zero_crossing_vertices_index_LIST 672\n",
      "zero_crossing_pairs_bisectors_LIST 502\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1808, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([502, 2])\n",
      "vertices length:  1808\n",
      "points length:  2310\n",
      "Epoch 21: loss = 4.078168943089597\n",
      "zero_crossing_vertices_index_LIST 634\n",
      "zero_crossing_pairs_bisectors_LIST 509\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1816, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([509, 2])\n",
      "vertices length:  1816\n",
      "points length:  2325\n",
      "Epoch 22: loss = 19.02876993516756\n",
      "zero_crossing_vertices_index_LIST 661\n",
      "zero_crossing_pairs_bisectors_LIST 507\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1816, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([507, 2])\n",
      "vertices length:  1816\n",
      "points length:  2323\n",
      "Epoch 23: loss = 17.045622564260913\n",
      "zero_crossing_vertices_index_LIST 686\n",
      "zero_crossing_pairs_bisectors_LIST 520\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1874, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([520, 2])\n",
      "vertices length:  1874\n",
      "points length:  2394\n",
      "Epoch 24: loss = 6042.548601827334\n",
      "zero_crossing_vertices_index_LIST 671\n",
      "zero_crossing_pairs_bisectors_LIST 528\n",
      "zero_crossing_vertices_index_TORCH torch.Size([1886, 4])\n",
      "zero_crossing_pairs_TORCH torch.Size([528, 2])\n",
      "vertices length:  1886\n",
      "points length:  2414\n",
      "Epoch 25: loss = 90.9233922315125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m profiler \u001b[38;5;241m=\u001b[39m cProfile\u001b[38;5;241m.\u001b[39mProfile()\n\u001b[1;32m     25\u001b[0m profiler\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 26\u001b[0m sites \u001b[38;5;241m=\u001b[39m \u001b[43mautograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m profiler\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m     29\u001b[0m stats \u001b[38;5;241m=\u001b[39m pstats\u001b[38;5;241m.\u001b[39mStats(profiler)\u001b[38;5;241m.\u001b[39msort_stats(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[180], line 35\u001b[0m, in \u001b[0;36mautograd\u001b[0;34m(sites, model, max_iter, stop_train_threshold, upsampling, lambda_weights)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_iter:\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m     vertices_to_compute, bisectors_to_compute \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_zero_crossing_vertices_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#vertices_to_compute to array before tensor because extremely slow\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#vertices = compute_all_vertices_3d(sites, vertices_to_compute)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     vertices \u001b[38;5;241m=\u001b[39m compute_vertices_3d_vectorized(sites, vertices_to_compute)\n",
      "Cell \u001b[0;32mIn[179], line 86\u001b[0m, in \u001b[0;36mcompute_zero_crossing_vertices_3d\u001b[0;34m(sites, model)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tetrahedron \u001b[38;5;129;01min\u001b[39;00m all_tetrahedra:\n\u001b[1;32m     82\u001b[0m     tetrahedron_pairs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m1\u001b[39m]), (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m2\u001b[39m]), (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m     84\u001b[0m         (tetrahedron[\u001b[38;5;241m1\u001b[39m], tetrahedron[\u001b[38;5;241m2\u001b[39m]), (tetrahedron[\u001b[38;5;241m1\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m]), (tetrahedron[\u001b[38;5;241m2\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     85\u001b[0m     }\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mzero_crossing_pairs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtetrahedron_pairs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     87\u001b[0m         zero_crossing_vertices_index\u001b[38;5;241m.\u001b[39mappend(tetrahedron)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_crossing_vertices_index_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(zero_crossing_vertices_index))\n",
      "Cell \u001b[0;32mIn[179], line 86\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tetrahedron \u001b[38;5;129;01min\u001b[39;00m all_tetrahedra:\n\u001b[1;32m     82\u001b[0m     tetrahedron_pairs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m1\u001b[39m]), (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m2\u001b[39m]), (tetrahedron[\u001b[38;5;241m0\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m     84\u001b[0m         (tetrahedron[\u001b[38;5;241m1\u001b[39m], tetrahedron[\u001b[38;5;241m2\u001b[39m]), (tetrahedron[\u001b[38;5;241m1\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m]), (tetrahedron[\u001b[38;5;241m2\u001b[39m], tetrahedron[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     85\u001b[0m     }\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(pair \u001b[38;5;129;01min\u001b[39;00m zero_crossing_pairs \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tetrahedron_pairs):\n\u001b[1;32m     87\u001b[0m         zero_crossing_vertices_index\u001b[38;5;241m.\u001b[39mappend(tetrahedron)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_crossing_vertices_index_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(zero_crossing_vertices_index))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_weights = [0.2,0,0.1,0,1.000101101000101,0.1,0,2]\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lamda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_target_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}3d_sites_{num_centroids}_chamfer{lamda_chamfer}.npy'\n",
    "#check if optimized sites file exists\n",
    "if os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)    \n",
    "else:\n",
    "    import cProfile, pstats\n",
    "    import time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "    \n",
    "    profiler.disable()\n",
    "    stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    stats.print_stats()\n",
    "    stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lamda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/3D/bunny100_100_3d_model_13824_chamfer1.00001101000101.pth\n",
      "sites ./images/autograd/3D/bunny100_100_3d_sites_13824_chamfer1.00001101000101.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "sites = torch.load(site_file_path)\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "#ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_crossing_mesh_3d(sites, model):\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    vor = Voronoi(sites_np)  # Compute 3D Voronoi diagram\n",
    "\n",
    "    sdf_values = model(sites)[:, 0].detach().cpu().numpy()  # Compute SDF values\n",
    "\n",
    "    valid_faces = []  # List of polygonal faces\n",
    "    used_vertices = set()  # Set of indices for valid vertices\n",
    "\n",
    "    for (point1, point2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:\n",
    "            continue  # Skip infinite ridges\n",
    "\n",
    "        # Check if SDF changes sign across this ridge\n",
    "        if np.sign(sdf_values[point1]) != np.sign(sdf_values[point2]):\n",
    "            valid_faces.append(ridge_vertices)\n",
    "            used_vertices.update(ridge_vertices)\n",
    "\n",
    "    # **Filter Voronoi vertices**\n",
    "    used_vertices = sorted(used_vertices)  # Keep unique, sorted indices\n",
    "    vertex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(used_vertices)}\n",
    "    filtered_vertices = vor.vertices[used_vertices]\n",
    "\n",
    "    # **Re-index faces to match the new filtered vertex list**\n",
    "    filtered_faces = [[vertex_map[v] for v in face] for face in valid_faces]\n",
    "\n",
    "    return filtered_vertices, filtered_faces\n",
    "\n",
    "\n",
    "\n",
    "final_mesh = get_zero_crossing_mesh_3d(sites, model)\n",
    "\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces\", final_mesh[0], final_mesh[1])\n",
    "ps.register_point_cloud(\"Mesh vertices\", final_mesh[0])\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh of epoch:  0\n",
      "mesh of epoch:  10\n",
      "mesh of epoch:  20\n",
      "mesh of epoch:  30\n",
      "mesh of epoch:  40\n",
      "mesh of epoch:  50\n",
      "mesh of epoch:  60\n",
      "mesh of epoch:  70\n",
      "mesh of epoch:  80\n",
      "mesh of epoch:  90\n",
      "mesh of epoch:  100\n"
     ]
    }
   ],
   "source": [
    "def export_visualisation_3d():\n",
    "    import imageio\n",
    "    img_buffer_mesh = []\n",
    "    img_buffer_model = []\n",
    "    for i in range(int(max_iter/10)+1):\n",
    "        epoch = i*10\n",
    "        site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "        model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lamda_chamfer}.pth'\n",
    "        \n",
    "        print(\"mesh of epoch: \", epoch)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_file_path))\n",
    "    \n",
    "        current_mesh = get_zero_crossing_mesh_3d(torch.load(site_file_path), model)\n",
    "        ps.remove_all_structures()\n",
    "        ps.register_surface_mesh(\"Zero-Crossing faces\", current_mesh[0], current_mesh[1])\n",
    "        ps.register_point_cloud(\"Mesh vertices\", current_mesh[0])\n",
    "        img_buffer_mesh.append(ps.screenshot_to_buffer(transparent_bg=False))\n",
    "        \n",
    "        ps.remove_all_structures()\n",
    "        polyscope_sdf(model)\n",
    "        img_buffer_model.append(ps.screenshot_to_buffer(transparent_bg=False))\n",
    "\n",
    "\n",
    "    imageio.mimsave(f'{destination}{max_iter}_3d_optimization_mesh.gif',img_buffer_mesh, fps=1, duration=1, loop=0)\n",
    "    imageio.mimsave(f'{destination}{max_iter}_3d_optimization_sdf.gif', img_buffer_model, fps=1, duration=1, loop=0)\n",
    "\n",
    "export_visualisation_3d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
