{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "\n",
    "#\n",
    "mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([4096, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 16**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "torch.manual_seed(69)\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489f6099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Octahedral grid points shape:  torch.Size([1728, 3])\n"
     ]
    }
   ],
   "source": [
    "vs = su.octahedral_grid_points(grid=8, domain=(-1.0, 1.0))\n",
    "print(\"Octahedral grid points shape: \", vs.shape)\n",
    "\n",
    "vs = vs + torch.randn_like(vs) * noise_scale\n",
    "\n",
    "# sites = vs.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "# ps_cloud = ps.register_point_cloud(\"vs\",vs.detach().cpu().numpy())\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path=mesh[1] + \".ply\",\n",
    "    n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "    n_samples=10001,  # args.n_iterations,\n",
    "    grid_res=256,  # args.grid_res,\n",
    "    grid_range=1.1,  # args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500,  # args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "    scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,  # args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "    nl=\"sine\",  # args.nl,\n",
    "    encoder_type=\"none\",  # args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "    init_type=\"mfgi\",  # args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "    n_repeat_period=30,  # args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######\n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "# add noise to mnfld_points\n",
    "# mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "\n",
    "\n",
    "# def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "#     Args:\n",
    "#         points: (N, 3) tensor of 3D query points\n",
    "#         center: (3,) tensor specifying the center of the sphere\n",
    "#         radius: float, radius of the sphere\n",
    "\n",
    "#     Returns:\n",
    "#         sdf: (N,) tensor of signed distances\n",
    "#     \"\"\"\n",
    "#     return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "\n",
    "# # generate points on the sphere\n",
    "# mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "# mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "# mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([4096, 3])\n",
      "Allocated: 2.694656 MB, Reserved: 23.068672 MB\n",
      "torch.Size([4096])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 24**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "sdf0 = model(sites)\n",
    "\n",
    "# sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "##sdf0 += torch.randn_like(sdf0) * noise_scale/2\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "\n",
    "\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n",
    "\n",
    "# print(sdf_grad0.shape)\n",
    "# print(sdf_grad0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites shape:  torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, tet_probs = su.get_clipped_mesh_numba(\n",
    "    sites, None, d3dsimplices, True, sdf0, True\n",
    ")\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa1bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def eikonal_loss(grad_sdf: torch.Tensor) -> torch.Tensor:\n",
    "# #     \"\"\"\n",
    "# #     Eikonal regularization loss.\n",
    "\n",
    "# #     Args:\n",
    "# #         grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "# #         variant: 'a' for E1a: ½ mean((||∇φ|| - 1)²)\n",
    "# #     Returns:\n",
    "# #         A scalar tensor containing the eikonal loss.\n",
    "# #     \"\"\"\n",
    "# #     norms = torch.norm(grad_sdf, dim=1)  # (N,)\n",
    "# #     loss = 0.5 * torch.mean((norms**2 - 1.0) ** 2)\n",
    "# #     return loss\n",
    "\n",
    "\n",
    "# def motion_by_mean_curvature_loss(\n",
    "#     sdf: torch.Tensor, grad_sdf: torch.Tensor, sites: torch.Tensor, d3dsimplices: torch.Tensor, factor: float = 1.5\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Motion-by-mean-curvature smoothing loss via a smeared Heaviside function.\n",
    "\n",
    "#     Args:\n",
    "#         sdf: Tensor of shape (N,) containing φ at each site.\n",
    "#         grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "#         epsilon_H: Bandwidth ε_H for the smearing (e.g., 1.5 * average edge length).\n",
    "\n",
    "#     Returns:\n",
    "#         A scalar tensor containing the smoothing loss.\n",
    "#     \"\"\"\n",
    "#     # compute epsion_H from sites and d3dsimplices\n",
    "#     d3d = torch.tensor(d3dsimplices).to(device).detach()  # (M,4)\n",
    "#     comb = torch.combinations(torch.arange(d3d.shape[1], device=device), r=2)  # (6,2)\n",
    "#     edges = d3d[:, comb]  # (M,6,2)\n",
    "#     edges = edges.reshape(-1, 2)  # (M*6,2)\n",
    "#     edges, _ = torch.sort(edges, dim=1)  # sort each row so (a,b) == (b,a)\n",
    "#     unique_edges = torch.unique(edges, dim=0)\n",
    "#     v0, v1 = sites[unique_edges[:, 0]], sites[unique_edges[:, 1]]  # (N,3)\n",
    "#     i, j = unique_edges[:, 0], unique_edges[:, 1]  # (N,3)\n",
    "\n",
    "#     phi = sdf\n",
    "#     sign_mask = phi[i] * phi[j] < 0\n",
    "#     v0, v1 = v0[sign_mask], v1[sign_mask]  # only keep edges with opposite signs\n",
    "#     edge_lengths = torch.norm(v1 - v0, dim=1)  # (N,)\n",
    "#     epsilon_H = factor * torch.mean(edge_lengths)  # Bandwidth for the smeared Heaviside function\n",
    "\n",
    "#     # Compute the derivative of the smeared Heaviside H'(φ)\n",
    "#     mask = torch.abs(phi) <= epsilon_H\n",
    "#     H_prime = torch.zeros_like(phi)\n",
    "#     # H'(φ) = (1/(2ε_H)) * (1 + cos(π φ / ε_H)) for |φ| ≤ ε_H\n",
    "#     H_prime[mask] = (1.0 / (2.0 * epsilon_H)) * (1.0 + torch.cos(np.pi * phi[mask] / epsilon_H))\n",
    "\n",
    "#     # Compute |∇H| = |H'(φ)| * ||∇φ||\n",
    "#     norms = torch.norm(grad_sdf, dim=1)\n",
    "#     magnitude = H_prime * norms\n",
    "\n",
    "#     # Ignore very small contributions (tetrahedra already smooth enough)\n",
    "#     valid = magnitude > 1e-8\n",
    "#     if valid.any():\n",
    "#         return torch.mean(magnitude[valid])\n",
    "#     else:\n",
    "#         return torch.tensor(0.0, device=sdf.device)\n",
    "\n",
    "\n",
    "# def smoothness_loss(sites, phi, d3dsimplices, eps=1e-8, weighted=True):\n",
    "#     \"\"\"\n",
    "#     Compute ∑_{(i,j)} w_ij (φ_i - φ_j)^2 over edges from tetrahedral simplices.\n",
    "\n",
    "#     Args:\n",
    "#         sites:     (N,3) float tensor of point positions.\n",
    "#         phi:       (N,)   float tensor of SDF values at sites.\n",
    "#         simplices: (M,4)  long tensor of tetrahedron indices.\n",
    "#         eps:       small constant to avoid divide-by-zero.\n",
    "#         weighted:  if True, weight edges by 1/dist else w=1.\n",
    "#     Returns:\n",
    "#         scalar smoothing loss.\n",
    "#     \"\"\"\n",
    "#     # 1) extract all simplex edges\n",
    "#     d3d = torch.tensor(d3dsimplices).to(device).detach()  # (M,4)\n",
    "#     comb = torch.combinations(torch.arange(d3d.shape[1], device=sites.device), r=2)  # (6,2)\n",
    "#     edges = d3d[:, comb]  # (M,6,2)\n",
    "#     edges = edges.reshape(-1, 2)  # (M*6,2)\n",
    "#     edges = torch.sort(edges, dim=1)[0]  # sort lexicographically\n",
    "#     edges = torch.unique(edges, dim=0)  # keep unique undirected edges\n",
    "#     i, j = edges[:, 0], edges[:, 1]  # index pairs\n",
    "#     sign_mask = phi[i] * phi[j] < 0\n",
    "\n",
    "#     i, j = i[sign_mask], j[sign_mask]  # only keep edges with opposite signs\n",
    "\n",
    "#     if weighted:\n",
    "#         dij = (sites[i] - sites[j]).norm(dim=1)  # (E,)\n",
    "#         w = 1.0 / (dij + eps)\n",
    "#     else:\n",
    "#         w = 1.0\n",
    "\n",
    "#     diff = phi[i] - phi[j]  # (E,)\n",
    "#     loss = torch.mean(w * diff * diff)\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "            {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "        ]\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.0)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        # if epoch % 2 == 0:\n",
    "        #     optimizer = optimizer_sites\n",
    "        #     clip = False\n",
    "        # else:\n",
    "        #     optimizer = optimizer_sdf\n",
    "        #     clip = True\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "        build_mesh = False\n",
    "        clip = True\n",
    "\n",
    "        v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(\n",
    "            sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "        )\n",
    "\n",
    "        if build_mesh:\n",
    "            triangle_faces = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "            triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "            hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "            chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "        else:\n",
    "            chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "        sites_loss = (\n",
    "            lambda_cvt / 10 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "            # lambda_chamfer * chamfer_loss_points\n",
    "            # + lambda_chamfer / 100 * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        )\n",
    "\n",
    "        # sites_sdf_grads = su.sdf_space_grad_pytorch_diego(\n",
    "        #     sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "        # )\n",
    "\n",
    "        # print norm min max for sites_sdf_grads\n",
    "        # print(\"sites_sdf_grads norm min: \", sites_sdf_grads.norm(dim=1).min().item())\n",
    "        # print(\"sites_sdf_grads norm max: \", sites_sdf_grads.norm(dim=1).max().item())\n",
    "        # print(\"sites_sdf_grads norm mean: \", sites_sdf_grads.norm(dim=1).mean().item())\n",
    "\n",
    "        # motion_loss = (\n",
    "        #     lambda_cvt\n",
    "        #     / 1000\n",
    "        #     * motion_by_mean_curvature_loss(sites_sdf, sites_sdf_grads, sites, d3dsimplices, factor=1.5)\n",
    "        # )\n",
    "        # eik_loss = lambda_cvt / 10 * torch.mean(sites_sdf_grads - 1) ** 2  # * eikonal_loss(sdf_verts_grads)\n",
    "        # print(\"eikonal_loss: \", eik_loss.item(), \"motion_loss: \", motion_loss.item())\n",
    "        # sm_loss = lambda_cvt / 100 * smoothness_loss(sites, sites_sdf, d3dsimplices)\n",
    "        # print(\"smoothness_loss: \", sm_loss.item())\n",
    "        # sdf_loss = eik_loss + motion_loss\n",
    "        # sdf_loss = eik_loss\n",
    "\n",
    "        loss = sites_loss  # + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        if epoch / (max_iter * 0.80) > upsampled / upsampling and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites, simplices=d3dsimplices, model=sites_sdf\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            if f_vect is not None:\n",
    "                ps_mesh = ps.register_surface_mesh(\n",
    "                    f\"{epoch} sdf clipped pmesh\",\n",
    "                    v_vect.detach().cpu().numpy(),\n",
    "                    f_vect,\n",
    "                    back_face_policy=\"identical\",\n",
    "                    enabled=False,\n",
    "                )\n",
    "                ps_mesh.add_vector_quantity(\n",
    "                    f\"{epoch} sdf verts grads\",\n",
    "                    sdf_verts_grads.detach().cpu().numpy(),\n",
    "                    enabled=False,\n",
    "                )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 211.19393920898438\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 1: loss = 208.1784210205078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  4096\n",
      "tensor(0.1250, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1500, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1100, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 34\n",
      "Before upsampling, number of sites: 4096 amount added: 136\n",
      "sites shape AFTER:  torch.Size([4232, 3])\n",
      "sites sdf shape AFTER:  torch.Size([4232])\n",
      "Epoch 2: loss = 206.19044494628906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 3: loss = 205.99549865722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 4: loss = 204.67477416992188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 5: loss = 204.05130004882812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 6: loss = 203.15631103515625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 7: loss = 202.34732055664062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 8: loss = 201.975830078125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 9: loss = 234.60411071777344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 10: loss = 231.1140899658203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 11: loss = 231.11033630371094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 12: loss = 230.3186492919922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 13: loss = 230.39231872558594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 14: loss = 230.57164001464844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 15: loss = 229.158447265625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 16: loss = 228.32794189453125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 17: loss = 220.12869262695312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 18: loss = 259.6341552734375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 19: loss = 259.0496520996094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 20: loss = 257.4958190917969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 21: loss = 257.1712646484375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 22: loss = 256.750244140625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 23: loss = 255.74928283691406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 24: loss = 254.36988830566406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 25: loss = 253.2541961669922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 26: loss = 253.37692260742188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 27: loss = 252.84678649902344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 28: loss = 133.53260803222656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 29: loss = 251.86529541015625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 30: loss = 251.2843780517578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 31: loss = 250.04920959472656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 32: loss = 249.110595703125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 33: loss = 249.00694274902344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 34: loss = 81.14869689941406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 35: loss = 98.37963104248047\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 36: loss = 97.552490234375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 37: loss = 80.96222686767578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 38: loss = 80.65930938720703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 39: loss = 81.68242645263672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 40: loss = 62.09697723388672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 41: loss = 80.97801208496094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 42: loss = 75.696044921875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 43: loss = 77.5431137084961\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 44: loss = 81.66912078857422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 45: loss = 82.53656768798828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 46: loss = 88.80546569824219\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 47: loss = 90.0916748046875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 48: loss = 89.02021789550781\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 49: loss = 88.45508575439453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 50: loss = 88.03268432617188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 51: loss = 88.73760986328125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 52: loss = 87.68577575683594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 53: loss = 88.4196548461914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 54: loss = 68.05696868896484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 55: loss = 81.34912872314453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 56: loss = 70.00180053710938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 57: loss = 67.69495391845703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 58: loss = 66.12741088867188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 59: loss = 65.47748565673828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 60: loss = 63.26616668701172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 61: loss = 64.01392364501953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 62: loss = 64.71492767333984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 63: loss = 65.46235656738281\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 64: loss = 64.74279022216797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 65: loss = 62.8436279296875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 66: loss = 63.49237823486328\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 67: loss = 64.18790435791016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 68: loss = 63.558799743652344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 69: loss = 61.99614334106445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 70: loss = 68.03240966796875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 71: loss = 66.63740539550781\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 72: loss = 64.05223846435547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 73: loss = 67.281494140625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 74: loss = 66.51042175292969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 75: loss = 65.59315490722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 76: loss = 55.616554260253906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 77: loss = 56.065956115722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 78: loss = 54.635562896728516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 79: loss = 55.739234924316406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 80: loss = 54.85725021362305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 81: loss = 55.85865783691406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  4232\n",
      "tensor(0.1203, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1443, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1058, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 81\n",
      "Before upsampling, number of sites: 4232 amount added: 324\n",
      "sites shape AFTER:  torch.Size([4556, 3])\n",
      "sites sdf shape AFTER:  torch.Size([4556])\n",
      "Epoch 82: loss = 45.043277740478516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 83: loss = 41.85727310180664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 84: loss = 44.05126190185547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 85: loss = 51.43499755859375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 86: loss = 43.86701202392578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 87: loss = 48.77466583251953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 88: loss = 47.60421371459961\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 89: loss = 51.676326751708984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 90: loss = 44.282840728759766\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 91: loss = 52.26432800292969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 92: loss = 44.26770782470703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 93: loss = 51.67033767700195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 94: loss = 48.064884185791016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 95: loss = 51.59419250488281\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 96: loss = 51.169517517089844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 97: loss = 52.67596435546875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 98: loss = 51.679264068603516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 99: loss = 49.018409729003906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 100: loss = 55.0246696472168\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 101: loss = 55.32959747314453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 102: loss = 54.60943603515625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 103: loss = 56.67338562011719\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 104: loss = 56.41119384765625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 105: loss = 55.4714241027832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 106: loss = 55.48100280761719\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 107: loss = 53.98278045654297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 108: loss = 53.176082611083984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 109: loss = 50.849632263183594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 110: loss = 52.06251907348633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 111: loss = 52.99707794189453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 112: loss = 50.253177642822266\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 113: loss = 50.473087310791016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 114: loss = 43.807411193847656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 115: loss = 50.0712776184082\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 116: loss = 49.12097930908203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 117: loss = 45.63020324707031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 118: loss = 49.78673553466797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 119: loss = 43.99260330200195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 120: loss = 43.94244384765625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 121: loss = 41.34264373779297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 122: loss = 41.807613372802734\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 123: loss = 37.98130798339844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 124: loss = 44.075889587402344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 125: loss = 39.389556884765625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 126: loss = 38.492332458496094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 127: loss = 35.25987243652344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 128: loss = 36.91032028198242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 129: loss = 36.572418212890625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 130: loss = 34.35157775878906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 131: loss = 33.047019958496094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 132: loss = 34.07435607910156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 133: loss = 32.7223014831543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 134: loss = 34.53860855102539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 135: loss = 35.09914779663086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 136: loss = 39.88758087158203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 137: loss = 41.129825592041016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 138: loss = 38.96235656738281\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 139: loss = 38.373619079589844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 140: loss = 38.9152717590332\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 141: loss = 34.981998443603516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 142: loss = 37.6373176574707\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 143: loss = 37.62730026245117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 144: loss = 32.92498779296875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 145: loss = 31.848491668701172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 146: loss = 36.63786315917969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 147: loss = 36.308189392089844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 148: loss = 37.34708023071289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 149: loss = 38.11168670654297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 150: loss = 39.40357208251953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 151: loss = 39.876895904541016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 152: loss = 40.06010055541992\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 153: loss = 40.97136688232422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 154: loss = 40.68723678588867\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 155: loss = 40.59955596923828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 156: loss = 39.39204788208008\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 157: loss = 37.72036361694336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 158: loss = 39.24345397949219\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 159: loss = 39.77838897705078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 160: loss = 41.08641815185547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 161: loss = 41.32330322265625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  4556\n",
      "tensor(0.1201, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1441, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1057, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 218\n",
      "Before upsampling, number of sites: 4556 amount added: 872\n",
      "sites shape AFTER:  torch.Size([5428, 3])\n",
      "sites sdf shape AFTER:  torch.Size([5428])\n",
      "Epoch 162: loss = 33.83566665649414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 163: loss = 31.857112884521484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 164: loss = 30.389785766601562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 165: loss = 31.596824645996094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 166: loss = 33.71217346191406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 167: loss = 31.063894271850586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 168: loss = 29.44919204711914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 169: loss = 30.088504791259766\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 170: loss = 30.672771453857422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 171: loss = 29.19564437866211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 172: loss = 27.430492401123047\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 173: loss = 27.821706771850586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 174: loss = 24.61962127685547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 175: loss = 21.16366958618164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 176: loss = 19.91081428527832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 177: loss = 23.010696411132812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 178: loss = 20.13349151611328\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 179: loss = 20.015884399414062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 180: loss = 21.061214447021484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 181: loss = 17.77452278137207\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 182: loss = 23.60299301147461\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 183: loss = 21.788482666015625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 184: loss = 21.227752685546875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 185: loss = 25.496623992919922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 186: loss = 22.050140380859375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 187: loss = 23.511798858642578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 188: loss = 23.86416244506836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 189: loss = 24.82452964782715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 190: loss = 24.099075317382812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 191: loss = 23.54139518737793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 192: loss = 21.000123977661133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 193: loss = 20.37049102783203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 194: loss = 20.901153564453125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 195: loss = 21.224206924438477\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 196: loss = 20.390037536621094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 197: loss = 22.476177215576172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 198: loss = 22.940351486206055\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 199: loss = 27.17200469970703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 200: loss = 24.820863723754883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 201: loss = 20.970993041992188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 202: loss = 25.025259017944336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 203: loss = 21.72032928466797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 204: loss = 20.548524856567383\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 205: loss = 23.420578002929688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 206: loss = 22.014827728271484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 207: loss = 21.193531036376953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 208: loss = 22.97275161743164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 209: loss = 19.621387481689453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 210: loss = 17.554912567138672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 211: loss = 19.825653076171875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 212: loss = 18.080970764160156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 213: loss = 19.82484245300293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 214: loss = 21.70403289794922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 215: loss = 22.407791137695312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 216: loss = 21.271656036376953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 217: loss = 20.618457794189453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 218: loss = 19.20296859741211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 219: loss = 18.15670394897461\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 220: loss = 17.371416091918945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 221: loss = 16.08401107788086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 222: loss = 17.313697814941406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 223: loss = 16.5759220123291\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 224: loss = 17.729286193847656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 225: loss = 18.85995101928711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 226: loss = 18.656696319580078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 227: loss = 17.55527687072754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 228: loss = 15.84283447265625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 229: loss = 15.646075248718262\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 230: loss = 17.335590362548828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 231: loss = 15.63608169555664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 232: loss = 16.13223648071289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 233: loss = 18.011816024780273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 234: loss = 17.156169891357422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 235: loss = 17.608739852905273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 236: loss = 14.300883293151855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 237: loss = 13.762979507446289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 238: loss = 14.20724868774414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 239: loss = 18.542003631591797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 240: loss = 19.00334930419922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 241: loss = 19.820127487182617\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  5428\n",
      "tensor(0.1159, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1391, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1020, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 542\n",
      "Before upsampling, number of sites: 5428 amount added: 2168\n",
      "sites shape AFTER:  torch.Size([7596, 3])\n",
      "sites sdf shape AFTER:  torch.Size([7596])\n",
      "Epoch 242: loss = 14.159297943115234\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 243: loss = 10.646276473999023\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 244: loss = 10.987469673156738\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 245: loss = 10.843740463256836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 246: loss = 10.9600191116333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 247: loss = 12.90870475769043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 248: loss = 12.618026733398438\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 249: loss = 11.470272064208984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 250: loss = 11.590703964233398\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 251: loss = 9.303030967712402\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 252: loss = 13.10178279876709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 253: loss = 13.071609497070312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 254: loss = 12.914957046508789\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 255: loss = 12.64919662475586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 256: loss = 11.592679977416992\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 257: loss = 13.804242134094238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 258: loss = 16.09524154663086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 259: loss = 15.577871322631836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 260: loss = 15.973564147949219\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 261: loss = 14.279193878173828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 262: loss = 13.242626190185547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 263: loss = 13.807465553283691\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 264: loss = 14.645061492919922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 265: loss = 14.678506851196289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 266: loss = 14.841182708740234\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 267: loss = 15.149839401245117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 268: loss = 14.80575180053711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 269: loss = 14.706253051757812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 270: loss = 15.226205825805664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 271: loss = 14.002487182617188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 272: loss = 14.976838111877441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 273: loss = 15.279313087463379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 274: loss = 14.196123123168945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 275: loss = 14.607129096984863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 276: loss = 14.273970603942871\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 277: loss = 12.741067886352539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 278: loss = 13.954965591430664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 279: loss = 12.051668167114258\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 280: loss = 11.486769676208496\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 281: loss = 11.864680290222168\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 282: loss = 9.450216293334961\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 283: loss = 13.506031036376953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 284: loss = 13.455217361450195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 285: loss = 13.116732597351074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 286: loss = 14.149738311767578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 287: loss = 13.070505142211914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 288: loss = 13.172748565673828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 289: loss = 13.448442459106445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 290: loss = 12.671783447265625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 291: loss = 12.078033447265625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 292: loss = 12.53397274017334\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 293: loss = 12.732552528381348\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 294: loss = 12.794321060180664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 295: loss = 9.750174522399902\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 296: loss = 11.769360542297363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 297: loss = 11.523782730102539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 298: loss = 11.515085220336914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 299: loss = 12.179869651794434\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 300: loss = 11.37295150756836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 301: loss = 12.220521926879883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 302: loss = 11.702125549316406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 303: loss = 11.856338500976562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 304: loss = 12.287857055664062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 305: loss = 11.865442276000977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 306: loss = 13.078020095825195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 307: loss = 13.055486679077148\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 308: loss = 12.379911422729492\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 309: loss = 12.394272804260254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 310: loss = 11.987727165222168\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 311: loss = 12.145149230957031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 312: loss = 12.89732551574707\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 313: loss = 12.623149871826172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 314: loss = 12.069160461425781\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 315: loss = 12.304983139038086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 316: loss = 12.697026252746582\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 317: loss = 12.812339782714844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 318: loss = 11.147430419921875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 319: loss = 9.929076194763184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 320: loss = 9.606315612792969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 321: loss = 9.014079093933105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  7596\n",
      "tensor(0.0748, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0898, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0658, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 759\n",
      "Before upsampling, number of sites: 7596 amount added: 3036\n",
      "sites shape AFTER:  torch.Size([10632, 3])\n",
      "sites sdf shape AFTER:  torch.Size([10632])\n",
      "Epoch 322: loss = 6.776115417480469\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 323: loss = 6.1453070640563965\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 324: loss = 5.941501140594482\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 325: loss = 6.3136372566223145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 326: loss = 5.992745399475098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 327: loss = 6.1083455085754395\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 328: loss = 5.973272323608398\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 329: loss = 6.38901948928833\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 330: loss = 6.218957901000977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 331: loss = 6.040493965148926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 332: loss = 6.1086506843566895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 333: loss = 5.653827667236328\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 334: loss = 5.7931389808654785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 335: loss = 5.977386951446533\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 336: loss = 6.257913112640381\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 337: loss = 6.187485694885254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 338: loss = 5.899381637573242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 339: loss = 5.41690731048584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 340: loss = 6.784815311431885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 341: loss = 6.275692939758301\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 342: loss = 5.757281303405762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 343: loss = 6.285496711730957\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 344: loss = 6.1762800216674805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 345: loss = 6.09599494934082\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 346: loss = 6.08955192565918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 347: loss = 7.079575061798096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 348: loss = 6.612249374389648\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 349: loss = 7.398065090179443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 350: loss = 6.849071502685547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 351: loss = 7.25386381149292\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 352: loss = 6.495323181152344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 353: loss = 6.570067405700684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 354: loss = 6.394665718078613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 355: loss = 5.616090774536133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 356: loss = 5.982452392578125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 357: loss = 5.735677719116211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 358: loss = 5.763476371765137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 359: loss = 6.362825393676758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 360: loss = 5.996272087097168\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 361: loss = 6.501105308532715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 362: loss = 6.376129150390625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 363: loss = 6.129085540771484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 364: loss = 5.955261707305908\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 365: loss = 5.905583381652832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 366: loss = 6.298912048339844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 367: loss = 6.012852191925049\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 368: loss = 5.411653518676758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 369: loss = 6.534734725952148\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 370: loss = 6.011939525604248\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 371: loss = 6.090294361114502\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 372: loss = 5.970258712768555\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 373: loss = 5.637110233306885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 374: loss = 5.7989583015441895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 375: loss = 5.33689022064209\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 376: loss = 6.263266563415527\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 377: loss = 5.939664840698242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 378: loss = 5.527951240539551\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 379: loss = 5.904730796813965\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 380: loss = 6.053150177001953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 381: loss = 6.4519219398498535\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 382: loss = 6.222711563110352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 383: loss = 6.493132591247559\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 384: loss = 6.660663604736328\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 385: loss = 6.040205955505371\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 386: loss = 5.3942413330078125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 387: loss = 5.790832996368408\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 388: loss = 6.058835983276367\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 389: loss = 6.2600555419921875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 390: loss = 5.8929853439331055\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 391: loss = 6.0411376953125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 392: loss = 6.4038262367248535\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 393: loss = 6.213008403778076\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 394: loss = 6.546928405761719\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 395: loss = 5.684736251831055\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 396: loss = 5.6978678703308105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 397: loss = 5.963435173034668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 398: loss = 5.090780258178711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 399: loss = 6.160694599151611\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 400: loss = 6.064758777618408\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 401: loss = 6.437774658203125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  10632\n",
      "tensor(0.0226, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0271, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0199, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 1063\n",
      "Before upsampling, number of sites: 10632 amount added: 4252\n",
      "sites shape AFTER:  torch.Size([14884, 3])\n",
      "sites sdf shape AFTER:  torch.Size([14884])\n",
      "Epoch 402: loss = 4.426258087158203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 403: loss = 3.6466169357299805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 404: loss = 3.949233293533325\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 405: loss = 3.861825704574585\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 406: loss = 4.102914810180664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 407: loss = 4.091080665588379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 408: loss = 4.157631874084473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 409: loss = 4.230898857116699\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 410: loss = 4.217339515686035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 411: loss = 3.982854127883911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 412: loss = 4.009184837341309\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 413: loss = 3.79626202583313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 414: loss = 3.4192919731140137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 415: loss = 3.716895818710327\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 416: loss = 4.011055946350098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 417: loss = 4.219874382019043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 418: loss = 4.022453308105469\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 419: loss = 3.832385301589966\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 420: loss = 4.484961986541748\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 421: loss = 4.056674003601074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 422: loss = 3.894076108932495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 423: loss = 4.038403511047363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 424: loss = 3.68379807472229\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 425: loss = 3.7239267826080322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 426: loss = 3.961571455001831\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 427: loss = 3.9212679862976074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 428: loss = 4.00632905960083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 429: loss = 4.090917587280273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 430: loss = 3.7439382076263428\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 431: loss = 3.8536996841430664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 432: loss = 3.682222843170166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 433: loss = 3.9168009757995605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 434: loss = 4.186000347137451\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 435: loss = 3.98361873626709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 436: loss = 4.011359691619873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 437: loss = 4.370094299316406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 438: loss = 4.064544200897217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 439: loss = 3.901012897491455\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 440: loss = 4.061601638793945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 441: loss = 4.1969380378723145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 442: loss = 3.780754327774048\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 443: loss = 3.4391860961914062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 444: loss = 3.794480085372925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 445: loss = 4.055020332336426\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 446: loss = 3.696396827697754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 447: loss = 3.9758684635162354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 448: loss = 4.056552886962891\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 449: loss = 4.006304740905762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 450: loss = 4.287651538848877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 451: loss = 3.812411069869995\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 452: loss = 3.8805224895477295\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 453: loss = 3.6690869331359863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 454: loss = 4.127952575683594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 455: loss = 4.585050582885742\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 456: loss = 4.463308811187744\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 457: loss = 3.6872596740722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 458: loss = 3.7225069999694824\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 459: loss = 4.473104476928711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 460: loss = 4.623568058013916\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 461: loss = 4.213580131530762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 462: loss = 4.127716541290283\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 463: loss = 4.277738571166992\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 464: loss = 4.281641006469727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 465: loss = 4.7021803855896\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 466: loss = 4.339463710784912\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 467: loss = 4.273492813110352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 468: loss = 4.079958438873291\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 469: loss = 3.8666915893554688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 470: loss = 4.106308460235596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 471: loss = 4.2630815505981445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 472: loss = 4.120725154876709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 473: loss = 4.327589511871338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 474: loss = 4.438505172729492\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 475: loss = 4.457265377044678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 476: loss = 4.078722953796387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 477: loss = 4.240822792053223\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 478: loss = 4.399394989013672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 479: loss = 4.12417459487915\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 480: loss = 4.277379512786865\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 481: loss = 4.521361827850342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  14884\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0179, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0131, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 1488\n",
      "Before upsampling, number of sites: 14884 amount added: 5952\n",
      "sites shape AFTER:  torch.Size([20836, 3])\n",
      "sites sdf shape AFTER:  torch.Size([20836])\n",
      "Epoch 482: loss = 3.3556249141693115\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 483: loss = 2.6353683471679688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 484: loss = 2.9039783477783203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 485: loss = 2.6970832347869873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 486: loss = 3.1801247596740723\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 487: loss = 2.7599260807037354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 488: loss = 2.7509891986846924\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 489: loss = 2.83739972114563\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 490: loss = 2.986449718475342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 491: loss = 2.7800979614257812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 492: loss = 2.6686151027679443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 493: loss = 2.7868759632110596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 494: loss = 2.8117640018463135\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 495: loss = 2.940885066986084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 496: loss = 2.848078966140747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 497: loss = 2.7628324031829834\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 498: loss = 2.6977124214172363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 499: loss = 2.6563916206359863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 500: loss = 2.7084057331085205\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 501: loss = 2.7685129642486572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 502: loss = 2.5921127796173096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 503: loss = 2.7601685523986816\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 504: loss = 2.834444046020508\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 505: loss = 3.064178466796875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 506: loss = 2.7507054805755615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 507: loss = 2.859816074371338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 508: loss = 2.8317365646362305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 509: loss = 2.767263412475586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 510: loss = 2.6919612884521484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 511: loss = 2.7410080432891846\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 512: loss = 2.582730770111084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 513: loss = 2.943023920059204\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 514: loss = 2.885084629058838\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 515: loss = 2.675166130065918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 516: loss = 2.9480574131011963\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 517: loss = 2.744729995727539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 518: loss = 2.79679799079895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 519: loss = 2.9714250564575195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 520: loss = 2.9053456783294678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 521: loss = 2.9906985759735107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 522: loss = 2.8531196117401123\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 523: loss = 2.762923240661621\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 524: loss = 2.72663950920105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 525: loss = 2.646657705307007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 526: loss = 2.9090418815612793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 527: loss = 2.7781107425689697\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 528: loss = 2.7914159297943115\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 529: loss = 2.7907028198242188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 530: loss = 2.895648717880249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 531: loss = 2.888040065765381\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 532: loss = 2.6635820865631104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 533: loss = 2.7147414684295654\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 534: loss = 3.044239044189453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 535: loss = 3.0064826011657715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 536: loss = 2.815739631652832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 537: loss = 3.1019842624664307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 538: loss = 2.9978220462799072\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 539: loss = 2.9478795528411865\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 540: loss = 2.79106068611145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 541: loss = 2.689077377319336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 542: loss = 2.979865312576294\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 543: loss = 2.790684938430786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 544: loss = 2.956500768661499\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 545: loss = 2.522240400314331\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 546: loss = 2.4718446731567383\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 547: loss = 2.746609687805176\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 548: loss = 2.9320807456970215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 549: loss = 3.010843515396118\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 550: loss = 2.8426995277404785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 551: loss = 2.7620434761047363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 552: loss = 2.9774749279022217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 553: loss = 2.9371790885925293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 554: loss = 2.700587034225464\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 555: loss = 2.5971357822418213\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 556: loss = 2.6623597145080566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 557: loss = 2.9298250675201416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 558: loss = 3.058753252029419\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 559: loss = 2.8454339504241943\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 560: loss = 2.578432559967041\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 561: loss = 2.758358955383301\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  20836\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0139, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0102, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 2083\n",
      "Before upsampling, number of sites: 20836 amount added: 8332\n",
      "sites shape AFTER:  torch.Size([29168, 3])\n",
      "sites sdf shape AFTER:  torch.Size([29168])\n",
      "Epoch 562: loss = 1.9927493333816528\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 563: loss = 1.6451103687286377\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 564: loss = 1.8278260231018066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 565: loss = 1.9183311462402344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 566: loss = 1.9127219915390015\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 567: loss = 1.8789585828781128\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 568: loss = 2.120967388153076\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 569: loss = 1.971834421157837\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 570: loss = 1.8929837942123413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 571: loss = 1.9564979076385498\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 572: loss = 2.052229881286621\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 573: loss = 1.8450347185134888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 574: loss = 1.9097504615783691\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 575: loss = 1.8552533388137817\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 576: loss = 1.990161657333374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 577: loss = 2.0156643390655518\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 578: loss = 1.841308355331421\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 579: loss = 2.09731388092041\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 580: loss = 1.9965581893920898\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 581: loss = 1.8883287906646729\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 582: loss = 1.8947277069091797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 583: loss = 2.0292210578918457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 584: loss = 1.866677165031433\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 585: loss = 2.116605281829834\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 586: loss = 1.8282815217971802\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 587: loss = 1.9506593942642212\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 588: loss = 1.9034576416015625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 589: loss = 1.8851161003112793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 590: loss = 1.8968733549118042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 591: loss = 1.835494875907898\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 592: loss = 1.9829355478286743\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 593: loss = 2.036759853363037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 594: loss = 2.114729166030884\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 595: loss = 2.0227341651916504\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 596: loss = 2.16886568069458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 597: loss = 1.9558049440383911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 598: loss = 1.7653007507324219\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 599: loss = 1.868167519569397\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 600: loss = 1.8519264459609985\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 601: loss = 1.6758480072021484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 602: loss = 1.8167186975479126\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 603: loss = 1.9721685647964478\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 604: loss = 1.8942747116088867\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 605: loss = 1.642109990119934\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 606: loss = 1.875026822090149\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 607: loss = 1.9726048707962036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 608: loss = 1.941227912902832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 609: loss = 1.8883033990859985\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 610: loss = 1.8305811882019043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 611: loss = 1.773309350013733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 612: loss = 1.9890143871307373\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 613: loss = 1.8938915729522705\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 614: loss = 1.9129165410995483\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 615: loss = 1.9210841655731201\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 616: loss = 2.0063462257385254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 617: loss = 1.9292231798171997\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 618: loss = 1.8286665678024292\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 619: loss = 1.8528465032577515\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 620: loss = 1.8462467193603516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 621: loss = 2.0658531188964844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 622: loss = 1.9240676164627075\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 623: loss = 1.891772985458374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 624: loss = 2.0052807331085205\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 625: loss = 1.9822648763656616\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 626: loss = 2.0606508255004883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 627: loss = 1.8789292573928833\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 628: loss = 1.934040904045105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 629: loss = 2.1582469940185547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 630: loss = 1.884171962738037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 631: loss = 1.7809315919876099\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 632: loss = 1.7936604022979736\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 633: loss = 1.824979543685913\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 634: loss = 1.8622931241989136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 635: loss = 1.9652037620544434\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 636: loss = 1.9829316139221191\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 637: loss = 1.8209995031356812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 638: loss = 1.9031566381454468\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 639: loss = 1.8629815578460693\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 640: loss = 1.7039202451705933\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 641: loss = 1.99613356590271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  29168\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0115, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 2916\n",
      "Before upsampling, number of sites: 29168 amount added: 11664\n",
      "sites shape AFTER:  torch.Size([40832, 3])\n",
      "sites sdf shape AFTER:  torch.Size([40832])\n",
      "Epoch 642: loss = 1.3786463737487793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 643: loss = 1.0989245176315308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 644: loss = 1.263382077217102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 645: loss = 1.3397343158721924\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 646: loss = 1.4816840887069702\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 647: loss = 1.3657068014144897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 648: loss = 1.4459428787231445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 649: loss = 1.3115260601043701\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 650: loss = 1.4367090463638306\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 651: loss = 1.40072500705719\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 652: loss = 1.4609837532043457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 653: loss = 1.342347502708435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 654: loss = 1.208842396736145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 655: loss = 1.6344321966171265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 656: loss = 1.4387792348861694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 657: loss = 1.3338302373886108\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 658: loss = 1.298030138015747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 659: loss = 1.2934621572494507\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 660: loss = 1.2363967895507812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 661: loss = 1.3398016691207886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 662: loss = 1.2933130264282227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 663: loss = 1.3665807247161865\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 664: loss = 1.341952919960022\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 665: loss = 1.156425952911377\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 666: loss = 1.278065800666809\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 667: loss = 1.1147315502166748\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 668: loss = 1.251481056213379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 669: loss = 1.1954952478408813\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 670: loss = 1.0922068357467651\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 671: loss = 1.309902310371399\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 672: loss = 1.292768120765686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 673: loss = 1.2730481624603271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 674: loss = 1.2661772966384888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 675: loss = 1.330206274986267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 676: loss = 1.1721633672714233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 677: loss = 1.2556874752044678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 678: loss = 1.2344930171966553\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 679: loss = 1.2109640836715698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 680: loss = 1.244503140449524\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 681: loss = 1.3071134090423584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 682: loss = 1.2800332307815552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 683: loss = 1.146610140800476\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 684: loss = 1.3289002180099487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 685: loss = 1.3174042701721191\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 686: loss = 1.4935598373413086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 687: loss = 1.429458737373352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 688: loss = 1.2683905363082886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 689: loss = 1.3479751348495483\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 690: loss = 1.248716115951538\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 691: loss = 1.2804394960403442\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 692: loss = 1.3416359424591064\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 693: loss = 1.3109270334243774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 694: loss = 1.3264151811599731\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 695: loss = 1.3292025327682495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 696: loss = 1.3831367492675781\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 697: loss = 1.394797444343567\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 698: loss = 1.3466458320617676\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 699: loss = 1.3625695705413818\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 700: loss = 1.3016562461853027\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 701: loss = 1.3015925884246826\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 702: loss = 1.2372857332229614\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 703: loss = 1.3124308586120605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 704: loss = 1.129941463470459\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 705: loss = 1.3121838569641113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 706: loss = 1.2980599403381348\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 707: loss = 1.3112082481384277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 708: loss = 1.3431532382965088\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 709: loss = 1.2484192848205566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 710: loss = 1.2722100019454956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 711: loss = 1.3347294330596924\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 712: loss = 1.3630836009979248\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 713: loss = 1.2987507581710815\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 714: loss = 1.2176029682159424\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 715: loss = 1.3245855569839478\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 716: loss = 1.1883225440979004\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 717: loss = 1.3589235544204712\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 718: loss = 1.2540804147720337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 719: loss = 1.2959411144256592\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 720: loss = 1.2716569900512695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 721: loss = 1.234109878540039\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  40832\n",
      "Skipping upsampling, too many sites, sites length:  40832 grid size:  32768\n",
      "Epoch 721: loss = 1.2790493965148926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 722: loss = 1.011568307876587\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 723: loss = 1.3643133640289307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 724: loss = 1.4332201480865479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 725: loss = 1.4255306720733643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 726: loss = 1.304984211921692\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 727: loss = 1.2555521726608276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 728: loss = 1.3008472919464111\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 729: loss = 1.2094717025756836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 730: loss = 1.3777482509613037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 731: loss = 1.1770269870758057\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 732: loss = 1.2231578826904297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 733: loss = 1.264487862586975\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 734: loss = 1.1064985990524292\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 735: loss = 1.2105565071105957\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 736: loss = 1.3021600246429443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 737: loss = 1.336071252822876\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 738: loss = 1.3405290842056274\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 739: loss = 1.160659909248352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 740: loss = 1.2551041841506958\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 741: loss = 1.1358591318130493\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 742: loss = 1.2172636985778809\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 743: loss = 1.2415542602539062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 744: loss = 1.1791492700576782\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 745: loss = 1.3331365585327148\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 746: loss = 1.2330257892608643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 747: loss = 1.3215996026992798\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 748: loss = 1.3954423666000366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 749: loss = 1.3109958171844482\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 750: loss = 1.2383025884628296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 751: loss = 1.2869952917099\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 752: loss = 1.2489960193634033\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 753: loss = 1.208710789680481\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 754: loss = 1.2309834957122803\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 755: loss = 1.1390806436538696\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 756: loss = 1.2137717008590698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 757: loss = 1.3470571041107178\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 758: loss = 1.3312653303146362\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 759: loss = 1.2302825450897217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 760: loss = 1.2987616062164307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 761: loss = 1.2079845666885376\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 762: loss = 1.2532613277435303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 763: loss = 1.3635579347610474\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 764: loss = 1.3058226108551025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 765: loss = 1.3314282894134521\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 766: loss = 1.347105622291565\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 767: loss = 1.248759150505066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 768: loss = 1.3243858814239502\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 769: loss = 1.4092388153076172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 770: loss = 1.2691596746444702\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 771: loss = 1.3006484508514404\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 772: loss = 1.3461934328079224\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 773: loss = 1.408424735069275\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 774: loss = 1.3113502264022827\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 775: loss = 1.3882049322128296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 776: loss = 1.3322137594223022\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 777: loss = 1.2607206106185913\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 778: loss = 1.4951999187469482\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 779: loss = 1.3000638484954834\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 780: loss = 1.295439600944519\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 781: loss = 1.341196060180664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 782: loss = 1.3566101789474487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 783: loss = 1.4552727937698364\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 784: loss = 1.2722376585006714\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 785: loss = 1.1763941049575806\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 786: loss = 1.2975473403930664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 787: loss = 1.2810044288635254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 788: loss = 1.3909482955932617\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 789: loss = 1.3682270050048828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 790: loss = 1.4192242622375488\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 791: loss = 1.2817513942718506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 792: loss = 1.3181145191192627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 793: loss = 1.3529984951019287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 794: loss = 1.358222484588623\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 795: loss = 1.3831793069839478\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 796: loss = 1.4529649019241333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 797: loss = 1.4466526508331299\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 798: loss = 1.4049715995788574\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 799: loss = 1.3563048839569092\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 800: loss = 1.331291675567627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 801: loss = 1.3294199705123901\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 802: loss = 1.385117530822754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 803: loss = 1.340789556503296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 804: loss = 1.3602057695388794\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 805: loss = 1.4183870553970337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 806: loss = 1.3784022331237793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 807: loss = 1.3793960809707642\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 808: loss = 1.2905669212341309\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 809: loss = 1.2704824209213257\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 810: loss = 1.3549561500549316\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 811: loss = 1.3515748977661133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 812: loss = 1.5711630582809448\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 813: loss = 1.304129719734192\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 814: loss = 1.3495960235595703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 815: loss = 1.3818604946136475\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 816: loss = 1.2453707456588745\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 817: loss = 1.215416669845581\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 818: loss = 1.3128308057785034\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 819: loss = 1.3860416412353516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 820: loss = 1.2655807733535767\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 821: loss = 1.324769139289856\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 822: loss = 1.2845990657806396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 823: loss = 1.454886794090271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 824: loss = 1.4112797975540161\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 825: loss = 1.339064598083496\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 826: loss = 1.2314844131469727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 827: loss = 1.3321090936660767\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 828: loss = 1.4363831281661987\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 829: loss = 1.2977172136306763\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 830: loss = 1.3360521793365479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 831: loss = 1.177178144454956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 832: loss = 1.1941884756088257\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 833: loss = 1.216638207435608\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 834: loss = 1.3572771549224854\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 835: loss = 1.2633689641952515\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 836: loss = 1.2902251482009888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 837: loss = 1.2524582147598267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 838: loss = 1.2120808362960815\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 839: loss = 1.2540936470031738\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 840: loss = 1.1715035438537598\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 841: loss = 1.2309036254882812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 842: loss = 1.265019178390503\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 843: loss = 1.3433637619018555\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 844: loss = 1.2581043243408203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 845: loss = 1.2790693044662476\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 846: loss = 1.207460880279541\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 847: loss = 1.3325577974319458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 848: loss = 1.313033938407898\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 849: loss = 1.1385047435760498\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 850: loss = 1.1675208806991577\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 851: loss = 1.1789261102676392\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 852: loss = 1.2496888637542725\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 853: loss = 1.2534425258636475\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 854: loss = 1.243255376815796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 855: loss = 1.2452383041381836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 856: loss = 1.163345456123352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 857: loss = 1.1024082899093628\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 858: loss = 1.2620134353637695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 859: loss = 1.186025619506836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 860: loss = 1.1525835990905762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 861: loss = 1.2849615812301636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 862: loss = 1.1543691158294678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 863: loss = 1.1382031440734863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 864: loss = 1.2586339712142944\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 865: loss = 1.2098332643508911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 866: loss = 1.2064034938812256\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 867: loss = 1.2092076539993286\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 868: loss = 1.0991207361221313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 869: loss = 1.0965471267700195\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 870: loss = 1.1023906469345093\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 871: loss = 1.2673918008804321\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 872: loss = 1.2540816068649292\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 873: loss = 1.1443753242492676\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 874: loss = 1.1491886377334595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 875: loss = 1.146673321723938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 876: loss = 1.14810049533844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 877: loss = 1.1786439418792725\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 878: loss = 1.223911166191101\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 879: loss = 1.2329727411270142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 880: loss = 1.2469277381896973\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 881: loss = 1.226698398590088\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 882: loss = 1.196716547012329\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 883: loss = 1.1053051948547363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 884: loss = 1.1066778898239136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 885: loss = 1.2208675146102905\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 886: loss = 1.1216540336608887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 887: loss = 1.165612816810608\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 888: loss = 1.2183465957641602\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 889: loss = 1.1823241710662842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 890: loss = 1.1630915403366089\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 891: loss = 1.2819819450378418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 892: loss = 1.1781625747680664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 893: loss = 1.1459189653396606\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 894: loss = 1.167314887046814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 895: loss = 1.1465399265289307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 896: loss = 1.2068870067596436\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 897: loss = 1.0939745903015137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 898: loss = 1.228380799293518\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 899: loss = 1.1021239757537842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 900: loss = 1.2574635744094849\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 901: loss = 1.2723864316940308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 902: loss = 1.1994436979293823\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 903: loss = 1.3390997648239136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 904: loss = 1.3043973445892334\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 905: loss = 1.1774189472198486\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 906: loss = 1.2095061540603638\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 907: loss = 1.1384758949279785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 908: loss = 1.2658919095993042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 909: loss = 1.2286882400512695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 910: loss = 1.1513354778289795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 911: loss = 1.1067718267440796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 912: loss = 1.168491005897522\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 913: loss = 1.187312126159668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 914: loss = 1.236109733581543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 915: loss = 1.1322364807128906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 916: loss = 0.96907639503479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 917: loss = 1.1812289953231812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 918: loss = 1.1155080795288086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 919: loss = 1.081599235534668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 920: loss = 1.0787714719772339\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 921: loss = 1.2640317678451538\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 922: loss = 1.2436296939849854\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 923: loss = 1.2710586786270142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 924: loss = 1.1463736295700073\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 925: loss = 1.121625304222107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 926: loss = 1.0823980569839478\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 927: loss = 1.2895127534866333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 928: loss = 1.103297233581543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 929: loss = 1.1559635400772095\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 930: loss = 1.1556087732315063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 931: loss = 1.16585111618042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 932: loss = 1.2213093042373657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 933: loss = 1.2378743886947632\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 934: loss = 1.269500732421875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 935: loss = 1.1071131229400635\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 936: loss = 1.170534372329712\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 937: loss = 1.1690505743026733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 938: loss = 1.1079152822494507\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 939: loss = 1.1507889032363892\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 940: loss = 1.0903048515319824\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 941: loss = 1.1510721445083618\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 942: loss = 1.201100468635559\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 943: loss = 1.1099683046340942\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 944: loss = 1.1041209697723389\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 945: loss = 1.158588171005249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 946: loss = 1.052193284034729\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 947: loss = 1.0492192506790161\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 948: loss = 1.1020712852478027\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 949: loss = 1.2910906076431274\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 950: loss = 1.1774940490722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 951: loss = 1.1172953844070435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 952: loss = 1.1096460819244385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 953: loss = 1.1679986715316772\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 954: loss = 1.0897960662841797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 955: loss = 1.1225886344909668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 956: loss = 1.1376986503601074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 957: loss = 1.2874444723129272\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 958: loss = 1.1950502395629883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 959: loss = 1.095162272453308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 960: loss = 1.0808753967285156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 961: loss = 1.0879851579666138\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 962: loss = 1.1861547231674194\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 963: loss = 1.1809278726577759\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 964: loss = 1.183797836303711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 965: loss = 1.1525232791900635\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 966: loss = 1.055643916130066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 967: loss = 1.0442676544189453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 968: loss = 1.1011844873428345\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 969: loss = 1.1160521507263184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 970: loss = 1.0992966890335083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 971: loss = 1.0470918416976929\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 972: loss = 1.0177587270736694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 973: loss = 1.243101954460144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 974: loss = 1.1336930990219116\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 975: loss = 1.0018715858459473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 976: loss = 1.0580048561096191\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 977: loss = 1.170337438583374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 978: loss = 1.2210904359817505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 979: loss = 1.098296880722046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 980: loss = 1.1001116037368774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 981: loss = 1.1162458658218384\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 982: loss = 1.013156771659851\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 983: loss = 1.0617917776107788\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 984: loss = 1.0391936302185059\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 985: loss = 0.9905421137809753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 986: loss = 1.1247961521148682\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 987: loss = 1.0772676467895508\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 988: loss = 1.056567668914795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 989: loss = 1.1400395631790161\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 990: loss = 1.1963915824890137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 991: loss = 1.0382678508758545\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 992: loss = 1.062469482421875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 993: loss = 1.1490486860275269\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 994: loss = 0.9857562780380249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 995: loss = 1.0791369676589966\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 996: loss = 1.1195907592773438\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 997: loss = 1.083177089691162\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 998: loss = 1.0708237886428833\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 999: loss = 1.0536608695983887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 1000: loss = 1.0506199598312378\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Sites length:  40832\n",
      "min sites:  tensor(-1.1949, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1.1972, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # #\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=10, lambda_weights=lambda_weights\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([40832])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/chair1000_1000_3d_sites_4096_chamfer1000.pth\n",
      "sites_np shape:  (40832, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Computing Delaunay simplices...\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer Accuracy (Ours → GT): 0.000177\n",
      "Chamfer Completeness (GT → Ours): 0.000123\n",
      "Chamfer Distance (symmetric): 0.000300\n"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "from trimesh.proximity import ProximityQuery\n",
    "\n",
    "\n",
    "def point_to_mesh_distance(points, mesh):\n",
    "    pq = ProximityQuery(mesh)\n",
    "    dists = pq.signed_distance(points)\n",
    "    return np.abs(dists)  # signed => unsigned\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT → Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours → GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours → GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT → Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
