{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "#lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "# # \n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class Voroloss_opt(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Voroloss_opt, self).__init__()\n",
    "#         self.knn = 16\n",
    "\n",
    "#     def __call__(self, points, spoints):\n",
    "#         \"\"\"points, self.points\"\"\"\n",
    "#         # WARNING: fecthing for knn\n",
    "#         with torch.no_grad():\n",
    "#             indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "#         points_knn = knn_gather(spoints, indices)\n",
    "#         points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "#         voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "#         voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "#         vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "#             -1\n",
    "#         ) / voronoi_edge_l\n",
    "#         sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "#         return sq_dist.min(-1)[0]\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 570.153.02\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "\n",
    "#add noise to meshgrid\n",
    "#meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 3.038208 MB, Reserved: 23.068672 MB\n",
      "torch.Size([32768, 1])\n",
      "True\n",
      "torch.Size([32768, 3])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# # # #add mnfld points with random noise to sites \n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 64**3 - num_centroids\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "sdf0 = model(sites)\n",
    "sdf_grad0 = torch.autograd.grad(sdf0, sites, grad_outputs=torch.ones_like(sdf0), create_graph=True)[0]\n",
    "\n",
    "sdf0 = sdf0.detach().requires_grad_()\n",
    "sdf_grad0 = sdf_grad0.detach().requires_grad_()\n",
    "\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n",
    "\n",
    "print(sdf_grad0.shape)\n",
    "print(sdf_grad0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c17cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sdf_gradient_autograd(sites, tets, sdf_values):\n",
    "    \"\"\"\n",
    "    Computes ∇phi per tetrahedron using PyTorch autograd.\n",
    "\n",
    "    Args:\n",
    "        sites:      (N, 3) tensor of vertex positions, requires_grad=True if you want to optimize them.\n",
    "        tets:       (M, 4) tensor of tetrahedron indices.\n",
    "        sdf_values: (N,) tensor of SDF values, can also require_grad=True if optimizing.\n",
    "\n",
    "    Returns:\n",
    "        grad_phi:   (M, 3) tensor of spatial gradients ∇phi in each tetrahedron.\n",
    "    \"\"\"\n",
    "    if isinstance(tets, np.ndarray):\n",
    "        tets = torch.from_numpy(tets).to(sites.device)\n",
    "    \n",
    "    # Get site positions at each corner of each tetrahedron\n",
    "    s0 = sites[tets[:, 0]]  # (num_tets, 3)\n",
    "    s1 = sites[tets[:, 1]]\n",
    "    s2 = sites[tets[:, 2]]\n",
    "    s3 = sites[tets[:, 3]]\n",
    "\n",
    "    # Build matrix D = [s1-s0, s2-s0, s3-s0] ∈ ℝ^{num_tets × 3 × 3}\n",
    "    D = torch.stack([s1 - s0, s2 - s0, s3 - s0], dim=-1)  # (num_tets, 3, 3)\n",
    "\n",
    "    # Compute D inverse transpose safely\n",
    "    DinvT = torch.linalg.pinv(D).transpose(-2, -1)        # (num_tets, 3, 3)\n",
    "\n",
    "    # Get sdf values at each site\n",
    "    phi0 = sdf_values[tets[:, 0]]  # (num_tets,)\n",
    "    phi1 = sdf_values[tets[:, 1]]\n",
    "    phi2 = sdf_values[tets[:, 2]]\n",
    "    phi3 = sdf_values[tets[:, 3]]\n",
    "\n",
    "    # Δphi stacked in shape (num_tets, 3, 1)\n",
    "    delta_phi = torch.stack([\n",
    "        phi1 - phi0,\n",
    "        phi2 - phi0,\n",
    "        phi3 - phi0\n",
    "    ], dim=1) # (num_tets, 3, 1)\n",
    "\n",
    "    # Batched gradient: (num_tets, 3)\n",
    "    grad_phi = torch.bmm(DinvT, delta_phi).squeeze(-1)  # (num_tets, 3)\n",
    "\n",
    "    # # Now accumulate per-site SDF gradients\n",
    "    # sdf_v_grad = torch.zeros_like(sites)                # (N, 3)\n",
    "    # sdf_v_count = torch.zeros(sites.shape[0], device=sites.device)  # (N,)\n",
    "\n",
    "    # for di in range(4):\n",
    "    #     vi = tets[:, di]                                # indices into sites\n",
    "    #     grad_contrib = grad_phi                         # (num_tets, 3)\n",
    "\n",
    "    #     sdf_v_grad.index_add_(0, vi, grad_contrib)\n",
    "    #     sdf_v_count.index_add_(0, vi, torch.ones_like(vi, dtype=torch.float))\n",
    "\n",
    "    # # Normalize (avoid division by zero)\n",
    "    # sdf_v_count = sdf_v_count.clamp(min=1.0).unsqueeze(-1)  # (N, 1)\n",
    "    # sdf_v_grad /= sdf_v_count                               # average gradient per site\n",
    "\n",
    "    # print(\"sdf_v_grad shape: \", sdf_v_grad.shape)\n",
    "\n",
    "    # return sdf_v_grad\n",
    "\n",
    "    return grad_phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 20.732928 MB, Reserved: 1038.09024 MB\n"
     ]
    }
   ],
   "source": [
    "sites_pred = model(sites).detach()#[\"nonmanifold_pnts_pred\"]\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "# ps.register_surface_mesh(\"model clipped initial mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "# ps.register_surface_mesh(\"model initial mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False, sdf0, sdf_grad0, target_pc=mnfld_points)\n",
    "# ps.register_surface_mesh(\"sdf initial mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True, sdf0, sdf_grad0, target_pc=mnfld_points)\n",
    "# ps.register_surface_mesh(\"sdf clipped initial mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, sdf_v, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites},\n",
    "    #{'params': [sdf_v], 'lr': lr_sites/100},\n",
    "    #{'params': model.parameters(), 'lr': lr_model}\n",
    "])\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        print(\"sdf_values\", sdf_v[:5])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, sdf_v)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "    \n",
    "        # cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        # print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "\n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        # print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        sdf_grad = compute_sdf_gradient_autograd(sites, d3dsimplices, sdf_v)\n",
    "        sdf_loss = (sdf_grad.norm()-1)**2\n",
    "        \n",
    "        \n",
    "        # sites_loss = (\n",
    "        #     lambda_cvt * cvt_loss +\n",
    "        #     #lambda_chamfer * chamfer_loss_mesh \n",
    "        #     lambda_chamfer * chamfer_loss_points\n",
    "        #     #lambda_chamfer * voroloss_loss\n",
    "        # )\n",
    "\n",
    "        # ---------\n",
    "\n",
    "        # v_vect, f_vect = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v)\n",
    "        # triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "        # triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "        # hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "        # print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        sites_loss = (\n",
    "            lambda_chamfer * voroloss_loss\n",
    "        )\n",
    "        \n",
    "\n",
    "    \n",
    "        loss = sites_loss #+ sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        #sdf_v = update_sdf_values(sdf_v, sdf_gradients, old_positions, sites.clone().detach())\n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "       \n",
    "        # if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "        #     print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "        #     sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "        #     sites = sites.detach().requires_grad_(True)\n",
    "        #     optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "        #                                   #{'params': model.parameters(), 'lr': lr_model}\n",
    "        #                                   ])\n",
    "        #     upsampled += 1.0\n",
    "        #     print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "                \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            #model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(sdf_v, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "\n",
    "    return sites, sdf_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 0: loss = 0.09304272383451462\n",
      "before loss.backward(): Allocated: 122.735104 MB, Reserved: 1163.91936 MB\n",
      "After loss.backward(): Allocated: 46.49216 MB, Reserved: 1289.74848 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 1: loss = 0.06958705186843872\n",
      "before loss.backward(): Allocated: 126.44608 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.114176 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 2: loss = 0.05262858793139458\n",
      "before loss.backward(): Allocated: 126.060032 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.728128 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 3: loss = 0.039996109902858734\n",
      "before loss.backward(): Allocated: 126.476288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.144384 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 4: loss = 0.031679071485996246\n",
      "before loss.backward(): Allocated: 126.470144 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.13824 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 5: loss = 0.02629421465098858\n",
      "before loss.backward(): Allocated: 126.293504 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.41376 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 6: loss = 0.02271481230854988\n",
      "before loss.backward(): Allocated: 126.300672 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.422464 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 7: loss = 0.019652554765343666\n",
      "before loss.backward(): Allocated: 126.081536 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.433216 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 8: loss = 0.016949696466326714\n",
      "before loss.backward(): Allocated: 126.488064 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.15616 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 9: loss = 0.014492307789623737\n",
      "before loss.backward(): Allocated: 125.781504 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.4496 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 10: loss = 0.012349908240139484\n",
      "before loss.backward(): Allocated: 126.820864 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.170496 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 11: loss = 0.011058337055146694\n",
      "before loss.backward(): Allocated: 126.509056 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.177152 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 12: loss = 0.01038395520299673\n",
      "before loss.backward(): Allocated: 125.800448 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.468544 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 13: loss = 0.009566655382514\n",
      "before loss.backward(): Allocated: 126.840832 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.188928 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 14: loss = 0.00862440001219511\n",
      "before loss.backward(): Allocated: 126.526464 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.19456 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 15: loss = 0.00786425918340683\n",
      "before loss.backward(): Allocated: 125.807104 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.4752 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 16: loss = 0.007256480865180492\n",
      "before loss.backward(): Allocated: 126.850048 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.197632 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 17: loss = 0.006889778189361095\n",
      "before loss.backward(): Allocated: 126.528512 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.196608 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 18: loss = 0.0065922061912715435\n",
      "before loss.backward(): Allocated: 126.318592 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.666176 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 19: loss = 0.006157068535685539\n",
      "before loss.backward(): Allocated: 126.528512 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.196608 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 20: loss = 0.005681135691702366\n",
      "before loss.backward(): Allocated: 125.809152 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.477248 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 21: loss = 0.005291402339935303\n",
      "before loss.backward(): Allocated: 126.854656 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.201728 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 22: loss = 0.005104766692966223\n",
      "before loss.backward(): Allocated: 126.530048 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.198144 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 23: loss = 0.004844877868890762\n",
      "before loss.backward(): Allocated: 126.32064 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.667712 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 24: loss = 0.004638713784515858\n",
      "before loss.backward(): Allocated: 126.534144 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.20224 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 25: loss = 0.004402301739901304\n",
      "before loss.backward(): Allocated: 125.813248 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.481344 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 26: loss = 0.0041745007038116455\n",
      "before loss.backward(): Allocated: 126.531584 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.19968 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 27: loss = 0.003930977545678616\n",
      "before loss.backward(): Allocated: 125.816832 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.484928 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 28: loss = 0.0037315397057682276\n",
      "before loss.backward(): Allocated: 126.859264 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.205824 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 29: loss = 0.003625319106504321\n",
      "before loss.backward(): Allocated: 126.535168 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.203264 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 30: loss = 0.003466951195150614\n",
      "before loss.backward(): Allocated: 126.322176 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.668736 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 31: loss = 0.0032974951900541782\n",
      "before loss.backward(): Allocated: 126.539264 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.20736 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 32: loss = 0.003167507005855441\n",
      "before loss.backward(): Allocated: 125.818368 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.486464 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 33: loss = 0.0030489640776067972\n",
      "before loss.backward(): Allocated: 126.861312 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.207872 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 34: loss = 0.0029250013176351786\n",
      "before loss.backward(): Allocated: 126.542848 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.210944 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 35: loss = 0.002812259830534458\n",
      "before loss.backward(): Allocated: 125.821952 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.490048 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 36: loss = 0.0027109384536743164\n",
      "before loss.backward(): Allocated: 126.86336 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.20992 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 37: loss = 0.0026152124628424644\n",
      "before loss.backward(): Allocated: 126.541824 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.20992 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 38: loss = 0.002544128568843007\n",
      "before loss.backward(): Allocated: 126.540288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.208384 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 39: loss = 0.0024594466667622328\n",
      "before loss.backward(): Allocated: 126.541824 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.20992 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 40: loss = 0.002373906783759594\n",
      "before loss.backward(): Allocated: 125.819904 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.488 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 41: loss = 0.0023085777647793293\n",
      "before loss.backward(): Allocated: 127.044608 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.211456 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 42: loss = 0.002242858987301588\n",
      "before loss.backward(): Allocated: 126.145536 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.491584 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 43: loss = 0.0021868336480110884\n",
      "before loss.backward(): Allocated: 126.543872 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.211968 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 44: loss = 0.002150509273633361\n",
      "before loss.backward(): Allocated: 126.544384 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.21248 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 45: loss = 0.002090087626129389\n",
      "before loss.backward(): Allocated: 126.3232 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.490048 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 46: loss = 0.002038112375885248\n",
      "before loss.backward(): Allocated: 126.324224 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.491584 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 47: loss = 0.001993519486859441\n",
      "before loss.backward(): Allocated: 126.323712 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.492096 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 48: loss = 0.0019474357832223177\n",
      "before loss.backward(): Allocated: 126.868992 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.21504 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 49: loss = 0.001907926402054727\n",
      "before loss.backward(): Allocated: 125.824 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.492096 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 50: loss = 0.0018757516518235207\n",
      "before loss.backward(): Allocated: 126.546944 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.21504 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 51: loss = 0.0018314630724489689\n",
      "before loss.backward(): Allocated: 125.825024 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.49312 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 52: loss = 0.001793737057596445\n",
      "before loss.backward(): Allocated: 127.04768 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.218112 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 53: loss = 0.0017570926574990153\n",
      "before loss.backward(): Allocated: 126.150144 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.49568 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 54: loss = 0.0017251602839678526\n",
      "before loss.backward(): Allocated: 126.55104 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.219136 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 55: loss = 0.0016990796430036426\n",
      "before loss.backward(): Allocated: 125.82912 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.497216 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 56: loss = 0.0016683995490893722\n",
      "before loss.backward(): Allocated: 126.552064 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.22016 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 57: loss = 0.0016395013080909848\n",
      "before loss.backward(): Allocated: 125.833216 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.501312 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 58: loss = 0.00161513383500278\n",
      "before loss.backward(): Allocated: 126.87616 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.221696 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 59: loss = 0.0015944158658385277\n",
      "before loss.backward(): Allocated: 126.002688 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.670784 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 60: loss = 0.0016120228683575988\n",
      "before loss.backward(): Allocated: 126.326272 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.497728 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 61: loss = 0.0015525929629802704\n",
      "before loss.backward(): Allocated: 126.325248 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.670784 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 62: loss = 0.0015487089985981584\n",
      "before loss.backward(): Allocated: 126.87616 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.222208 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 63: loss = 0.0015178503235802054\n",
      "before loss.backward(): Allocated: 126.001664 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.66976 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 64: loss = 0.0015101124299690127\n",
      "before loss.backward(): Allocated: 126.873088 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.218624 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 65: loss = 0.0014824187383055687\n",
      "before loss.backward(): Allocated: 126.003712 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.671808 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 66: loss = 0.001469544367864728\n",
      "before loss.backward(): Allocated: 126.153216 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.498752 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 67: loss = 0.0014548774342983961\n",
      "before loss.backward(): Allocated: 126.003712 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.671808 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 68: loss = 0.0014290702529251575\n",
      "before loss.backward(): Allocated: 126.875136 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.220672 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 69: loss = 0.0014252401888370514\n",
      "before loss.backward(): Allocated: 125.830656 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.498752 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 70: loss = 0.001408012118190527\n",
      "before loss.backward(): Allocated: 127.049216 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.22272 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 71: loss = 0.0013977258931845427\n",
      "before loss.backward(): Allocated: 126.156288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.501312 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 72: loss = 0.0013885658700019121\n",
      "before loss.backward(): Allocated: 126.55616 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.224256 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 73: loss = 0.0013625164283439517\n",
      "before loss.backward(): Allocated: 125.834752 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.502848 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 74: loss = 0.0013517411425709724\n",
      "before loss.backward(): Allocated: 126.555648 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.223744 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 75: loss = 0.0013461585622280836\n",
      "before loss.backward(): Allocated: 125.833216 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.501312 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 76: loss = 0.0013467142125591636\n",
      "before loss.backward(): Allocated: 127.05024 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.22528 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 77: loss = 0.0013232759665697813\n",
      "before loss.backward(): Allocated: 126.158848 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.503872 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 78: loss = 0.001307441620156169\n",
      "before loss.backward(): Allocated: 126.55616 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.224256 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 79: loss = 0.001308936276473105\n",
      "before loss.backward(): Allocated: 126.558208 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.226304 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 80: loss = 0.0012879077112302184\n",
      "before loss.backward(): Allocated: 126.330368 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506432 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 81: loss = 0.001281124074012041\n",
      "before loss.backward(): Allocated: 126.329344 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506944 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 82: loss = 0.0012686968548223376\n",
      "before loss.backward(): Allocated: 126.329344 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506432 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 83: loss = 0.001263481448404491\n",
      "before loss.backward(): Allocated: 126.557696 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.225792 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 84: loss = 0.0012517367722466588\n",
      "before loss.backward(): Allocated: 125.836288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.504384 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 85: loss = 0.0012444541789591312\n",
      "before loss.backward(): Allocated: 126.881792 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.226816 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 86: loss = 0.0012453774688765407\n",
      "before loss.backward(): Allocated: 126.558208 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.226304 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 87: loss = 0.0012284705881029367\n",
      "before loss.backward(): Allocated: 126.32832 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.673344 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 88: loss = 0.0012332285987213254\n",
      "before loss.backward(): Allocated: 126.558208 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.226304 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 89: loss = 0.0012147873640060425\n",
      "before loss.backward(): Allocated: 125.834752 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.502848 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 90: loss = 0.0012153368443250656\n",
      "before loss.backward(): Allocated: 125.838336 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506432 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 91: loss = 0.0012043858878314495\n",
      "before loss.backward(): Allocated: 126.00576 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.673856 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 92: loss = 0.0012060534209012985\n",
      "before loss.backward(): Allocated: 125.836288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.504384 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 93: loss = 0.0012064878828823566\n",
      "before loss.backward(): Allocated: 126.560768 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.228864 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 94: loss = 0.001194062060676515\n",
      "before loss.backward(): Allocated: 125.835776 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.503872 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 95: loss = 0.0011785293463617563\n",
      "before loss.backward(): Allocated: 127.052288 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.228864 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 96: loss = 0.0011740861227735877\n",
      "before loss.backward(): Allocated: 126.162432 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506944 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 97: loss = 0.0011711465194821358\n",
      "before loss.backward(): Allocated: 126.560768 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.228864 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 98: loss = 0.0011712369741871953\n",
      "before loss.backward(): Allocated: 126.329344 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.506944 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 99: loss = 0.001164174871519208\n",
      "before loss.backward(): Allocated: 126.561792 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 48.229888 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "sdf_values tensor([[1.3140],\n",
      "        [1.2899],\n",
      "        [1.2543],\n",
      "        [1.2190],\n",
      "        [1.1923]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch 100: loss = 0.001152298878878355\n",
      "before loss.backward(): Allocated: 126.329856 MB, Reserved: 1291.845632 MB\n",
      "After loss.backward(): Allocated: 47.507968 MB, Reserved: 1291.845632 MB\n",
      "-----------------\n",
      "Sites length:  32768\n",
      "min sites:  tensor(-1., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=1, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites, sdf_v = train_DCCVT(sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768, 1])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/bunny100_100_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "#model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "\n",
    "\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "#ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "sdf_grad = compute_sdf_gradient_autograd(sites, d3dsimplices, sdf_v)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True, sdf_v, sdf_grad, target_pc=mnfld_points)\n",
    "# ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, None, target_pc=mnfld_points)\n",
    "ps.register_surface_mesh(\"sdf final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
