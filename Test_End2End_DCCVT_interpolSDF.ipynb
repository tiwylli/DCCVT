{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "\n",
    "# import diffvoronoi\n",
    "import pygdel3d\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "torch.manual_seed(69)\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_2000.pth\"\n",
    "\n",
    "\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n",
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 432.168448 MB, Reserved: 448.790528 MB\n",
      "torch.Size([32768])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "\n",
    "import sys\n",
    "\n",
    "if mesh[0] != \"sphere\":\n",
    "    sys.path.append(\"3rdparty/HotSpot\")\n",
    "    from dataset import shape_3d\n",
    "    import models.Net as Net\n",
    "\n",
    "    loss_type = \"igr_w_heat\"\n",
    "    loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "    train_set = shape_3d.ReconDataset(\n",
    "        file_path=mesh[1] + \".ply\",\n",
    "        n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "        n_samples=10001,  # args.n_iterations,\n",
    "        grid_res=256,  # args.grid_res,\n",
    "        grid_range=1.1,  # args.grid_range,\n",
    "        sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "        sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "        n_random_samples=7500,  # args.n_random_samples,\n",
    "        resample=True,\n",
    "        compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "        scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "    )\n",
    "\n",
    "    model = Net.Network(\n",
    "        latent_size=0,  # args.latent_size,\n",
    "        in_dim=3,\n",
    "        decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "        nl=\"sine\",  # args.nl,\n",
    "        encoder_type=\"none\",  # args.encoder_type,\n",
    "        decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "        neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "        init_type=\"mfgi\",  # args.init_type,\n",
    "        sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "        n_repeat_period=30,  # args.n_repeat_period,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ######\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_data = next(iter(test_dataloader))\n",
    "    mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "    # add noise to mnfld_points\n",
    "    # mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "    mnfld_points.requires_grad_()\n",
    "    print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "    if torch.cuda.is_available():\n",
    "        map_location = torch.device(\"cuda\")\n",
    "    else:\n",
    "        map_location = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "    sdf0 = model(sites)\n",
    "\n",
    "else:\n",
    "\n",
    "    def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3) tensor of 3D query points\n",
    "            center: (3,) tensor specifying the center of the sphere\n",
    "            radius: float, radius of the sphere\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,) tensor of signed distances\n",
    "        \"\"\"\n",
    "        return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "    def sphere_sdf_with_noise(\n",
    "        points: torch.Tensor, center: torch.Tensor, radius: float, noise_amplitude=0.05\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sphere SDF with smooth directional noise added near the surface.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3)\n",
    "            center: (3,)\n",
    "            radius: float\n",
    "            noise_amplitude: float\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,)\n",
    "        \"\"\"\n",
    "        rel = points - center\n",
    "        norm = torch.norm(rel, dim=-1)  # (N,)\n",
    "        base_sdf = norm - radius  # (N,)\n",
    "\n",
    "        # Smooth periodic noise based on direction\n",
    "        unit_dir = rel / (norm.unsqueeze(-1) + 1e-9)  # (N,3)\n",
    "        noise = torch.sin(10 * unit_dir[:, 0]) * torch.sin(10 * unit_dir[:, 1]) * torch.sin(10 * unit_dir[:, 2])\n",
    "\n",
    "        # Weight noise so it mostly affects surface area\n",
    "        falloff = torch.exp(-20 * (base_sdf**2))  # (N,) ~1 near surface, ~0 far\n",
    "        sdf = base_sdf + noise_amplitude * noise * falloff\n",
    "\n",
    "        return sdf\n",
    "\n",
    "    # generate points on the sphere\n",
    "    mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "    mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "    mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()\n",
    "    sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "    # sdf0 = sphere_sdf_with_noise(sites, torch.zeros(3).to(device), 0.50, noise_amplitude=0.1)\n",
    "\n",
    "##add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 24**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba12786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delaunay simplices shape:  (223649, 4)\n",
      "sites shape:  torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "print(\"Delaunay simplices shape: \", d3dsimplices.shape)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "# ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf0.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"init MTET\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    faces.detach().cpu().numpy(),\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "# ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "    voroloss_optim=False,\n",
    "):\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ],\n",
    "            betas=(0.8, 0.95),\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)  # torch.tensor(0)  #\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "            mtet = False\n",
    "            sites_sdf_grads = None\n",
    "\n",
    "            if mtet:\n",
    "                print(\"Using MTET\")\n",
    "                d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "                marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "                    sites.unsqueeze(0), d3dsimplices, sites_sdf.unsqueeze(0), return_tet_idx=False\n",
    "                )\n",
    "                vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "                v_vect = vertices_list[0]\n",
    "                faces = faces_list[0]\n",
    "                print(\"v_vect shape: \", v_vect.shape)\n",
    "\n",
    "            else:\n",
    "                v_vect, f_vect, sites_sdf_grads, tets_sdf_grads, W = su.get_clipped_mesh_numba(\n",
    "                    sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "                )\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "            print(\n",
    "                \"cvt_loss: \",\n",
    "                lambda_cvt / 10 * cvt_loss.item(),\n",
    "                \"chamfer_loss_mesh: \",\n",
    "                lambda_chamfer * chamfer_loss_mesh.item(),\n",
    "            )\n",
    "            sites_loss = lambda_cvt / 10 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "\n",
    "            if sites_sdf_grads is None:\n",
    "                sites_sdf_grads, tets_sdf_grads, W = su.sdf_space_grad_pytorch_diego_sites_tets(\n",
    "                    sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "                )\n",
    "\n",
    "            # eik_loss = lambda_cvt / 10 * lf.discrete_tet_volume_eikonal_loss(sites, sites_sdf_grads, d3dsimplices)\n",
    "            # shl = lambda_cvt / 0.1 * lf.smoothed_heaviside_loss(sites, sites_sdf, sites_sdf_grads, d3dsimplices)\n",
    "\n",
    "            eik_loss = lambda_cvt / 10 * lf.tet_sdf_grad_eikonal_loss(sites, tets_sdf_grads, d3dsimplices)\n",
    "            print(\"eikonal_loss: \", eik_loss.item())\n",
    "            shl = lambda_cvt * 10 * lf.tet_sdf_motion_mean_curvature_loss(sites, sites_sdf, W, d3dsimplices)\n",
    "            print(\"smoothed_heaviside_loss: \", shl.item())\n",
    "\n",
    "            sdf_loss = eik_loss + shl\n",
    "        else:\n",
    "            sites_loss = lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "\n",
    "        loss = sites_loss + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        # TODO: test epoch == 300 growthrate 300%\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites,\n",
    "                simplices=d3dsimplices,\n",
    "                model=sites_sdf,\n",
    "                sites_sdf_grads=sites_sdf_grads,\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvt_loss:  0.055268509313464165 chamfer_loss_mesh:  0.430283194873482\n",
      "95% percentile of tet volume: 5.843797407578677e-05\n",
      "max tet volume: 0.0011402172967791557\n",
      "tensor([[-0.6200, -1.0396, -0.1443],\n",
      "        [-0.6223, -0.6367, -0.4844],\n",
      "        [-0.7367, -2.7033, -0.3729],\n",
      "        ...,\n",
      "        [-0.1103,  0.0654,  0.9976],\n",
      "        [ 0.1054,  0.0230,  1.0459],\n",
      "        [ 0.0968,  0.0650,  1.0047]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  19.828372955322266\n",
      "smoothed_heaviside_loss:  0.013277333229780197\n",
      "Epoch 0: loss = 20.32720184326172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.0540754571557045 chamfer_loss_mesh:  0.40051800897344947\n",
      "95% percentile of tet volume: 5.8391797210788354e-05\n",
      "max tet volume: 0.0008829224389046431\n",
      "tensor([[ -0.5663,  -3.4152,  -0.4613],\n",
      "        [ -0.5525,  -0.6681,  -0.5053],\n",
      "        [-24.7818, -30.6760,  -0.0622],\n",
      "        ...,\n",
      "        [  0.1421,   0.9328,  -0.3687],\n",
      "        [  0.4982,   0.6823,   0.3806],\n",
      "        [  0.4467,   0.7014,   0.4947]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  16.057607650756836\n",
      "smoothed_heaviside_loss:  0.01347517129033804\n",
      "Epoch 1: loss = 16.525676727294922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.05244255065917969 chamfer_loss_mesh:  0.3859495045617223\n",
      "95% percentile of tet volume: 5.8407043979968876e-05\n",
      "max tet volume: 0.0010402549523860216\n",
      "tensor([[ -0.7341, -19.7497,   0.1995],\n",
      "        [ -0.0852,  -3.3742,   9.7659],\n",
      "        [ -0.5488,  -0.3873,  -0.4962],\n",
      "        ...,\n",
      "        [  0.6730,  -0.7521,   0.0352],\n",
      "        [  0.6745,  -0.7513,   0.0501],\n",
      "        [ -0.7776,  -0.5793,   0.1655]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  12.626470565795898\n",
      "smoothed_heaviside_loss:  0.01361505314707756\n",
      "Epoch 2: loss = 13.078476905822754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.05060942843556404 chamfer_loss_mesh:  0.3638607158791274\n",
      "95% percentile of tet volume: 5.852503454661928e-05\n",
      "max tet volume: 0.0009438846609555185\n",
      "tensor([[-6.2725e-01, -5.8592e-01, -4.7948e-01],\n",
      "        [-2.4534e+01, -2.6880e+01,  1.6553e-02],\n",
      "        [-4.9837e-01, -5.0661e-01,  2.5942e+00],\n",
      "        ...,\n",
      "        [ 6.9730e-01, -5.4201e-01,  3.9676e-01],\n",
      "        [-7.8969e-01,  8.1071e-02,  2.0754e-01],\n",
      "        [-6.9921e-01,  5.7448e-01,  7.9118e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  17.891563415527344\n",
      "smoothed_heaviside_loss:  0.013765143230557442\n",
      "Epoch 3: loss = 18.319799423217773\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04887471906840801 chamfer_loss_mesh:  0.3590426058508456\n",
      "95% percentile of tet volume: 5.864542617928237e-05\n",
      "max tet volume: 0.00111336016561836\n",
      "tensor([[ 0.3625, -1.0046, -0.0470],\n",
      "        [-0.3963, -0.6836, -0.5805],\n",
      "        [-0.2269, -0.5457, -0.5202],\n",
      "        ...,\n",
      "        [ 1.0005,  0.0867,  0.0927],\n",
      "        [-0.1961, -0.0909, -0.9913],\n",
      "        [-0.3094, -0.0930, -0.9456]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  6.843374252319336\n",
      "smoothed_heaviside_loss:  0.01393204741179943\n",
      "Epoch 4: loss = 7.265223979949951\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.047200145199894905 chamfer_loss_mesh:  0.3376311797183007\n",
      "95% percentile of tet volume: 5.877711737412028e-05\n",
      "max tet volume: 0.0018356144428253174\n",
      "tensor([[-0.1386,  0.0045, -0.4442],\n",
      "        [-0.4929, -0.7176, -0.5003],\n",
      "        [ 0.0064, -0.4859, -0.2742],\n",
      "        ...,\n",
      "        [ 0.8846,  0.4063,  0.4999],\n",
      "        [ 0.8826,  0.1266,  0.2330],\n",
      "        [ 1.0935, -0.2242,  0.0773]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  5.650951385498047\n",
      "smoothed_heaviside_loss:  0.014075595885515213\n",
      "Epoch 5: loss = 6.049858570098877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04569499287754297 chamfer_loss_mesh:  0.32622882281430066\n",
      "95% percentile of tet volume: 5.893712295801379e-05\n",
      "max tet volume: 0.0019012456759810448\n",
      "tensor([[-3.4906e-01, -3.8706e+00,  4.5515e-03],\n",
      "        [-2.5930e+01, -1.5923e+01, -1.7312e-02],\n",
      "        [ 1.0177e-02, -5.2216e-01, -2.4425e-01],\n",
      "        ...,\n",
      "        [ 4.7563e-01, -6.1892e-01, -1.8651e+00],\n",
      "        [ 4.6915e-01, -6.0991e-01, -1.8331e+00],\n",
      "        [-9.9565e-01, -8.0574e-02,  1.5447e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  3.102508544921875\n",
      "smoothed_heaviside_loss:  0.014207521453499794\n",
      "Epoch 6: loss = 3.4886398315429688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04442431963980198 chamfer_loss_mesh:  0.3137962776236236\n",
      "95% percentile of tet volume: 5.910190520808101e-05\n",
      "max tet volume: 0.001980627654120326\n",
      "tensor([[-6.9224e-01,  1.3523e-02,  4.1016e-02],\n",
      "        [-4.1548e-01, -2.7089e+01, -1.0754e-01],\n",
      "        [-5.3530e-01, -2.1436e-01, -4.6841e-01],\n",
      "        ...,\n",
      "        [ 1.0526e+00,  4.6651e-01,  5.2887e-02],\n",
      "        [ 8.9802e-01,  3.1778e-01,  7.4210e-02],\n",
      "        [ 9.1791e-01,  4.2470e-01,  1.7027e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  4.239137649536133\n",
      "smoothed_heaviside_loss:  0.014322362840175629\n",
      "Epoch 7: loss = 4.611680507659912\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04332005511969328 chamfer_loss_mesh:  0.3034146793652326\n",
      "95% percentile of tet volume: 5.9312384109944105e-05\n",
      "max tet volume: 0.0020658893045037985\n",
      "tensor([[ -0.2613,  -3.7787,  -0.3107],\n",
      "        [ -0.4174,  -0.8044,   0.3687],\n",
      "        [ -0.1611, -42.0809,  -6.1608],\n",
      "        ...,\n",
      "        [  0.8935,  -0.5966,   0.1016],\n",
      "        [  0.8591,  -0.5086,   0.0443],\n",
      "        [  0.8845,  -0.5070,   0.0641]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  2.3258020877838135\n",
      "smoothed_heaviside_loss:  0.014445883221924305\n",
      "Epoch 8: loss = 2.6869826316833496\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04169600084424019 chamfer_loss_mesh:  0.29416894540190697\n",
      "95% percentile of tet volume: 5.946393866906874e-05\n",
      "max tet volume: 0.0021505740005522966\n",
      "tensor([[-1.0840, -0.6238, -0.4538],\n",
      "        [-0.5213, -1.2971, -0.4454],\n",
      "        [-0.4572, -0.5925, -0.6204],\n",
      "        ...,\n",
      "        [-1.0502,  0.0156,  0.0523],\n",
      "        [-0.9829,  0.0739,  0.2310],\n",
      "        [-0.9801,  0.0747,  0.0399]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  2.560610771179199\n",
      "smoothed_heaviside_loss:  0.014529616571962833\n",
      "Epoch 9: loss = 2.9110054969787598\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.04058615770190954 chamfer_loss_mesh:  0.2894726349040866\n",
      "95% percentile of tet volume: 5.9644073189701885e-05\n",
      "max tet volume: 0.002239413093775511\n",
      "tensor([[-0.4672, -0.4807, -0.5764],\n",
      "        [-0.5374, -2.4345, -0.4463],\n",
      "        [-0.4106, -0.6035, -0.6554],\n",
      "        ...,\n",
      "        [ 0.1841, -0.9219,  0.2369],\n",
      "        [ 0.1491,  0.8968, -0.0339],\n",
      "        [ 0.1321,  0.9954,  0.0348]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  2.038569450378418\n",
      "smoothed_heaviside_loss:  0.01463073305785656\n",
      "Epoch 10: loss = 2.3832590579986572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.039444537833333015 chamfer_loss_mesh:  0.28317977557890117\n",
      "95% percentile of tet volume: 5.978140325169079e-05\n",
      "max tet volume: 0.0023220712319016457\n",
      "tensor([[-0.4768, -2.0899, -0.4365],\n",
      "        [-0.4925, -0.6495, -1.2167],\n",
      "        [ 0.0082, -0.3315, -0.3168],\n",
      "        ...,\n",
      "        [ 0.7000, -0.1198, -0.5804],\n",
      "        [ 0.7181, -0.1266, -0.5970],\n",
      "        [ 0.7690, -0.0781, -0.6315]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  1.9901221990585327\n",
      "smoothed_heaviside_loss:  0.014744464308023453\n",
      "Epoch 11: loss = 2.32749080657959\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03828804474323988 chamfer_loss_mesh:  0.27555067208595574\n",
      "95% percentile of tet volume: 5.988990960759111e-05\n",
      "max tet volume: 0.00240944791585207\n",
      "tensor([[-3.7009e-01, -6.6308e+00,  2.0765e-02],\n",
      "        [-1.7152e-01, -2.4657e+01, -2.7272e+00],\n",
      "        [ 1.3555e-01,  2.0058e+00, -5.7867e-01],\n",
      "        ...,\n",
      "        [ 5.8770e-01, -5.9500e-01,  4.0589e-01],\n",
      "        [ 6.5202e-01, -6.6751e-01,  4.7911e-01],\n",
      "        [ 5.8151e-01, -5.9658e-01,  5.4043e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  3.926701068878174\n",
      "smoothed_heaviside_loss:  0.014838110655546188\n",
      "Epoch 12: loss = 4.255377769470215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03748089540749788 chamfer_loss_mesh:  0.2752022701315582\n",
      "95% percentile of tet volume: 5.997676998958923e-05\n",
      "max tet volume: 0.002418159507215023\n",
      "tensor([[ 1.3869, -0.5691, -0.2756],\n",
      "        [-1.2713, -0.4287, -0.4721],\n",
      "        [-0.6243,  0.2716, -0.4826],\n",
      "        ...,\n",
      "        [ 0.0656,  0.7965, -0.4653],\n",
      "        [ 0.0656,  0.7994, -0.4676],\n",
      "        [-0.7434,  0.4465,  0.5793]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  3.7934205532073975\n",
      "smoothed_heaviside_loss:  0.014927526004612446\n",
      "Epoch 13: loss = 4.121031284332275\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.036703969817608595 chamfer_loss_mesh:  0.2697452437132597\n",
      "95% percentile of tet volume: 6.008798663970083e-05\n",
      "max tet volume: 0.002433572895824909\n",
      "tensor([[-2.0523, -0.6848, -0.2088],\n",
      "        [-1.3097, -0.4294, -0.4712],\n",
      "        [-0.2652, -1.2190, -7.3634],\n",
      "        ...,\n",
      "        [ 0.5223, -0.4489,  0.5643],\n",
      "        [ 0.5229, -0.4488,  0.5641],\n",
      "        [ 0.6119, -0.4232,  0.6826]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  3.672999858856201\n",
      "smoothed_heaviside_loss:  0.015002523548901081\n",
      "Epoch 14: loss = 3.9944515228271484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.0359438406303525 chamfer_loss_mesh:  0.26490725576877594\n",
      "95% percentile of tet volume: 6.018230124027468e-05\n",
      "max tet volume: 0.0024552014656364918\n",
      "tensor([[-0.6476, -0.4946,  1.7925],\n",
      "        [-0.4401, -8.6279, -0.0157],\n",
      "        [-0.3891, -5.0067,  0.0681],\n",
      "        ...,\n",
      "        [ 0.7850, -0.6692,  0.1018],\n",
      "        [ 0.7989, -0.6716,  0.1701],\n",
      "        [-0.9434, -0.5255,  0.0432]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  13.11711311340332\n",
      "smoothed_heaviside_loss:  0.015097241848707199\n",
      "Epoch 15: loss = 13.433061599731445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03479771316051483 chamfer_loss_mesh:  0.26070166495628655\n",
      "95% percentile of tet volume: 6.0225349443499e-05\n",
      "max tet volume: 0.0024800533428788185\n",
      "tensor([[ 0.5114, -0.4921, -0.5818],\n",
      "        [-0.5348, -0.4173, -0.4861],\n",
      "        [-0.6058, -0.5278, -0.6872],\n",
      "        ...,\n",
      "        [-0.8526,  0.4149,  0.3641],\n",
      "        [-0.7115,  0.5196,  0.3029],\n",
      "        [-0.7103,  0.5004,  0.2794]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  21.739797592163086\n",
      "smoothed_heaviside_loss:  0.015187601558864117\n",
      "Epoch 16: loss = 22.050485610961914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.034084110520780087 chamfer_loss_mesh:  0.2623763866722584\n",
      "95% percentile of tet volume: 6.0274363931966946e-05\n",
      "max tet volume: 0.002504987409338355\n",
      "tensor([[-0.7485,  3.7283,  0.1450],\n",
      "        [-5.2849, -2.3030,  0.0229],\n",
      "        [-0.5352, -0.6450, -0.5147],\n",
      "        ...,\n",
      "        [-0.0507,  0.2616, -1.1123],\n",
      "        [-0.0391,  0.1915, -1.1030],\n",
      "        [ 0.0122,  0.2411, -1.1619]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  30.27492332458496\n",
      "smoothed_heaviside_loss:  0.015232594683766365\n",
      "Epoch 17: loss = 30.58661651611328\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03355524269863963 chamfer_loss_mesh:  0.25069923140108585\n",
      "95% percentile of tet volume: 6.031057273503393e-05\n",
      "max tet volume: 0.0025294264778494835\n",
      "tensor([[-0.1207, -3.1829, -0.6096],\n",
      "        [-0.5497, -0.7280, -0.5782],\n",
      "        [-0.6713, -0.6646, -0.5050],\n",
      "        ...,\n",
      "        [ 0.4721, -0.2412, -0.8824],\n",
      "        [ 1.0216, -0.2641,  0.4538],\n",
      "        [ 0.1848,  0.9846, -0.0825]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  9.491329193115234\n",
      "smoothed_heaviside_loss:  0.015288444235920906\n",
      "Epoch 18: loss = 9.790871620178223\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03246844746172428 chamfer_loss_mesh:  0.24413997016381472\n",
      "95% percentile of tet volume: 6.032260716892779e-05\n",
      "max tet volume: 0.0025608004070818424\n",
      "tensor([[-0.7706, -0.7489, -0.1902],\n",
      "        [-0.7031, -0.6135, -0.5274],\n",
      "        [-2.9504, -0.4569, -0.4784],\n",
      "        ...,\n",
      "        [ 0.4812,  0.7079, -0.6327],\n",
      "        [ 0.4123,  0.6474, -0.5556],\n",
      "        [ 0.4880,  0.7102, -0.5531]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  6.77191162109375\n",
      "smoothed_heaviside_loss:  0.015338529832661152\n",
      "Epoch 19: loss = 7.063858509063721\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03223396837711334 chamfer_loss_mesh:  0.24461984867230058\n",
      "95% percentile of tet volume: 6.0328151448629797e-05\n",
      "max tet volume: 0.0025791695807129145\n",
      "tensor([[ 0.1169, -5.1731,  0.1013],\n",
      "        [-0.6590, -0.6668, -0.4909],\n",
      "        [-0.5290, -0.5805, -0.4573],\n",
      "        ...,\n",
      "        [ 0.2472, -0.4621,  0.6566],\n",
      "        [ 0.4080, -0.3989,  0.6957],\n",
      "        [ 0.4081, -0.4041,  0.6900]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  4.965007781982422\n",
      "smoothed_heaviside_loss:  0.015394345857203007\n",
      "Epoch 20: loss = 5.257255554199219\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.031332909129559994 chamfer_loss_mesh:  0.2433861664030701\n",
      "95% percentile of tet volume: 6.034115722286515e-05\n",
      "max tet volume: 0.0025655333884060383\n",
      "tensor([[-0.5637, -0.6578, -0.4560],\n",
      "        [ 0.9510, -0.4879, -0.6066],\n",
      "        [-3.9989, -1.9150,  0.0102],\n",
      "        ...,\n",
      "        [ 0.6043, -0.7436,  0.1443],\n",
      "        [ 0.5996, -0.8202,  0.1978],\n",
      "        [ 0.6155, -0.8070,  0.2134]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  3.600034713745117\n",
      "smoothed_heaviside_loss:  0.015448002144694328\n",
      "Epoch 21: loss = 3.8902018070220947\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.03061697818338871 chamfer_loss_mesh:  0.2385508269071579\n",
      "95% percentile of tet volume: 6.039037543814629e-05\n",
      "max tet volume: 0.0023866272531449795\n",
      "tensor([[-0.8351, -0.4606, -0.6048],\n",
      "        [-0.3011, -0.8183, -3.3160],\n",
      "        [-0.2764, -3.0358, -0.2644],\n",
      "        ...,\n",
      "        [-0.2398,  0.6208, -0.8613],\n",
      "        [-0.1237,  0.4997, -0.8589],\n",
      "        [-0.1927,  0.5783, -0.9207]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  2.8538804054260254\n",
      "smoothed_heaviside_loss:  0.015500369481742382\n",
      "Epoch 22: loss = 3.1385486125946045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.030081130098551512 chamfer_loss_mesh:  0.23665328626520932\n",
      "95% percentile of tet volume: 6.040582593414001e-05\n",
      "max tet volume: 0.0022965623065829277\n",
      "tensor([[-5.3893, -0.6854, -0.2201],\n",
      "        [-0.4836, -1.0232, -0.4665],\n",
      "        [-0.8449, -1.4130, -0.1845],\n",
      "        ...,\n",
      "        [ 0.9459, -0.2210,  0.1572],\n",
      "        [ 0.8252, -0.2525,  0.0699],\n",
      "        [ 0.9479, -0.2213,  0.1560]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  13.174135208129883\n",
      "smoothed_heaviside_loss:  0.01556418277323246\n",
      "Epoch 23: loss = 13.456433296203613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.02939969301223755 chamfer_loss_mesh:  0.2287928364239633\n",
      "95% percentile of tet volume: 6.0433867474785075e-05\n",
      "max tet volume: 0.0022451961413025856\n",
      "tensor([[ -0.2568, -19.7086,  -2.2071],\n",
      "        [ -2.1069,  -0.6539,  -0.1251],\n",
      "        [  0.1800,  -1.7365,  -0.2339],\n",
      "        ...,\n",
      "        [  0.9890,  -0.2590,   0.1221],\n",
      "        [  1.0933,  -0.1055,   0.1479],\n",
      "        [  1.1797,  -0.1716,   0.2278]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  84.60342407226562\n",
      "smoothed_heaviside_loss:  0.015596807934343815\n",
      "Epoch 24: loss = 84.87721252441406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.028928108513355255 chamfer_loss_mesh:  0.22676301887258887\n",
      "95% percentile of tet volume: 6.043853500159457e-05\n",
      "max tet volume: 0.0022387129720300436\n",
      "tensor([[ -0.6261,  -0.6293,  -0.4490],\n",
      "        [ -0.2357, -17.0611,  -4.1419],\n",
      "        [ -0.8462,  -0.3101,   0.0882],\n",
      "        ...,\n",
      "        [  0.2798,   0.9133,  -0.3568],\n",
      "        [  0.2696,   1.0039,  -0.3536],\n",
      "        [  0.3228,   0.9155,  -0.2726]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  49.88338088989258\n",
      "smoothed_heaviside_loss:  0.01562631130218506\n",
      "Epoch 25: loss = 50.15469741821289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.028257917147129774 chamfer_loss_mesh:  0.22351028746925294\n",
      "95% percentile of tet volume: 6.0412989114411175e-05\n",
      "max tet volume: 0.0022658687084913254\n",
      "tensor([[-0.3709, -1.0704, -0.1570],\n",
      "        [-3.9501, -0.4583, -0.6740],\n",
      "        [-0.5162, -0.3873, -0.6393],\n",
      "        ...,\n",
      "        [ 0.8736, -0.4540,  0.0319],\n",
      "        [ 0.8899, -0.4781,  0.0456],\n",
      "        [ 0.8665, -0.4727,  0.0483]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  28.091222763061523\n",
      "smoothed_heaviside_loss:  0.01564292050898075\n",
      "Epoch 26: loss = 28.358633041381836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.027704313397407532 chamfer_loss_mesh:  0.21873146761208773\n",
      "95% percentile of tet volume: 6.040630614734255e-05\n",
      "max tet volume: 0.002385851461440325\n",
      "tensor([[ -0.1756, -13.3601,  -0.4954],\n",
      "        [ -0.2635,  -1.1742,  -1.0732],\n",
      "        [-79.5439,  -3.4965,   0.1686],\n",
      "        ...,\n",
      "        [  0.7303,  -0.5046,  -0.5076],\n",
      "        [  0.7328,  -0.5115,  -0.5096],\n",
      "        [  0.6096,  -0.4754,  -0.7014]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  55.73892593383789\n",
      "smoothed_heaviside_loss:  0.015673553571105003\n",
      "Epoch 27: loss = 56.00103759765625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.027443915605545044 chamfer_loss_mesh:  0.21698960335925221\n",
      "95% percentile of tet volume: 6.0451679019024596e-05\n",
      "max tet volume: 0.002500042784959078\n",
      "tensor([[-6.2464e-01, -6.4586e-01, -4.4841e-01],\n",
      "        [-2.3616e-01, -1.7895e+01, -4.4960e+00],\n",
      "        [-6.0483e-01, -7.0105e-01, -4.4656e-01],\n",
      "        ...,\n",
      "        [ 1.1529e+00, -8.2514e-04, -2.7867e-01],\n",
      "        [ 1.1709e+00,  2.3028e-02, -2.5956e-01],\n",
      "        [ 1.1561e+00,  1.0666e-01, -2.7144e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  56.94941711425781\n",
      "smoothed_heaviside_loss:  0.015693090856075287\n",
      "Epoch 28: loss = 57.20954513549805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.026849505957216024 chamfer_loss_mesh:  0.21295581245794892\n",
      "95% percentile of tet volume: 6.048278737580404e-05\n",
      "max tet volume: 0.0026055672205984592\n",
      "tensor([[ -0.2622,  -1.1384,  -1.0617],\n",
      "        [ -0.2360, -18.1263,  -4.5832],\n",
      "        [ -0.2703,  -0.9719,  -0.4408],\n",
      "        ...,\n",
      "        [  0.7780,   0.4655,   0.2548],\n",
      "        [  0.8655,   0.4952,   0.1178],\n",
      "        [  0.7638,   0.5363,   0.0448]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  15.750080108642578\n",
      "smoothed_heaviside_loss:  0.01571020856499672\n",
      "Epoch 29: loss = 16.005596160888672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.026443551760166883 chamfer_loss_mesh:  0.21822635608259588\n",
      "95% percentile of tet volume: 6.046143244020641e-05\n",
      "max tet volume: 0.0026976808439940214\n",
      "tensor([[-1.6506, -0.5590, -0.2966],\n",
      "        [-6.4517, -0.5474, -0.3457],\n",
      "        [-2.3623, -0.7685, -0.0068],\n",
      "        ...,\n",
      "        [-0.0335,  1.1533,  0.1920],\n",
      "        [-0.1175,  1.0600,  0.1186],\n",
      "        [-0.0969,  1.0970,  0.2475]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  14.692660331726074\n",
      "smoothed_heaviside_loss:  0.01573142595589161\n",
      "Epoch 30: loss = 14.953062057495117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.026049918960779905 chamfer_loss_mesh:  0.2179719158448279\n",
      "95% percentile of tet volume: 6.048884097253904e-05\n",
      "max tet volume: 0.002776780631393194\n",
      "tensor([[-0.4369, -0.5481, -0.3353],\n",
      "        [-0.4458, -0.5936, -0.7691],\n",
      "        [-1.1396, -7.3671,  0.0993],\n",
      "        ...,\n",
      "        [-0.3723,  0.2209, -1.1214],\n",
      "        [-0.3291,  0.1624, -1.0386],\n",
      "        [-0.3162,  0.0702, -1.0354]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  13.243823051452637\n",
      "smoothed_heaviside_loss:  0.015746327117085457\n",
      "Epoch 31: loss = 13.50359058380127\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.025509046390652657 chamfer_loss_mesh:  0.2137994160875678\n",
      "95% percentile of tet volume: 6.047877104720101e-05\n",
      "max tet volume: 0.0024652518332004547\n",
      "tensor([[-1.7180e+01, -5.6953e+00,  4.8488e-02],\n",
      "        [-2.5140e-01, -5.0721e+01, -4.9943e-01],\n",
      "        [-5.1001e-01,  6.4266e-01, -3.9456e-01],\n",
      "        ...,\n",
      "        [ 9.6002e-01, -1.1960e-01,  3.0952e-01],\n",
      "        [ 9.2780e-01, -2.1549e-01,  4.1865e-01],\n",
      "        [ 9.4606e-01, -2.3104e-01,  4.0681e-01]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  11.91240406036377\n",
      "smoothed_heaviside_loss:  0.015781503170728683\n",
      "Epoch 32: loss = 12.16749382019043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.025544087402522564 chamfer_loss_mesh:  0.21089182700961828\n",
      "95% percentile of tet volume: 6.0482027038233355e-05\n",
      "max tet volume: 0.0024815271608531475\n",
      "tensor([[-20.9290,  -0.7098,  -0.2289],\n",
      "        [ -0.7015,  -0.5719,  -0.5336],\n",
      "        [ -0.2893,  -1.3623,  -0.5385],\n",
      "        ...,\n",
      "        [  0.3373,  -0.4912,   0.9184],\n",
      "        [  0.0998,  -0.4441,   1.0892],\n",
      "        [  0.3554,  -0.6719,   0.8626]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  117.93093872070312\n",
      "smoothed_heaviside_loss:  0.01580660045146942\n",
      "Epoch 33: loss = 118.18318176269531\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.025220008101314306 chamfer_loss_mesh:  0.20698727166745812\n",
      "95% percentile of tet volume: 6.0495473007904366e-05\n",
      "max tet volume: 0.0024977209977805614\n",
      "tensor([[-0.4852, -0.7157, -0.4698],\n",
      "        [-0.6908, -0.6509, -0.3633],\n",
      "        [-0.4820,  0.2684, -0.3944],\n",
      "        ...,\n",
      "        [-0.5063,  0.1706,  0.3735],\n",
      "        [-0.5306, -0.1226,  0.5735],\n",
      "        [-0.4233, -0.0553,  1.2018]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  89.52132415771484\n",
      "smoothed_heaviside_loss:  0.015841549262404442\n",
      "Epoch 34: loss = 89.76937103271484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "cvt_loss:  0.02490272279828787 chamfer_loss_mesh:  0.20018314535263926\n",
      "95% percentile of tet volume: 6.047832721378654e-05\n",
      "max tet volume: 0.002523553092032671\n",
      "tensor([[ -0.6493,  -1.1465,  -0.0601],\n",
      "        [ -0.4198, -23.3404,  -0.1974],\n",
      "        [ -0.5204,  -0.6575,  -0.4445],\n",
      "        ...,\n",
      "        [  0.6204,  -0.7665,  -0.1084],\n",
      "        [  0.6199,  -0.7660,  -0.1085],\n",
      "        [  0.6203,  -0.7664,  -0.1093]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "eikonal_loss:  61.04266357421875\n",
      "smoothed_heaviside_loss:  0.015882549807429314\n",
      "Epoch 35: loss = 61.283634185791016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m      7\u001b[39m     sites = torch.from_numpy(sites).to(device).requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# import cProfile, pstats\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# prof.export_chrome_trace(\"trace.json\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     sites, optimized_sites_sdf = \u001b[43mtrain_DCCVT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambda_weights\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     sites_np = sites.detach().cpu().numpy()\n\u001b[32m     35\u001b[39m     np.save(site_file_path, sites_np)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mtrain_DCCVT\u001b[39m\u001b[34m(sites, sites_sdf, max_iter, stop_train_threshold, upsampling, lambda_weights, voroloss_optim)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mv_vect shape: \u001b[39m\u001b[33m\"\u001b[39m, v_vect.shape)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     v_vect, f_vect, sites_sdf_grads, tets_sdf_grads, W = \u001b[43msu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_clipped_mesh_numba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md3dsimplices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_mesh\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m build_mesh:\n\u001b[32m     89\u001b[39m     triangle_faces = [[f[\u001b[32m0\u001b[39m], f[i], f[i + \u001b[32m1\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m f_vect \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(f) - \u001b[32m1\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/sdfpred_utils/sdfpred_utils.py:1874\u001b[39m, in \u001b[36mget_clipped_mesh_numba\u001b[39m\u001b[34m(sites, model, d3dsimplices, clip, sites_sdf, build_mesh, quaternion_slerp, barycentric_weights)\u001b[39m\n\u001b[32m   1871\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1872\u001b[39m     \u001b[38;5;66;03m# print(\"-> clipping\")\u001b[39;00m\n\u001b[32m   1873\u001b[39m     vertices_sdf = interpolate_sdf_of_vertices(all_vor_vertices, d3d, sites, sites_sdf)\n\u001b[32m-> \u001b[39m\u001b[32m1874\u001b[39m     sites_sdf_grad, tets_sdf_grads, W = \u001b[43msdf_space_grad_pytorch_diego_sites_tets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md3d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1875\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m barycentric_weights:\n\u001b[32m   1876\u001b[39m         \u001b[38;5;66;03m# Use barycentric weights for interpolation\u001b[39;00m\n\u001b[32m   1877\u001b[39m         vertices_sdf_grad = interpolate_sdf_grad_of_vertices(\n\u001b[32m   1878\u001b[39m             all_vor_vertices, d3d, sites, sites_sdf_grad, quaternion_slerp=quaternion_slerp\n\u001b[32m   1879\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/sdfpred_utils/sdfpred_utils.py:1700\u001b[39m, in \u001b[36msdf_space_grad_pytorch_diego_sites_tets\u001b[39m\u001b[34m(sites, sdf, tets)\u001b[39m\n\u001b[32m   1697\u001b[39m dX_T = dX.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# (M, 3, 4)\u001b[39;00m\n\u001b[32m   1699\u001b[39m G = torch.bmm(dX_T, dX)  \u001b[38;5;66;03m# (M, 3, 3)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m Ginv = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (M, 3, 3)\u001b[39;00m\n\u001b[32m   1702\u001b[39m W = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mmij,mnj->mni\u001b[39m\u001b[33m\"\u001b[39m, Ginv, dX)  \u001b[38;5;66;03m# (M, 4, 3)\u001b[39;00m\n\u001b[32m   1704\u001b[39m sdf_stack = torch.stack([sdf_a, sdf_b, sdf_c, sdf_d], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (M, 4)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True,  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(\n",
    "    #         sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    #     )\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/gargoyle1000_1000_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zc true_Sdf shape:  torch.Size([6071, 4])\n",
      "zc optimized sdf : torch.Size([6071, 4])\n",
      "sum of zc true Sdf:  254.93850708007812\n",
      "sum of zc opti Sdf:  188.321533203125\n",
      "Diff   of   sum:  66.61695861816406\n",
      "Mean of zc true Sdf:  0.002743244869634509\n"
     ]
    }
   ],
   "source": [
    "# metric between sites sdf values and their corresponding sdf values on hotspot model\n",
    "true_Sdf = model(sites).squeeze(-1)\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "vertices_to_compute, bisectors_to_compute, used_tet = su.compute_zero_crossing_vertices_3d(\n",
    "    sites, None, None, d3dsimplices, sdf_v\n",
    ")\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "d3d = d3dsimplices[used_tet]\n",
    "zc_sdf = sdf_v[d3d]\n",
    "zc_truesdf = true_Sdf[d3d]\n",
    "print(\"zc true_Sdf shape: \", zc_truesdf.shape)\n",
    "print(\"zc optimized sdf :\", zc_sdf.shape)\n",
    "print(\"sum of zc true Sdf: \", torch.sum(zc_truesdf).item())\n",
    "print(\"sum of zc opti Sdf: \", torch.sum(zc_sdf).item())\n",
    "print(\"Diff   of   sum: \", torch.sum(zc_truesdf - zc_sdf).item())\n",
    "print(\"Mean of zc true Sdf: \", torch.mean(zc_truesdf - zc_sdf).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 200117\n",
      "Delaunay simplices shape: [[ 1056  2048    64  1088]\n",
      " [ 1056    64  2080  1024]\n",
      " [ 3076   224  1024   288]\n",
      " ...\n",
      " [22846 21790 21822 22813]\n",
      " [21822 22845 21854 22846]\n",
      " [21822 22845 21821 21854]]\n",
      "Max vertex index in simplices: 32767\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 32768\n",
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 200117\n",
      "Delaunay simplices shape: [[12357 13413 13381 13380]\n",
      " [ 1056  2048    64  1088]\n",
      " [ 1056    64  2080  1024]\n",
      " ...\n",
      " [25746 24754 24722 24755]\n",
      " [24754 25746 25778 24755]\n",
      " [21822 22845 21821 21854]]\n",
      "Max vertex index in simplices: 32767\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 32768\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "\n",
    "f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final unclipped polygon mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\"\n",
    ")\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf_v.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"MTET\", v_vect.detach().cpu().numpy(), faces.detach().cpu().numpy(), back_face_policy=\"identical\"\n",
    ")\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites, sdf = train_DCCVT(\n",
    "#     sites, sdf_v, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=True\n",
    "# )\n",
    "# (\n",
    "#     v_vect,\n",
    "#     f_vect,\n",
    "#     _,\n",
    "#     _,\n",
    "#     _,\n",
    "# ) = su.get_clipped_mesh_numba(sites, None, None, False, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5485 is out of bounds for axis 0 with size 4062",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m     accuracy = np.mean(dists_ours_to_gt**\u001b[32m2\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, completeness\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m ours_pts, _ = \u001b[43msample_points_on_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_obj_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m m = mesh[\u001b[32m1\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m gt_pts, _ = sample_points_on_mesh(m + \u001b[33m\"\u001b[39m\u001b[33m.obj\u001b[39m\u001b[33m\"\u001b[39m, n_points=\u001b[32m100000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36msample_points_on_mesh\u001b[39m\u001b[34m(mesh_path, n_points)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_points_on_mesh\u001b[39m(mesh_path, n_points=\u001b[32m100000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     mesh = \u001b[43mtrimesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# normalize mesh\u001b[39;00m\n\u001b[32m     10\u001b[39m     mesh.apply_translation(-mesh.centroid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/load.py:111\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file_obj, file_type, resolver, force, allow_remote, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mTHIS FUNCTION IS DEPRECATED but there are no current plans for it to be removed.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m  Loaded geometry as trimesh classes\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# call the most general loading case into a `Scene`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m loaded = \u001b[43mload_scene\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_remote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_remote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force == \u001b[33m\"\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# new code should use `load_mesh` for this\u001b[39;00m\n\u001b[32m    121\u001b[39m     log.debug(\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`trimesh.load(force=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)` is a compatibility wrapper for `trimesh.load_mesh`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/load.py:216\u001b[39m, in \u001b[36mload_scene\u001b[39m\u001b[34m(file_obj, file_type, resolver, allow_remote, metadata, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg.file_type \u001b[38;5;129;01min\u001b[39;00m mesh_loaders:\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# use mesh loader\u001b[39;00m\n\u001b[32m    214\u001b[39m     parsed = deepcopy(kwargs)\n\u001b[32m    215\u001b[39m     parsed.update(\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[43mmesh_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     )\n\u001b[32m    224\u001b[39m     loaded = _load_kwargs(**parsed)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg.file_type \u001b[38;5;129;01min\u001b[39;00m compressed_loaders:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# for archives, like ZIP files\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/obj.py:244\u001b[39m, in \u001b[36mload_obj\u001b[39m\u001b[34m(file_obj, resolver, group_material, skip_materials, maintain_order, metadata, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m     mask_v = np.zeros(\u001b[38;5;28mlen\u001b[39m(v), dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[43mmask_v\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m]\u001b[49m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# reconstruct the faces with the new vertex indices\u001b[39;00m\n\u001b[32m    247\u001b[39m inverse = np.zeros(\u001b[38;5;28mlen\u001b[39m(v), dtype=np.int64)\n",
      "\u001b[31mIndexError\u001b[39m: index 5485 is out of bounds for axis 0 with size 4062"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    print(mesh_path)\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT  Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours  GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours  GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT  Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "\n",
    "    # Normalize mesh (centered and scaled uniformly)\n",
    "    bbox = mesh.bounds\n",
    "    center = mesh.centroid\n",
    "    scale = np.linalg.norm(bbox[1] - bbox[0])\n",
    "    mesh.apply_translation(-center)\n",
    "    mesh.apply_scale(1.0 / scale)\n",
    "\n",
    "    # Export normalized mesh\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "_, _ = sample_points_on_mesh(\n",
    "    \"/home/wylliam/dev/Kyushu_experiments/outputs/gargoyle_unconverged/cdp1000_v0_cvt100_clipTrue_buildFalse_upsampling0_num_centroids32_target_size32_final.obj\",\n",
    "    n_points=100000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
