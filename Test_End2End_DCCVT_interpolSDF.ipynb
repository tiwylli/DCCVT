{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "#lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "#mesh = [\"sphere\"]\n",
    "\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "\n",
    "# # # \n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "torch.manual_seed(69)\n",
    "#add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "\n",
    "# def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "#     Args:\n",
    "#         points: (N, 3) tensor of 3D query points\n",
    "#         center: (3,) tensor specifying the center of the sphere\n",
    "#         radius: float, radius of the sphere\n",
    "\n",
    "#     Returns:\n",
    "#         sdf: (N,) tensor of signed distances\n",
    "#     \"\"\"\n",
    "#     return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "# # generate points on the sphere\n",
    "# mnfld_points = torch.randn(grid*grid*150, 3, device=device)\n",
    "# mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5 \n",
    "# mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 3.03872 MB, Reserved: 23.068672 MB\n",
      "torch.Size([32768])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#add mnfld points with random noise to sites \n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 32**3 - num_centroids\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "sdf0 = model(sites)\n",
    "\n",
    "#sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "##sdf0 += torch.randn_like(sdf0) * noise_scale/2\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "\n",
    "\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n",
    "\n",
    "#print(sdf_grad0.shape)\n",
    "#print(sdf_grad0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites shape:  torch.Size([32768, 3])\n",
      "-> tracing mesh\n",
      "-> clipping\n",
      "sorted(used) torch.Size([5090])\n",
      "sorted(used) 0 tensor([ 55, 109, 113, 159, 314])\n",
      "d3d[sorted(used)] torch.Size([5090, 4])\n",
      "tets shape: torch.Size([5090, 4])\n",
      "tets 0: tensor([18870, 19895, 18871, 19894], device='cuda:0')\n",
      "tet_sites shape: torch.Size([5090, 4, 3]), tet_sdf shape: torch.Size([5090, 4]), tet_grads shape: torch.Size([5090, 4, 3])\n",
      "-> not tracing mesh\n",
      "-> not clipping\n",
      "-> not tracing mesh\n",
      "-> clipping\n",
      "vertices_to_compute torch.Size([5090, 4])\n",
      "d3d[mask] torch.Size([5090, 4, 4])\n",
      "tets shape: torch.Size([5090, 4, 4])\n",
      "tets 0: tensor([[23122, 23123, 22099, 23090],\n",
      "        [22256, 21200, 22224, 22225],\n",
      "        [22038, 22005, 23029, 23030],\n",
      "        [23185, 23186, 23217, 22161]], device='cuda:0')\n",
      "tet_sites shape: torch.Size([5090, 4, 4, 3]), tet_sdf shape: torch.Size([5090, 4, 4]), tet_grads shape: torch.Size([5090, 4, 4, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m v_vect, _, sdf_verts, sdf_verts_grads \u001b[38;5;241m=\u001b[39m su\u001b[38;5;241m.\u001b[39mget_clipped_mesh_numba(sites, \u001b[38;5;28;01mNone\u001b[39;00m, d3dsimplices, \u001b[38;5;28;01mFalse\u001b[39;00m, sdf0, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m ps\u001b[38;5;241m.\u001b[39mregister_point_cloud(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj verts from unmeshed unclipp\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_vect\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 27\u001b[0m v_vect, _, sdf_verts, sdf_verts_grads, _ \u001b[38;5;241m=\u001b[39m \u001b[43msu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_clipped_mesh_numba\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md3dsimplices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m ps\u001b[38;5;241m.\u001b[39mregister_point_cloud(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj verts from unmeshed clip\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_vect\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     31\u001b[0m ps\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/dev/Kyushu_experiments/sdfpred_utils/sdfpred_utils.py:1138\u001b[0m, in \u001b[0;36mget_clipped_mesh_numba\u001b[0;34m(sites, model, d3dsimplices, clip, sites_sdf, build_mesh)\u001b[0m\n\u001b[1;32m   1135\u001b[0m sdf_verts \u001b[38;5;241m=\u001b[39m vertices_sdf[vertices_to_compute]\n\u001b[1;32m   1136\u001b[0m grads \u001b[38;5;241m=\u001b[39m vertices_sdf_grad[vertices_to_compute]  \u001b[38;5;66;03m# (M,3)\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m proj_vertices, tet_probs \u001b[38;5;241m=\u001b[39m \u001b[43mtet_plane_clipping\u001b[49m\u001b[43m(\u001b[49m\u001b[43md3d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvertices_to_compute\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (M,3)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proj_vertices, \u001b[38;5;28;01mNone\u001b[39;00m, sdf_verts, grads, tet_probs\n",
      "File \u001b[0;32m~/dev/Kyushu_experiments/sdfpred_utils/sdfpred_utils.py:1239\u001b[0m, in \u001b[0;36mtet_plane_clipping\u001b[0;34m(tets, sites, sdf_values, sdf_grads, voronoi_vertices)\u001b[0m\n\u001b[1;32m   1236\u001b[0m centered \u001b[38;5;241m=\u001b[39m projected_pts \u001b[38;5;241m-\u001b[39m centroid                        \u001b[38;5;66;03m# (M, 4, 3)\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# Compute covariance matrix\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m cov \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmni,mnj->mij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentered\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# (M, 3, 3)\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;66;03m# Compute eigenvectors â€” last one is normal\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m _, eigvecs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigh(cov)                         \u001b[38;5;66;03m# (M, 3), (M, 3, 3)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()       \n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sdf0.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\", vminmax=(-0.00005, 0.00005))\n",
    "\n",
    "# v_vect, f_vect, sdf_verts, sdf_verts_grads = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "# ps_mesh = ps.register_surface_mesh(\"sdf unclipped initial mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\")\n",
    "# ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, tet_probs = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\"sdf clipped initial mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\")\n",
    "ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1,3).detach().cpu().numpy())\n",
    "ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1,3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "v_vect, _, sdf_verts, sdf_verts_grads = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, False)\n",
    "ps.register_point_cloud(\"proj verts from unmeshed unclipp\", v_vect.detach().cpu().numpy())\n",
    "\n",
    "v_vect, _, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, False)\n",
    "ps.register_point_cloud(\"proj verts from unmeshed clip\", v_vect.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, sites_sdf, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites*.1},\n",
    "    {'params': [sites_sdf], 'lr': lr_sites*.1},\n",
    "])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    #optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    #optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        # if epoch % 2 == 0:\n",
    "        #     optimizer = optimizer_sites\n",
    "        #     clip = False\n",
    "        # else:  \n",
    "        #     optimizer = optimizer_sdf\n",
    "        #     clip = True\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, sites_sdf)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "    \n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        #print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "\n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        # print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        #sites_sdf_grads = sdf_site_gradients(sites, torch.tensor(d3dsimplices, device=device), sites_sdf)\n",
    "        #sites_positions = sites.detach().clone()\n",
    "        \n",
    "        v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sites_sdf, True)\n",
    "        \n",
    "\n",
    "        triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "        triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "        hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "        \n",
    "        chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "        \n",
    "        #chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "        \n",
    "        #chamfer_loss_mesh = sphere_sdf(v_vect, torch.zeros(3).to(device), 0.50).abs().mean()\n",
    "        \n",
    "        #print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        \n",
    "        sites_loss = (\n",
    "            #lambda_cvt * cvt_loss +\n",
    "            lambda_chamfer * chamfer_loss_mesh \n",
    "            #lambda_chamfer * chamfer_loss_points\n",
    "            #lambda_chamfer * voroloss_loss\n",
    "        )\n",
    "\n",
    "        # voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        # sites_loss = (\n",
    "        #     lambda_chamfer * voroloss_loss\n",
    "        # )\n",
    "\n",
    "        loss = sites_loss #+ sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        \n",
    "        #print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        #print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        #sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "                \n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "       \n",
    "        # if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "        #     print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "        #     sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "        #     sites = sites.detach().requires_grad_(True)\n",
    "        #     optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "        #                                   #{'params': model.parameters(), 'lr': lr_model}\n",
    "        #                                   ])\n",
    "        #     upsampled += 1.0\n",
    "        #     print(\"sites length AFTER: \",len(sites))\n",
    "        \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0 or epoch == max_iter:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "            \n",
    "            #ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            ps.register_point_cloud('sampled points end', v_vect.detach().cpu().numpy(), enabled=False)\n",
    "            \n",
    "            ps_mesh = ps.register_surface_mesh(f\"{epoch} sdf clipped pmesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\", enabled=False)\n",
    "            ps_mesh.add_vector_quantity(f\"{epoch} sdf verts grads\", sdf_verts_grads.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "                \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            #model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> tracing mesh\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # # \n",
    "    sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/gargoyle1_1_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "\n",
    "#model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "\n",
    "\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\"vis_grid_pred\", sdf_v.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\", vminmax=(-0.15, 0.15))\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "d3d shape: torch.Size([223565, 4]), dtype: torch.int64\n",
      "vor_vertices shape: torch.Size([223565, 3]), dtype: torch.float32\n",
      "-> clipping\n",
      "tet_sites shape: torch.Size([5148, 4, 3]), tet_sdf shape: torch.Size([5148, 4]), tet_grads shape: torch.Size([5148, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "#v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "#ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "v_vect, f_vect, _, _, _  = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v)\n",
    "ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect) \n",
    "\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
