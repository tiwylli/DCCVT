{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "\n",
    "# import diffvoronoi\n",
    "import pygdel3d\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "# Improve reproducibility\n",
    "torch.manual_seed(69)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(69)\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "ROOT_DIR = \"/home/wylliam/dev/Kyushu_experiments\"\n",
    "\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"hole_gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\n",
    "#     \"gargoyle\",\n",
    "#     f\"{ROOT_DIR}/mesh/thingi32/64764\",\n",
    "# ]\n",
    "# trained_model_path = f\"{ROOT_DIR}/hotspots_model/thingi32/64764.pth\"\n",
    "\n",
    "\n",
    "# mesh = [\"zombie\", f\"{ROOT_DIR}/mesh/thingi32/398259\"]\n",
    "# trained_model_path = f\"{ROOT_DIR}/hotspots_model/thingi32/398259.pth\"\n",
    "\n",
    "mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_500.pth\"\n",
    "\n",
    "\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([4096, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 16**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# def z_axis_weights(points, mode=\"exp\", strength=3.0, percentile_clip=(0.02, 0.98), eps=1e-12):\n",
    "#     \"\"\"\n",
    "#     points: (N,3) or (1,N,3)\n",
    "#     \"\"\"\n",
    "#     # --- normalize shape ---\n",
    "#     if points.dim() == 3 and points.shape[0] == 1:\n",
    "#         points = points.squeeze(0)\n",
    "#     if not (points.dim() == 2 and points.shape[1] == 3):\n",
    "#         raise ValueError(f\"points must be (N,3) or (1,N,3); got {tuple(points.shape)}\")\n",
    "\n",
    "#     z = points[:, 2]\n",
    "\n",
    "#     # Robust min-max -> z_norm in [0,1]\n",
    "#     if percentile_clip is not None:\n",
    "#         lo, hi = percentile_clip\n",
    "#         z_lo = torch.quantile(z, lo)\n",
    "#         z_hi = torch.quantile(z, hi)\n",
    "#     else:\n",
    "#         z_lo, z_hi = z.min(), z.max()\n",
    "\n",
    "#     denom = (z_hi - z_lo).abs()\n",
    "#     if denom < 1e-12:\n",
    "#         # all z ~ equal -> uniform weights\n",
    "#         return torch.ones_like(z)\n",
    "\n",
    "#     z_norm = ((z - z_lo) / (denom + eps)).clamp(0.0, 1.0)\n",
    "\n",
    "#     if mode == \"linear\":\n",
    "#         w = 1.0 - z_norm\n",
    "#     elif mode == \"poly\":\n",
    "#         p = float(strength)\n",
    "#         w = (1.0 - z_norm).pow(p)\n",
    "#     elif mode == \"exp\":\n",
    "#         lam = float(strength)\n",
    "#         w = torch.exp(-lam * z_norm)\n",
    "#     elif mode == \"sigmoid\":\n",
    "#         k = float(strength)\n",
    "#         w = torch.sigmoid(-k * (z_norm - 0.5))\n",
    "#     else:\n",
    "#         raise ValueError(\"mode must be one of: linear | poly | exp | sigmoid\")\n",
    "\n",
    "#     return (w - w.min()).clamp_min(0) + eps\n",
    "\n",
    "\n",
    "# def weighted_subsample(points, M, weights, *, allow_replacement=None):\n",
    "#     \"\"\"\n",
    "#     points:   (N,3) or (1,N,3)\n",
    "#     weights:  (N,) or (1,N)\n",
    "#     If allow_replacement is None: auto True when M>N, else False.\n",
    "#     \"\"\"\n",
    "#     # --- normalize shapes ---\n",
    "#     if points.dim() == 3 and points.shape[0] == 1:\n",
    "#         points = points.squeeze(0)\n",
    "#     if not (points.dim() == 2 and points.shape[1] == 3):\n",
    "#         raise ValueError(f\"points must be (N,3) or (1,N,3); got {tuple(points.shape)}\")\n",
    "\n",
    "#     if weights.dim() == 2 and weights.shape[0] == 1:\n",
    "#         weights = weights.squeeze(0)\n",
    "#     if weights.dim() != 1 or weights.shape[0] != points.shape[0]:\n",
    "#         raise ValueError(f\"weights must be (N,) matching points; got {tuple(weights.shape)} vs N={points.shape[0]}\")\n",
    "\n",
    "#     N = points.shape[0]\n",
    "#     if allow_replacement is None:\n",
    "#         allow_replacement = M > N\n",
    "\n",
    "#     w = (weights.float() - weights.min()).clamp_min(0) + 1e-12\n",
    "#     M_eff = min(M, N) if not allow_replacement else M\n",
    "\n",
    "#     idx = torch.multinomial(w, M_eff, replacement=allow_replacement)\n",
    "#     return points[idx], idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n",
      "torch.float32\n",
      "torch.Size([4096, 3])\n",
      "Allocated: 63.913472 MB, Reserved: 65.011712 MB\n",
      "torch.Size([4096])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "\n",
    "import sys\n",
    "\n",
    "if mesh[0] != \"sphere\":\n",
    "    sys.path.append(\"3rdparty/HotSpot\")\n",
    "    from dataset import shape_3d\n",
    "    import models.Net as Net\n",
    "\n",
    "    loss_type = \"igr_w_heat\"\n",
    "    loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "    train_set = shape_3d.ReconDataset(\n",
    "        file_path=mesh[1] + \".ply\",\n",
    "        n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "        n_samples=10001,  # args.n_iterations,\n",
    "        grid_res=256,  # args.grid_res,\n",
    "        grid_range=1.1,  # args.grid_range,\n",
    "        sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "        sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "        n_random_samples=7500,  # args.n_random_samples,\n",
    "        resample=True,\n",
    "        compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "        scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "    )\n",
    "\n",
    "    model = Net.Network(\n",
    "        latent_size=0,  # args.latent_size,\n",
    "        in_dim=3,\n",
    "        decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "        nl=\"sine\",  # args.nl,\n",
    "        encoder_type=\"none\",  # args.encoder_type,\n",
    "        decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "        neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "        init_type=\"mfgi\",  # args.init_type,\n",
    "        sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "        n_repeat_period=30,  # args.n_repeat_period,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ######\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_data = next(iter(test_dataloader))\n",
    "    mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "    # add noise to mnfld_points\n",
    "    # mnfld_points += torch.randn_like(mnfld_points) * (2 / 32) * (32 / 100)\n",
    "\n",
    "    mnfld_points.requires_grad_()\n",
    "    print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "    if torch.cuda.is_available():\n",
    "        map_location = torch.device(\"cuda\")\n",
    "    else:\n",
    "        map_location = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "    sdf0 = model(sites)\n",
    "\n",
    "else:\n",
    "\n",
    "    def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3) tensor of 3D query points\n",
    "            center: (3,) tensor specifying the center of the sphere\n",
    "            radius: float, radius of the sphere\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,) tensor of signed distances\n",
    "        \"\"\"\n",
    "        return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "    def sphere_sdf_with_noise(\n",
    "        points: torch.Tensor, center: torch.Tensor, radius: float, noise_amplitude=0.05\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sphere SDF with smooth directional noise added near the surface.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3)\n",
    "            center: (3,)\n",
    "            radius: float\n",
    "            noise_amplitude: float\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,)\n",
    "        \"\"\"\n",
    "        rel = points - center\n",
    "        norm = torch.norm(rel, dim=-1)  # (N,)\n",
    "        base_sdf = norm - radius  # (N,)\n",
    "\n",
    "        # Smooth periodic noise based on direction\n",
    "        unit_dir = rel / (norm.unsqueeze(-1) + 1e-9)  # (N,3)\n",
    "        noise = torch.sin(10 * unit_dir[:, 0]) * torch.sin(10 * unit_dir[:, 1]) * torch.sin(10 * unit_dir[:, 2])\n",
    "\n",
    "        # Weight noise so it mostly affects surface area\n",
    "        falloff = torch.exp(-20 * (base_sdf**2))  # (N,) ~1 near surface, ~0 far\n",
    "        sdf = base_sdf + noise_amplitude * noise * falloff\n",
    "\n",
    "        return sdf\n",
    "\n",
    "    # generate points on the sphere\n",
    "    mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "    mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "    mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()\n",
    "    sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "    # sdf0 = sphere_sdf_with_noise(sites, torch.zeros(3).to(device), 0.50, noise_amplitude=0.1)\n",
    "\n",
    "\n",
    "# add mnfld points with random noise to sites\n",
    "N = mnfld_points.squeeze(0).shape[0]\n",
    "num_samples = 18**3 - (num_centroids)\n",
    "num_samples = 16**3\n",
    "idx = torch.randint(0, N, (num_samples,))\n",
    "sampled = mnfld_points.squeeze(0)[idx]\n",
    "perturbed = sampled + (torch.rand_like(sampled) - 0.5) * noise_scale * 10\n",
    "\n",
    "\n",
    "# M = 16**3  # 4096\n",
    "# w = z_axis_weights(mnfld_points, mode=\"exp\", strength=4.0)\n",
    "\n",
    "# print(\"N =\", mnfld_points.squeeze(0).shape[0], \"M =\", M)  # sanity check\n",
    "\n",
    "# # Option A: keep unique samples only (if N<M, youâ€™ll just get N)\n",
    "# nonuni_pts, idx = weighted_subsample(mnfld_points, M, w, allow_replacement=False)\n",
    "\n",
    "# # (optional) add your jitter\n",
    "# perturbed = nonuni_pts + (torch.rand_like(nonuni_pts) - 0.5) * noise_scale\n",
    "\n",
    "\n",
    "sites = torch.cat((sites, perturbed), dim=0)\n",
    "sdf0 = model(sites)\n",
    "\n",
    "\n",
    "# START FROM SPHERE EXPERIMENTS\n",
    "# sdf0 = torch.norm(sites - torch.zeros(3).to(device), dim=-1) - 0.50\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba12786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "# print(\"Delaunay simplices shape: \", d3dsimplices.shape)\n",
    "\n",
    "# print(\"sites shape: \", sites.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, faces = su.cvt_extraction(sites, sdf0, d3dsimplices, True)\n",
    "ps.register_point_cloud(\"cvt extraction\", p.detach().cpu().numpy(), enabled=False)\n",
    "ps.register_surface_mesh(\"cvt extraction\", p.detach().cpu().numpy(), faces, back_face_policy=\"identical\", enabled=True)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "# ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf0.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"init MTET\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    faces.detach().cpu().numpy(),\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "# ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "    voroloss_optim=False,\n",
    "    Hotspot_model=None,\n",
    "):\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ],\n",
    "            betas=(0.8, 0.95),\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    lambda_shl = lambda_cvt / 10\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    sites_sdf_grads = None\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        # if epoch == 990:\n",
    "        #     max_iter += 1000\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "            if epoch % 100 == 0 and epoch <= 500:\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 5).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "            elif epoch % 100 == 0 and epoch <= 800:\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 2).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "            mtet = False\n",
    "            sites_sdf_grads = None\n",
    "            barycentric_weights = False\n",
    "            ups_m = \"tet_frame\"\n",
    "            no_mp = False\n",
    "\n",
    "            if mtet:\n",
    "                print(\"Using MTET\")\n",
    "                d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "                marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "                    sites.unsqueeze(0), d3dsimplices, sites_sdf.unsqueeze(0), return_tet_idx=False\n",
    "                )\n",
    "                vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "                v_vect = vertices_list[0]\n",
    "                faces = faces_list[0]\n",
    "                print(\"v_vect shape: \", v_vect.shape)\n",
    "\n",
    "            else:\n",
    "                v_vect, faces_or_clippedvert, sites_sdf_grads, tets_sdf_grads, W = su.get_clipped_mesh_numba(\n",
    "                    sites, None, d3dsimplices, clip, sites_sdf, build_mesh, False, barycentric_weights, no_mp\n",
    "                )\n",
    "\n",
    "                # _, bisectors_to_compute, _ = su.compute_zero_crossing_vertices_3d(\n",
    "                #     sites, None, None, d3dsimplices, sites_sdf\n",
    "                # )\n",
    "                # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "                # bisectors_sdf = (sites_sdf[bisectors_to_compute[:, 0]] + sites_sdf[bisectors_to_compute[:, 1]]) / 2\n",
    "                # bisectors_sdf_grad = (\n",
    "                #     sites_sdf_grads[bisectors_to_compute[:, 0]] + sites_sdf_grads[bisectors_to_compute[:, 1]]\n",
    "                # ) / 2\n",
    "                # proj_bisectors = su.newton_step_clipping(bisectors_sdf_grad, bisectors_sdf, bisectors)  # (M,3)\n",
    "\n",
    "            #  v_vect, faces_or_clippedvert = su.cvt_extraction(sites, sites_sdf, d3dsimplices, build_mesh)\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in faces_or_clippedvert for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "                # chamfer_loss_proj_bisect, _ = chamfer_distance(mnfld_points.detach(), proj_bisectors.unsqueeze(0))\n",
    "\n",
    "                # chamfer_loss_mesh = 0.9 * chamfer_loss_mesh + 0.1 * chamfer_loss_proj_bisect\n",
    "\n",
    "            if sites_sdf_grads is None:\n",
    "                sites_sdf_grads, tets_sdf_grads, W = su.sdf_space_grad_pytorch_diego_sites_tets(\n",
    "                    sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "                )\n",
    "\n",
    "            # do cvt loss on the clipped voronoi vertices positions TODO\n",
    "            # cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            # cvt_loss = lf.compute_cvt_loss_true(sites, d3dsimplices, faces_or_clippedvert)\n",
    "\n",
    "            eik_loss = lambda_cvt / 10 * lf.discrete_tet_volume_eikonal_loss(sites, sites_sdf_grads, d3dsimplices)\n",
    "            # shl = lambda_cvt / 0.1 * lf.smoothed_heaviside_loss(sites, sites_sdf, sites_sdf_grads, d3dsimplices)\n",
    "\n",
    "            # eik_loss = lambda_cvt / 1000 * lf.tet_sdf_grad_eikonal_loss(sites, tets_sdf_grads, d3dsimplices)\n",
    "\n",
    "            shl = lambda_cvt / 1 * lf.tet_sdf_motion_mean_curvature_loss(sites, sites_sdf, W, d3dsimplices, eps_H)\n",
    "            # print(\"smoothed_heaviside_loss: \", shl.item())\n",
    "\n",
    "            # sites_eik_loss = lambda_cvt * 0.5 * torch.mean(((sites_sdf_grads**2).sum(dim=1) - 1) ** 2)\n",
    "\n",
    "            sdf_loss = eik_loss + shl  # sites_eik_loss  # +\n",
    "\n",
    "            if not mtet:\n",
    "                cvt_loss = lf.compute_cvt_loss_CLIPPED_vertices(sites, None, None, d3dsimplices, faces_or_clippedvert)\n",
    "            else:\n",
    "                cvt_loss = torch.tensor(0)\n",
    "                eik_loss = torch.tensor(0)\n",
    "                shl = torch.tensor(0)\n",
    "\n",
    "            print(\n",
    "                \"eikonal_loss: \",\n",
    "                eik_loss.item(),\n",
    "                \"cvt_loss: \",\n",
    "                lambda_cvt / 1 * cvt_loss.item(),\n",
    "                \"chamfer_loss_mesh: \",\n",
    "                lambda_chamfer * chamfer_loss_mesh.item(),\n",
    "            )\n",
    "            sites_loss = lambda_cvt / 1 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "\n",
    "        else:\n",
    "            sdf_loss = 0\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            sites_loss = (\n",
    "                lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "            ) + lambda_cvt / 10000 * cvt_loss\n",
    "\n",
    "        loss = sites_loss  # + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        # TODO: test epoch == 300 growthrate 300%\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 3).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            if sites_sdf_grads is None or sites_sdf_grads.shape[0] != sites.shape[0]:\n",
    "                sites_sdf_grads, tets_sdf_grads, W = su.sdf_space_grad_pytorch_diego_sites_tets(\n",
    "                    sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "                )\n",
    "\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites,\n",
    "                simplices=d3dsimplices,\n",
    "                model=sites_sdf,\n",
    "                sites_sdf_grads=sites_sdf_grads,\n",
    "                ups_method=ups_m,\n",
    "                score=\"conservative\",\n",
    "            )\n",
    "            if voroloss_optim:\n",
    "                sites_sdf = Hotspot_model(sites)\n",
    "                sites_sdf = sites_sdf.detach().squeeze(-1).requires_grad_()\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "            eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 5).detach()\n",
    "            print(\"Estimated eps_H: \", eps_H)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000\n",
    "voroloss_optim = False\n",
    "ups = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated eps_H:  tensor(1.1722, device='cuda:0')\n",
      "eikonal_loss:  17.05349349975586 cvt_loss:  4.388182610273361 chamfer_loss_mesh:  1.7202278831973672\n",
      "Epoch 0: loss = 6.108410358428955\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  17.6711483001709 cvt_loss:  4.24342155456543 chamfer_loss_mesh:  1.6681974520906806\n",
      "Epoch 1: loss = 5.911619186401367\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  4096\n",
      "Sampled indices: 158 out of 304 candidates (M=304)\n",
      "Estimated eps_H:  tensor(1.1702, device='cuda:0')\n",
      "sites shape AFTER:  torch.Size([4728, 3])\n",
      "sites sdf shape AFTER:  torch.Size([4728])\n",
      "eikonal_loss:  19.17742156982422 cvt_loss:  3.749532252550125 chamfer_loss_mesh:  1.1780207278206944\n",
      "Epoch 2: loss = 4.927553176879883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m      8\u001b[39m     sites = torch.from_numpy(sites).to(device).requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# import cProfile, pstats\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# prof.export_chrome_trace(\"trace.json\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     sites, optimized_sites_sdf = \u001b[43mtrain_DCCVT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43msdf0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvoroloss_optim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvoroloss_optim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mHotspot_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvoroloss_optim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     sites_np = sites.detach().cpu().numpy()\n\u001b[32m     42\u001b[39m     np.save(site_file_path, sites_np)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mtrain_DCCVT\u001b[39m\u001b[34m(sites, sites_sdf, max_iter, stop_train_threshold, upsampling, lambda_weights, voroloss_optim, Hotspot_model)\u001b[39m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mv_vect shape: \u001b[39m\u001b[33m\"\u001b[39m, v_vect.shape)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     v_vect, faces_or_clippedvert, sites_sdf_grads, tets_sdf_grads, W = \u001b[43msu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_clipped_mesh_numba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md3dsimplices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbarycentric_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_mp\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# _, bisectors_to_compute, _ = su.compute_zero_crossing_vertices_3d(\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m#     sites, None, None, d3dsimplices, sites_sdf\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m#  v_vect, faces_or_clippedvert = su.cvt_extraction(sites, sites_sdf, d3dsimplices, build_mesh)\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m build_mesh:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/sdfpred_utils/sdfpred_utils.py:1880\u001b[39m, in \u001b[36mget_clipped_mesh_numba\u001b[39m\u001b[34m(sites, model, d3dsimplices, clip, sites_sdf, build_mesh, quaternion_slerp, barycentric_weights, no_mp)\u001b[39m\n\u001b[32m   1877\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1878\u001b[39m     \u001b[38;5;66;03m# print(\"-> clipping\")\u001b[39;00m\n\u001b[32m   1879\u001b[39m     vertices_sdf = interpolate_sdf_of_vertices(all_vor_vertices, d3d, sites, sites_sdf)\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m     sites_sdf_grad, tets_sdf_grads, W = \u001b[43msdf_space_grad_pytorch_diego_sites_tets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md3d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1881\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m barycentric_weights:\n\u001b[32m   1882\u001b[39m         \u001b[38;5;66;03m# print(\"-> using barycentric weights for interpolation\")\u001b[39;00m\n\u001b[32m   1883\u001b[39m         \u001b[38;5;66;03m# Use barycentric weights for interpolation\u001b[39;00m\n\u001b[32m   1884\u001b[39m         vertices_sdf_grad, bary_w = interpolate_sdf_grad_of_vertices(\n\u001b[32m   1885\u001b[39m             all_vor_vertices, d3d, sites, sites_sdf_grad, quaternion_slerp=quaternion_slerp\n\u001b[32m   1886\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/sdfpred_utils/sdfpred_utils.py:1696\u001b[39m, in \u001b[36msdf_space_grad_pytorch_diego_sites_tets\u001b[39m\u001b[34m(sites, sdf, tets)\u001b[39m\n\u001b[32m   1693\u001b[39m center = (a + b + c + d) / \u001b[32m4\u001b[39m\n\u001b[32m   1694\u001b[39m sdf_center = (sdf_a + sdf_b + sdf_c + sdf_d) / \u001b[32m4\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1696\u001b[39m volume = \u001b[43mvolume_tetrahedron\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (M,)\u001b[39;00m\n\u001b[32m   1698\u001b[39m X = torch.stack([a, b, c, d], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (M, 4, 3)\u001b[39;00m\n\u001b[32m   1699\u001b[39m dX = X - center[:, \u001b[38;5;28;01mNone\u001b[39;00m, :]  \u001b[38;5;66;03m# (M, 4, 3)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/sdfpred_utils/sdfpred_utils.py:1564\u001b[39m, in \u001b[36mvolume_tetrahedron\u001b[39m\u001b[34m(a, b, c, d)\u001b[39m\n\u001b[32m   1559\u001b[39m     grad_interp = torch.matmul(R, canonical.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)).squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (M, 3)\u001b[39;00m\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_interp\n\u001b[32m-> \u001b[39m\u001b[32m1564\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvolume_tetrahedron\u001b[39m(a, b, c, d):\n\u001b[32m   1565\u001b[39m     ad = a - d\n\u001b[32m   1566\u001b[39m     bd = b - d\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True,  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(\n",
    "    #         sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    #     )\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites,\n",
    "        sdf0,\n",
    "        max_iter=max_iter,\n",
    "        upsampling=ups,\n",
    "        lambda_weights=lambda_weights,\n",
    "        voroloss_optim=voroloss_optim,\n",
    "        Hotspot_model=model if voroloss_optim else None,\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([36056])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/hole_gargoyle1000_1000_3d_sites_4096_chamfer1000.pth\n",
      "sites_np shape:  (36056, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "if voroloss_optim:\n",
    "    sdf_v = model(sites).squeeze(-1)\n",
    "\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0f86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric between sites sdf values and their corresponding sdf values on hotspot model\n",
    "# true_Sdf = model(sites).squeeze(-1)\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 213601\n",
      "Delaunay simplices shape: [[   17   259     3   257]\n",
      " [  512     0   514   257]\n",
      " [18337 30380 20403 32341]\n",
      " ...\n",
      " [34384 34385 35740 22725]\n",
      " [34385 34384 34387 29320]\n",
      " [34385 35740 34387 34384]]\n",
      "Max vertex index in simplices: 36055\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 36056\n",
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 213601\n",
      "Delaunay simplices shape: [[   17   259     3   257]\n",
      " [26874  5921 27290 35834]\n",
      " [    2    16     0    17]\n",
      " ...\n",
      " [26662 33206 33379 26660]\n",
      " [16555 33039 25235 16553]\n",
      " [33037 25235 33039 16553]]\n",
      "Max vertex index in simplices: 36055\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 36056\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "\n",
    "output_obj_file = f\"{destination}{mesh[0]}_unclipped.obj\"\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final unclipped polygon mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "if voroloss_optim:\n",
    "    ps.show()\n",
    "\n",
    "p, faces = su.cvt_extraction(sites, sdf_v, d3dsimplices.detach().cpu().numpy(), True)\n",
    "output_obj_file = f\"{destination}{mesh[0]}_interpol_extract.obj\"\n",
    "su.save_obj(output_obj_file, p.detach().cpu().numpy(), faces)\n",
    "# ps.register_point_cloud(\"cvt extraction\", p.detach().cpu().numpy())\n",
    "ps.register_surface_mesh(\"cvt extraction final\", p.detach().cpu().numpy(), faces, back_face_policy=\"identical\")\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "output_obj_file = f\"{destination}{mesh[0]}_proj_extract.obj\"\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\"\n",
    ")\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf_v.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"MTET\", v_vect.detach().cpu().numpy(), faces.detach().cpu().numpy(), back_face_policy=\"identical\"\n",
    ")\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = f\"{destination}{mesh[0]}_MT.obj\"\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), faces)\n",
    "# su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import voronoiaccel\n",
    "\n",
    "ERROR_SCALE = 1e5  # Scale the error metrics to match the unit of the model (e.g., if model is in cm, scale by 100)\n",
    "obj_pts, obj_normals, obj_mesh = su.sample_points_on_mesh(obj_path, n_points=N_POINTS, GT=False)\n",
    "gt_mesh, gt_normals, gt_mesh = su.sample_points_on_mesh(obj_path, GT=True)\n",
    "cd1, cd2, f1, nc, recall, precision, completeness1, completeness2, accuracy1, accuracy2 = (\n",
    "    voronoiaccel.compute_error_fcpw(\n",
    "        np.array(gt_mesh.vertices),\n",
    "        np.array(gt_mesh.faces).astype(np.int32),\n",
    "        np.array(gt_pts),\n",
    "        np.array(gt_normals),\n",
    "        np.array(obj_mesh.vertices),\n",
    "        np.array(obj_mesh.faces).astype(np.int32),\n",
    "        np.array(obj_pts),\n",
    "        np.array(obj_normals),\n",
    "        0.003,\n",
    "        0.45,\n",
    "    )\n",
    ")\n",
    "cd2 = cd2 * ERROR_SCALE  # Scale the Chamfer distance\n",
    "cd1 = cd1 * ERROR_SCALE  # Scale the Chamfer distance\n",
    "completeness1 = completeness1 * ERROR_SCALE  # Scale the completeness\n",
    "completeness2 = completeness2 * ERROR_SCALE  # Scale the completeness\n",
    "accuracy1 = accuracy1 * ERROR_SCALE  # Scale the accuracy\n",
    "accuracy2 = accuracy2 * ERROR_SCALE  # Scale the accuracy\n",
    "\n",
    "errors[obj_path] = {\n",
    "    \"cd1\": cd1,\n",
    "    \"cd2\": cd2,\n",
    "    \"f1\": f1,\n",
    "    \"nc\": nc,\n",
    "    \"recall\": recall,\n",
    "    \"precision\": precision,\n",
    "    \"completeness1\": completeness1,\n",
    "    \"completeness2\": completeness2,\n",
    "    \"accuracy1\": accuracy1,\n",
    "    \"accuracy2\": accuracy2,\n",
    "}\n",
    "print(\n",
    "    f\"  CD: {cd2:.4f},\\t F1: {f1:.4f},\\t NC: {nc:.4f},\\t Recall: {recall:.4f},\\t Precision: {precision:.4f},\\t Completeness2: {completeness2:.4f},\\t Accuracy2: {accuracy2:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites, sdf = train_DCCVT(\n",
    "#     sites, sdf_v, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=True\n",
    "# )\n",
    "# (\n",
    "#     v_vect,\n",
    "#     f_vect,\n",
    "#     _,\n",
    "#     _,\n",
    "#     _,\n",
    "# ) = su.get_clipped_mesh_numba(sites, None, None, False, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/base.py:605: RuntimeWarning: Mean of empty slice.\n",
      "  centroid = self.triangles_center.mean(axis=0)\n",
      "/home/wylliam/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Translation must be (3,) or (2,)!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m     accuracy = np.mean(dists_ours_to_gt**\u001b[32m2\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, completeness\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m ours_pts, _ = \u001b[43msample_points_on_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_obj_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m m = mesh[\u001b[32m1\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m gt_pts, _ = sample_points_on_mesh(m + \u001b[33m\"\u001b[39m\u001b[33m.obj\u001b[39m\u001b[33m\"\u001b[39m, n_points=\u001b[32m100000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36msample_points_on_mesh\u001b[39m\u001b[34m(mesh_path, n_points)\u001b[39m\n\u001b[32m      8\u001b[39m mesh = trimesh.load(mesh_path)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# normalize mesh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mmesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcentroid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m mesh.apply_scale(\u001b[32m1.0\u001b[39m / np.max(np.abs(mesh.vertices)))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# export mesh to obj file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/parent.py:192\u001b[39m, in \u001b[36mGeometry.apply_translation\u001b[39m\u001b[34m(self, translation)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_transform(tf.planar_matrix(offset=translation))\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m translation.shape != (\u001b[32m3\u001b[39m,):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTranslation must be (3,) or (2,)!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# manually create a translation matrix\u001b[39;00m\n\u001b[32m    195\u001b[39m matrix = np.eye(\u001b[32m4\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Translation must be (3,) or (2,)!"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    print(mesh_path)\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT â†’ Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours â†’ GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours â†’ GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT â†’ Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mesh:  68381.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/68381_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/68381.ply\n",
      "Processing mesh:  79241.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/79241_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/79241.ply\n",
      "Processing mesh:  68380.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/68380_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/68380.ply\n",
      "Processing mesh:  72870.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/72870_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/72870.ply\n",
      "Processing mesh:  64764.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/64764_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/64764.ply\n",
      "Processing mesh:  75662.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75662_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75662.ply\n",
      "Processing mesh:  47984.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/47984_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/47984.ply\n",
      "Processing mesh:  77245.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/77245_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/77245.ply\n",
      "Processing mesh:  75656.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75656_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75656.ply\n",
      "Processing mesh:  76277.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/76277_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/76277.ply\n",
      "Processing mesh:  398259.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/398259_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/398259.ply\n",
      "Processing mesh:  90889.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/90889_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/90889.ply\n",
      "Processing mesh:  75496.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75496_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75496.ply\n",
      "Processing mesh:  44234.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/44234_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/44234.ply\n",
      "Processing mesh:  354371.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/354371_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/354371.ply\n",
      "Processing mesh:  92880.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/92880_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/92880.ply\n",
      "Processing mesh:  316358.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/316358_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/316358.ply\n",
      "Processing mesh:  64444.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/64444_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/64444.ply\n",
      "Processing mesh:  72960.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/72960_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/72960.ply\n",
      "Processing mesh:  313444.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/313444_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/313444.ply\n",
      "Processing mesh:  78671.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/78671_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/78671.ply\n",
      "Processing mesh:  75665.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75665_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75665.ply\n",
      "Processing mesh:  96481.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/96481_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/96481.ply\n",
      "Processing mesh:  441708_150k.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/441708_150k_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/441708_150k.ply\n",
      "Processing mesh:  58168.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/58168_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/58168.ply\n",
      "Processing mesh:  75655.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75655_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/75655.ply\n",
      "Processing mesh:  92763.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/92763_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/92763.ply\n",
      "Processing mesh:  252119.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/252119_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/252119.ply\n",
      "Processing mesh:  527631.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/527631_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/527631.ply\n",
      "Processing mesh:  53159.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/53159_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/53159.ply\n",
      "Processing mesh:  95444.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/95444_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/95444.ply\n",
      "Processing mesh:  441708.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/441708_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/441708.ply\n",
      "Processing mesh:  73075.obj\n",
      "Sampled points shape:  (153600, 3)\n",
      "Sampled normals shape:  (153600, 3)\n",
      "Normalized mesh saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/73075_norm.obj\n",
      "Sampled points saved to:  /home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/73075.ply\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import trimesh\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def sample_points_on_mesh(mesh_path, n_points=32 * 32 * 150, GT=True):\n",
    "#     mesh = trimesh.load(mesh_path)\n",
    "#     points, idx = trimesh.sample.sample_surface(mesh, n_points)\n",
    "#     normals = mesh.face_normals[idx]  # Get normals at sampled points\n",
    "\n",
    "#     if GT:\n",
    "#         # Center the point cloud\n",
    "#         points = np.asarray(points, dtype=np.float32)\n",
    "#         center = points.mean(axis=0)\n",
    "#         points -= center\n",
    "\n",
    "#         # Compute scale factor (same as HotSpot)\n",
    "#         scale = np.percentile(np.linalg.norm(points, axis=-1), 70) / 0.45\n",
    "#         scale = max(scale, np.abs(points).max())\n",
    "#         points /= scale\n",
    "\n",
    "#         # Apply same normalization to mesh vertices\n",
    "#         mesh.vertices = (mesh.vertices - center) / scale\n",
    "\n",
    "#         mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "#         # Save sampled points as a .ply file using trimesh\n",
    "#         pc = trimesh.PointCloud(points)\n",
    "#         pc.export(mesh_path.replace(\".obj\", \".ply\"))\n",
    "\n",
    "#     return points, normals, mesh\n",
    "\n",
    "\n",
    "# thingi32_150k_dir = \"/home/wylliam/dev/Kyushu_experiments/mesh/thingi32_150k/\"\n",
    "# all_mesh = [m for m in os.listdir(thingi32_150k_dir) if m.endswith(\".obj\")]\n",
    "# for mesh in all_mesh:\n",
    "#     print(\"Processing mesh: \", mesh)\n",
    "#     m = thingi32_150k_dir + mesh\n",
    "#     points, normals, mesh = sample_points_on_mesh(m)\n",
    "#     print(\"Sampled points shape: \", points.shape)\n",
    "#     print(\"Sampled normals shape: \", normals.shape)\n",
    "#     print(\"Normalized mesh saved to: \", m.replace(\".obj\", \"_norm.obj\"))\n",
    "#     print(\"Sampled points saved to: \", m.replace(\".obj\", \".ply\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
