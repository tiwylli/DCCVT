{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "#lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "# \n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class Voroloss_opt(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Voroloss_opt, self).__init__()\n",
    "#         self.knn = 16\n",
    "\n",
    "#     def __call__(self, points, spoints):\n",
    "#         \"\"\"points, self.points\"\"\"\n",
    "#         # WARNING: fecthing for knn\n",
    "#         with torch.no_grad():\n",
    "#             indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "#         points_knn = knn_gather(spoints, indices)\n",
    "#         points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "#         voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "#         voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "#         vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "#             -1\n",
    "#         ) / voronoi_edge_l\n",
    "#         sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "#         return sq_dist.min(-1)[0]\n",
    "\n",
    "#voroloss = lf.Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([262144, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.57.08\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 64**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.05\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "\n",
    "#add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([262144, 3])\n",
      "Allocated: 5.79072 MB, Reserved: 23.068672 MB\n",
      "torch.Size([262144])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# # #add mnfld points with random noise to sites \n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 64**3 - num_centroids\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "sdf0 = model(sites)\n",
    "#sdf_grad0 = torch.autograd.grad(sdf0, sites, grad_outputs=torch.ones_like(sdf0), create_graph=True)[0]\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "#sdf_grad0 = sdf_grad0.detach().requires_grad_()\n",
    "#offset = torch.zeros_like(sdf0).to(device)\n",
    "\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n",
    "\n",
    "#print(sdf_grad0.shape)\n",
    "#print(sdf_grad0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c17cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_site_gradients(\n",
    "    sites: torch.Tensor,          # (N_sites, 3)  – xyz positions\n",
    "    tets:  torch.LongTensor,      # (N_tets, 4)   – indices into `sites`\n",
    "    sdf:   torch.Tensor,          # (N_sites,)    – φ-values at each site\n",
    "    eps:   float = 1e-12\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Vectorised re-write of `sdf_space_grad_cuda_kernel`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : (N_sites, 3) torch.Tensor\n",
    "        Volume-weighted ∇φ for every site, averaged over all tets incident\n",
    "        to that site (the CUDA code accumulates   volume * elem   and then\n",
    "        divides by   Σ volume ; we do the same).\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Gather tetra vertex data\n",
    "    # ---------------------------------------------------------------------\n",
    "    v  = sites[tets]                 # (nT,4,3) vertices\n",
    "    phi  = sdf[tets]                   # (nT,4)\n",
    "    ctr = v.mean(dim=1, keepdim=True)  # (nT,1,3)\n",
    "    phi_ctr = phi.mean(dim=1, keepdim=True) # (nT,1)\n",
    "\n",
    "    dX = v - ctr                     # (nT,4,3)   x_i - x̄\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. G = Σ dX_i dX_iᵀ   (nT,3,3)     and its inverse\n",
    "    # ---------------------------------------------------------------------\n",
    "    G   = torch.matmul(dX.transpose(1,2), dX)          # (nT,3,3)\n",
    "    det = torch.det(G)                                 # (nT,)\n",
    "\n",
    "    # Guard against degenerate tets\n",
    "    mask = det.abs() > eps\n",
    "    G_inv = torch.zeros_like(G)\n",
    "    if mask.any():\n",
    "        G_inv[mask] = torch.inverse(G[mask])\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3. Weights_curr  =  G⁻¹ dX_i        (nT,3,4)\n",
    "    # ---------------------------------------------------------------------\n",
    "    W = torch.matmul(G_inv, dX.transpose(1,2))         # (nT,3,4)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4. elem = Σ (φ_i - φ̄) * W_i        (nT,3)\n",
    "    # ---------------------------------------------------------------------\n",
    "    grad_phi   = (phi - phi_ctr)                               # (nT,4)\n",
    "    elem = (W * grad_phi.unsqueeze(1)).sum(dim=2)            # (nT,3)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5. |T|  = volume of each tet       (nT,)\n",
    "    #    |T| = | (a-d)·((b-d)×(c-d)) | / 6\n",
    "    # ---------------------------------------------------------------------\n",
    "    a, b, c, d = v[:,0], v[:,1], v[:,2], v[:,3]\n",
    "    vol = torch.abs(\n",
    "        torch.sum((a-d) * torch.cross(b-d, c-d, dim=1), dim=1)\n",
    "    ) / 6.0                                           # (nT,)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6. Scatter-add to per-site accumulators\n",
    "    # ---------------------------------------------------------------------\n",
    "    n_sites = sites.size(0)\n",
    "    grad_accum   = torch.zeros(n_sites, 3, device=sites.device, dtype=sites.dtype)\n",
    "    weight_accum = torch.zeros(n_sites, 1, device=sites.device, dtype=sites.dtype)\n",
    "\n",
    "    # replicate elem*vol for the 4 vertices of every tet\n",
    "    per_vtx_grad   = (elem * vol.unsqueeze(1)).unsqueeze(1).expand(-1,4,-1)  # (nT,4,3)\n",
    "    per_vtx_weight = vol.unsqueeze(1)                                        # (nT,1)\n",
    "\n",
    "    flat_ids   = tets.reshape(-1)                    # (nT*4,)\n",
    "    flat_grad  = per_vtx_grad.reshape(-1, 3)         # (nT*4,3)\n",
    "    flat_w     = per_vtx_weight.repeat(1,4).reshape(-1,1)  # (nT*4,1)\n",
    "\n",
    "    grad_accum.scatter_add_(0, flat_ids.unsqueeze(-1).expand(-1,3), flat_grad)\n",
    "    weight_accum.scatter_add_(0, flat_ids.unsqueeze(-1),                flat_w)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. Normalise: ∇φ_site = Σ |T|·elem  /  Σ |T|\n",
    "    # ---------------------------------------------------------------------\n",
    "    grad = grad_accum / weight_accum.clamp_min(eps)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites_pred = model(sites).detach()#[\"nonmanifold_pnts_pred\"]\n",
    "#print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sdf0.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\", vminmax=(-0.00005, 0.00005))\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "# ps.register_surface_mesh(\"model clipped initial mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads = su.get_clipped_mesh_numba(sites, None, None, False, sdf0)\n",
    "ps_mesh = ps.register_surface_mesh(\"sdf initial mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "ps_mesh.add_vector_quantity(\"sdf verts grads\", sdf_verts_grads.detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "ps_vect = ps.register_point_cloud(\"sdf initial verts\", v_vect.detach().cpu().numpy())\n",
    "ps_vect.add_scalar_quantity(\"sdf verts values\", sdf_verts.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\")\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads = su.get_clipped_mesh_numba(sites, None, None, True, sdf0)\n",
    "ps_mesh = ps.register_surface_mesh(\"sdf clipped initial mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\")\n",
    "ps_mesh.add_vector_quantity(\"sdf verts grads\", sdf_verts_grads.detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "ps_vect = ps.register_point_cloud(\"sdf clipped initial verts\", v_vect.detach().cpu().numpy())\n",
    "ps_vect.add_scalar_quantity(\"sdf verts values\", sdf_verts.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\")\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, sites_sdf, offset=None, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.SGD([\n",
    "    #{'params': [sites], 'lr': lr_sites},\n",
    "    {'params': [sites_sdf], 'lr': lr_sites * 100},\n",
    "    #{'params': [offset], 'lr': lr_sites},\n",
    "    #{'params': model.parameters(), 'lr': lr_model}\n",
    "])\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        print(\"sdf_values\", sites_sdf[:5])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, sites_sdf)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "    \n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        #print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "\n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        # print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        #sites_sdf_grads = sdf_site_gradients(sites, torch.tensor(d3dsimplices, device=device), sites_sdf)\n",
    "        #sites_positions = sites.detach().clone()\n",
    "        \n",
    "        v_vect, f_vect, sdf_verts, sdf_verts_grads = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sites_sdf, offset)\n",
    "        \n",
    "        ps_mesh = ps.register_surface_mesh(f\"{epoch} sdf clipped pmesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\")\n",
    "        ps_mesh.add_scalar_quantity(f\"{epoch} sdf clipped pmesh\", sdf_verts.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\")\n",
    "        ps_mesh.add_vector_quantity(f\"{epoch} sdf verts grads\", sdf_verts_grads.detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "        triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "        triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "        hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "        chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "        \n",
    "        # chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "        \n",
    "        print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        \n",
    "        sites_loss = (\n",
    "            #lambda_cvt * cvt_loss +\n",
    "            lambda_chamfer * chamfer_loss_mesh \n",
    "            #lambda_chamfer * chamfer_loss_points\n",
    "            #lambda_chamfer * voroloss_loss\n",
    "        )\n",
    "\n",
    "        # voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        # sites_loss = (\n",
    "        #     lambda_chamfer * voroloss_loss\n",
    "        # )\n",
    "\n",
    "        loss = sites_loss #+ sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        \n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        #sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "                \n",
    "        \n",
    "        #scheduler.step()\n",
    "\n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "       \n",
    "        # if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "        #     print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "        #     sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "        #     sites = sites.detach().requires_grad_(True)\n",
    "        #     optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "        #                                   #{'params': model.parameters(), 'lr': lr_model}\n",
    "        #                                   ])\n",
    "        #     upsampled += 1.0\n",
    "        #     print(\"sites length AFTER: \",len(sites))\n",
    "        \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0 or epoch == max_iter:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "            \n",
    "            #ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            ps.register_point_cloud('sampled points end', v_vect.detach().cpu().numpy())\n",
    "            \n",
    "                \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            #model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 8.456445357296616e-05 weighted: 0.0845644548535347 : Allocated: 3159.384064 MB, Reserved: 3856.662528 MB\n",
      "Epoch 0: loss = 0.0845644548535347\n",
      "before loss.backward(): Allocated: 3159.384576 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1601.033728 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.01597399264574051 weighted: 15.973992347717285 : Allocated: 3175.978496 MB, Reserved: 3858.75968 MB\n",
      "Epoch 1: loss = 15.973992347717285\n",
      "before loss.backward(): Allocated: 3175.978496 MB, Reserved: 3858.75968 MB\n",
      "After loss.backward(): Allocated: 1604.954112 MB, Reserved: 3858.75968 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.010650216601788998 weighted: 10.650217056274414 : Allocated: 3179.085824 MB, Reserved: 3860.856832 MB\n",
      "Epoch 2: loss = 10.650217056274414\n",
      "before loss.backward(): Allocated: 3179.085824 MB, Reserved: 3860.856832 MB\n",
      "After loss.backward(): Allocated: 1606.71488 MB, Reserved: 3862.953984 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.023706406354904175 weighted: 23.706405639648438 : Allocated: 3184.306688 MB, Reserved: 3860.856832 MB\n",
      "Epoch 3: loss = 23.706405639648438\n",
      "before loss.backward(): Allocated: 3184.306688 MB, Reserved: 3860.856832 MB\n",
      "After loss.backward(): Allocated: 1609.180672 MB, Reserved: 3860.856832 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.022824574261903763 weighted: 22.824573516845703 : Allocated: 3185.793024 MB, Reserved: 3860.856832 MB\n",
      "Epoch 4: loss = 22.824573516845703\n",
      "before loss.backward(): Allocated: 3185.793024 MB, Reserved: 3860.856832 MB\n",
      "After loss.backward(): Allocated: 1609.954816 MB, Reserved: 3860.856832 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.026060236617922783 weighted: 26.06023597717285 : Allocated: 3189.073408 MB, Reserved: 3856.662528 MB\n",
      "Epoch 5: loss = 26.06023597717285\n",
      "before loss.backward(): Allocated: 3189.073408 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1611.60192 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.029848426580429077 weighted: 29.848426818847656 : Allocated: 3191.586304 MB, Reserved: 3856.662528 MB\n",
      "Epoch 6: loss = 29.848426818847656\n",
      "before loss.backward(): Allocated: 3191.586304 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1613.035008 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.034045059233903885 weighted: 34.04505920410156 : Allocated: 3195.353088 MB, Reserved: 3856.662528 MB\n",
      "Epoch 7: loss = 34.04505920410156\n",
      "before loss.backward(): Allocated: 3195.353088 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1616.218112 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.0399763360619545 weighted: 39.97633743286133 : Allocated: 3197.745664 MB, Reserved: 3856.662528 MB\n",
      "Epoch 8: loss = 39.97633743286133\n",
      "before loss.backward(): Allocated: 3197.745664 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1617.621504 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.04566254839301109 weighted: 45.66254806518555 : Allocated: 3198.226432 MB, Reserved: 3856.662528 MB\n",
      "Epoch 9: loss = 45.66254806518555\n",
      "before loss.backward(): Allocated: 3198.226432 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1617.352192 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "sdf_values tensor([1.2222, 1.2698, 1.2132, 1.1966, 1.2119], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Mesh Chamfer loss PYTORCH3D 0.0487259142100811 weighted: 48.725914001464844 : Allocated: 3200.760832 MB, Reserved: 3856.662528 MB\n",
      "Epoch 10: loss = 48.725914001464844\n",
      "before loss.backward(): Allocated: 3200.760832 MB, Reserved: 3856.662528 MB\n",
      "After loss.backward(): Allocated: 1619.006976 MB, Reserved: 3856.662528 MB\n",
      "-----------------\n",
      "Sites length:  262144\n",
      "min sites:  tensor(-1.1840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1.1755, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=1, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "    # print(\"Offset:\", torch.min(offset), torch.max(offset))\n",
    "    # print(\"Offset mean:\", torch.mean(offset))\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([262144])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/gargoyle10_10_3d_sites_262144_chamfer1000.pth\n",
      "sites_np shape:  (262144, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "\n",
    "#model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "sdf_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "\n",
    "\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\"vis_grid_pred\", sdf_v.detach().cpu().numpy(), enabled=True, cmap=\"coolwarm\", vminmax=(-0.15, 0.15))\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "#ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v)\n",
    "# ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect) \n",
    "\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
