{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "#mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "#trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class Voroloss_opt(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Voroloss_opt, self).__init__()\n",
    "#         self.knn = 16\n",
    "\n",
    "#     def __call__(self, points, spoints):\n",
    "#         \"\"\"points, self.points\"\"\"\n",
    "#         # WARNING: fecthing for knn\n",
    "#         with torch.no_grad():\n",
    "#             indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "#         points_knn = knn_gather(spoints, indices)\n",
    "#         points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "#         voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "#         voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "#         vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "#             -1\n",
    "#         ) / voronoi_edge_l\n",
    "#         sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "#         return sq_dist.min(-1)[0]\n",
    "    \n",
    "voroloss = lf.Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([512, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 570.144\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 8**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "\n",
    "#add noise to meshgrid\n",
    "#meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "Allocated: 3.812352 MB, Reserved: 25.165824 MB\n"
     ]
    }
   ],
   "source": [
    "# #add mnfld points with random noise to sites \n",
    "N = mnfld_points.squeeze(0).shape[0]\n",
    "num_samples = grid**3 - num_centroids\n",
    "idx = torch.randint(0, N, (num_samples,))\n",
    "sampled = mnfld_points.squeeze(0)[idx]\n",
    "perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 12.463104 MB, Reserved: 448.790528 MB\n"
     ]
    }
   ],
   "source": [
    "sites_pred = model(sites).detach()#[\"nonmanifold_pnts_pred\"]\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "#mnfld_preds = model(mnfld_points)#[\"nonmanifold_pnts_pred\"]\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "#mnf_cloud.add_scalar_quantity(\"mnfld_points_pred\", mnfld_preds.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "#initial_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"initial Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=4096)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites},\n",
    "    #{'params': model.parameters(), 'lr': lr_model}\n",
    "])\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        # if epoch > 100:\n",
    "        #     lambda_cvt = 0\n",
    "        #     lambda_chamfer = 0\n",
    "        #     lambda_voroloss = 1000\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "        # print(\"points\", points.shape) \n",
    "    \n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        # \n",
    "#         from pytorch3d.loss import chamfer_distance\n",
    "#         chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "#         print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "# # \n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         s1 = torch.mean(model(points)**2)\n",
    "#         s2 = torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "#         sdf_loss = 0*s1+s2\n",
    "#         sdf_loss.backward(retain_graph=True)\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = True\n",
    "#             # \n",
    "#         #print(\"SDF loss: \", sdf_loss, \"weighted: \", lambda_chamfer/10*sdf_loss)\n",
    "#         # \n",
    "#         #v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, d3dsimplices, batch_size=4096)\n",
    "#         v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, d3dsimplices)\n",
    "#         # \n",
    "#         # \n",
    "#         #ps.register_surface_mesh(\"polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # \n",
    "#         # fanning to transform polygon faces to triangle faces\n",
    "#         triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "#         #ps.register_surface_mesh(\"triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "# # \n",
    "#         triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "#         hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "#         print(\"hs_p shape: \", hs_p.shape)\n",
    "#         #ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "#         # \n",
    "#         from pytorch3d.loss import chamfer_distance\n",
    "#         chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "#         print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "# # \n",
    "\n",
    "        voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        # #voroloss_loss = voroloss(sites.unsqueeze(0), mnfld_points)\n",
    "        \n",
    "        \n",
    "        # # triangle area loss\n",
    "        # # 1) Gather triangle vertices\n",
    "        # v0 = v_vect[triangle_faces[:, 0]]  # (F,3)\n",
    "        # v1 = v_vect[triangle_faces[:, 1]]  # (F,3)\n",
    "        # v2 = v_vect[triangle_faces[:, 2]]  # (F,3)\n",
    "\n",
    "        # # 2) Compute triangle areas for weighting\n",
    "        # e0 = v1 - v0               # (F,3)\n",
    "        # e1 = v2 - v0               # (F,3)\n",
    "        # cross = torch.cross(e0, e1, dim=1)  # (F,3)\n",
    "        # areas = 0.5 * cross.norm(dim=1)     # (F,)\n",
    "        # mean_area = areas.mean()  # (1,)\n",
    "        # triangle_area_loss = torch.mean(areas-mean_area)**2\n",
    "        # print(\"triangle loss: \", triangle_area_loss, \"weighted: \", lambda_chamfer*0.01*triangle_area_loss)\n",
    "\n",
    "\n",
    "        sites_loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            #lambda_chamfer * chamfer_loss_mesh \n",
    "            #+ lambda_chamfer*0.01 * triangle_area_loss\n",
    "            #+ lambda_chamfer * chamfer_loss_points\n",
    "            lambda_chamfer * voroloss_loss\n",
    "            #+ lambda_chamfer/10 * sdf_loss\n",
    "        )\n",
    "            \n",
    "        loss = sites_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            #if upsampled > 0:\n",
    "                #print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "        if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "                                          #{'params': model.parameters(), 'lr': lr_model}\n",
    "                                          ])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "            \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "    \n",
    "    #Export the sites, their sdf values, the gradients of the sdf values and the hessian\n",
    "    sdf_values = model(sites)\n",
    "\n",
    "    sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "\n",
    "    N, D = sites.shape\n",
    "    hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    for i in range(D):\n",
    "        grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "        hess_sdf[:, i, :] = grad2 # fill row i of each 3Ã—3 Hessian\n",
    "    \n",
    "    np.savez(f'{mesh[0]}voroloss_to_clip{model_trained_it}.npz', sites=sites.detach().cpu().numpy(), sdf_values=sdf_values.detach().cpu().numpy(), sdf_gradients=sdf_gradients.detach().cpu().numpy(), sdf_hessians=hess_sdf.detach().cpu().numpy())\n",
    "    print(f\"Saved to {mesh[0]}voroloss_to_clip{model_trained_it}.npz\")\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVT loss:  tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.0159, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 0: loss = 0.023530054837465286\n",
      "before loss.backward(): Allocated: 254.591488 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 176.685568 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.0141, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 1: loss = 0.020853035151958466\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.0163, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 2: loss = 0.021745678037405014\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.4039, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 3: loss = 0.4084813594818115\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.5280, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 4: loss = 0.5319578051567078\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.8388, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 5: loss = 0.8421597480773926\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.0024, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 6: loss = 1.0053378343582153\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.3746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 7: loss = 1.3771635293960571\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.4143, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 8: loss = 1.4164913892745972\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0185, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.8496, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 9: loss = 1.851484775543213\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0241, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.4135, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 10: loss = 2.4151031970977783\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.4675, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 11: loss = 2.4689950942993164\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0274, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7438, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 12: loss = 2.745079755783081\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.9736, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 13: loss = 2.9747703075408936\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0238, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.3796, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 14: loss = 2.3805787563323975\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0317, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1664, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 15: loss = 3.1673097610473633\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7692, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 16: loss = 2.7700679302215576\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0335, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3461, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 17: loss = 3.346839666366577\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5409, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 18: loss = 3.541628837585449\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0944, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 19: loss = 3.0949833393096924\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5412, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 20: loss = 3.5417680740356445\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0329, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2865, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 21: loss = 3.28700852394104\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0304, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0354, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 22: loss = 3.0358774662017822\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.9919, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 23: loss = 2.9924113750457764\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0375, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7521, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 24: loss = 3.752565383911133\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0369, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6937, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 25: loss = 3.6941423416137695\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0366, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6624, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 26: loss = 3.6627449989318848\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6506, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 27: loss = 3.651010036468506\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7951, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 28: loss = 3.7954442501068115\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0341, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4100, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 29: loss = 3.410330295562744\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0371, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7054, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 30: loss = 3.7057080268859863\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5662, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 31: loss = 3.566502571105957\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0329, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 32: loss = 3.293863534927368\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0813, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 33: loss = 4.0815324783325195\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7996, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 34: loss = 2.7998642921447754\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0319, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1935, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 35: loss = 3.1937835216522217\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7195, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 36: loss = 3.719745397567749\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0351, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5093, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 37: loss = 3.5095043182373047\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0347, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4724, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 38: loss = 3.4725959300994873\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0363, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6294, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 39: loss = 3.6295835971832275\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.9537, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 40: loss = 3.9538815021514893\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.1002, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 41: loss = 4.10042142868042\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 42: loss = 3.0254976749420166\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0337, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3707, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 43: loss = 3.370926856994629\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2606, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 44: loss = 4.260812759399414\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0367, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 45: loss = 3.674760341644287\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0378, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7825, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 46: loss = 3.7827227115631104\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0320, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2002, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 47: loss = 3.2003543376922607\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2681, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 48: loss = 3.268244981765747\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0338, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3759, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 49: loss = 3.3760836124420166\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0329, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2876, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 50: loss = 3.287797212600708\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0345, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4502, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 51: loss = 3.4503438472747803\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0458, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.5841, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 52: loss = 4.584225654602051\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0322, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2161, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 53: loss = 3.216299295425415\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0287, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.8691, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 54: loss = 2.8692240715026855\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0700, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 55: loss = 4.070129871368408\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3925, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 56: loss = 3.3926358222961426\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0335, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3490, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 57: loss = 3.3491013050079346\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7156, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 58: loss = 3.7157437801361084\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.1679, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 59: loss = 4.167999267578125\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0374, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7406, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 60: loss = 3.7407760620117188\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0497, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 61: loss = 3.049814224243164\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0295, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.9475, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 62: loss = 2.947615146636963\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0342, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4222, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 63: loss = 3.4223756790161133\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0362, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6221, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 64: loss = 3.6222782135009766\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1814, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 65: loss = 3.181549310684204\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0377, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7707, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 66: loss = 3.7708489894866943\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0368, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6767, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 67: loss = 3.6768503189086914\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2806, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 68: loss = 4.280742645263672\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0272, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 69: loss = 3.0272977352142334\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0324, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2450, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 70: loss = 3.245119333267212\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0040, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 71: loss = 4.0041351318359375\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0853, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 72: loss = 3.0854170322418213\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0429, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2900, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 73: loss = 4.290164470672607\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5389, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 74: loss = 3.539050340652466\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0069, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 75: loss = 4.0070624351501465\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0383, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.8336, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 76: loss = 3.833681106567383\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0438, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.3802, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 77: loss = 4.380334377288818\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0347, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4730, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 78: loss = 3.4731106758117676\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3871, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 79: loss = 3.387237310409546\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0405, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0452, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 80: loss = 4.045302391052246\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0343, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4309, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 81: loss = 3.430972099304199\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0362, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6205, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 82: loss = 3.6206555366516113\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0573, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 83: loss = 4.057447910308838\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0451, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.5066, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 84: loss = 4.5066752433776855\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0938, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 85: loss = 3.0939459800720215\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0338, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3794, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 86: loss = 3.3794867992401123\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0425, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2470, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 87: loss = 4.247054576873779\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0375, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7467, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 88: loss = 3.746831178665161\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0278, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7819, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 89: loss = 2.7820069789886475\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.9251, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 90: loss = 3.92517352104187\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.8451, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 91: loss = 3.8451640605926514\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7243, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 92: loss = 3.7244040966033936\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0335, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3543, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 93: loss = 3.35434889793396\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1164, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 94: loss = 3.1165144443511963\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3923, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 95: loss = 3.3924059867858887\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.5716, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 96: loss = 3.5716474056243896\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.8594, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 97: loss = 3.859520196914673\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.9699, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 98: loss = 3.9700186252593994\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0416, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.1550, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 99: loss = 4.15510892868042\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "CVT loss:  tensor(0.0342, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.4245, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 100: loss = 3.4245522022247314\n",
      "before loss.backward(): Allocated: 257.22112 MB, Reserved: 3170.893824 MB\n",
      "After loss.backward(): Allocated: 177.472 MB, Reserved: 3170.893824 MB\n",
      "-----------------\n",
      "Saved to gargoylevoroloss_to_clip.npz\n",
      "Sites length:  32768\n",
      "min sites:  tensor(-1.2053, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1.2407, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=1, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/End2End_DCCVT/gargoyle100_100_3d_model_512_chamfer1000.pth\n",
      "sites ./images/autograd/End2End_DCCVT/gargoyle100_100_3d_sites_512_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"Zero-Crossing faces direct\", final_mesh[0], final_mesh[1])\n",
    "\n",
    "#save to file\n",
    "final_mesh_file = f'{mesh[0]}voroloss_sdf_trained{model_trained_it}.npz'\n",
    "faces = np.array(final_mesh[1], dtype=object)\n",
    "np.savez(final_mesh_file, vertices=final_mesh[0], faces=faces)\n",
    "\n",
    "data = np.load(final_mesh_file, allow_pickle=True)\n",
    "verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "# print(\"Zero-Crossing faces final shape: \", verts.shape)\n",
    "# ps.register_surface_mesh(\"Zero-Crossing faces final\", verts, faces, back_face_policy=\"identical\")\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=3072)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final clipped triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "# triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "# s_p = su.sample_mesh_points(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"sampled clipped mesh\", s_p.detach().cpu().numpy())\n",
    "\n",
    "# hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "\n",
    "# ##register original mesh\n",
    "# mesh_file = mesh[1]+\".stl\"\n",
    "# #load mesh \n",
    "# m = trimesh.load(mesh_file)\n",
    "# #convert to numpy\n",
    "# mesh_np = np.array(m.vertices)\n",
    "# #normalize mesh\n",
    "# mesh_np = mesh_np - np.mean(mesh_np, axis=0)\n",
    "# mesh_np = mesh_np / np.max(np.abs(mesh_np))\n",
    "# mesh_faces = np.array(m.faces)\n",
    "# ps.register_surface_mesh(\"Original Mesh\", mesh_np, mesh_faces)\n",
    "\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "tri = Delaunay(sites_np)\n",
    "delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "sdf_values = model(sites)\n",
    "\n",
    "# Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "vertices = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "ps.register_surface_mesh(\"Marching Tetrahedra Mesh final\", vertices_np, faces_np)\n",
    "\n",
    "# clipped_cvt = \"clipped_CVT.obj\"\n",
    "# if os.path.exists(clipped_cvt):\n",
    "#     clipped_cvt_mesh = trimesh.load(clipped_cvt)\n",
    "#     ps.register_surface_mesh(\"Clipped CVT\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
