{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "\n",
    "# import diffvoronoi\n",
    "import pygdel3d\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "# Improve reproducibility\n",
    "torch.manual_seed(69)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(69)\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "ROOT_DIR = \"/home/wylliam/dev/Kyushu_experiments\"\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "mesh = [\n",
    "    \"gargoyle\",\n",
    "    f\"{ROOT_DIR}/mesh/thingi32/64764\",\n",
    "]\n",
    "trained_model_path = f\"{ROOT_DIR}/hotspots_model/thingi32/64764.pth\"\n",
    "\n",
    "# mesh = [\"zombie\", f\"{ROOT_DIR}/mesh/thingi32/398259\"]\n",
    "# trained_model_path = f\"{ROOT_DIR}/hotspots_model/thingi32/398259.pth\"\n",
    "\n",
    "# mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_500.pth\"\n",
    "\n",
    "\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "# #\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beltegeuse/projects/Voronoi/Kyushu_experiments/.venv/lib/python3.13/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 9600, 3])\n",
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 430.440448 MB, Reserved: 444.596224 MB\n",
      "torch.Size([32768])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "\n",
    "import sys\n",
    "\n",
    "if mesh[0] != \"sphere\":\n",
    "    sys.path.append(\"3rdparty/HotSpot\")\n",
    "    from dataset import shape_3d\n",
    "    import models.Net as Net\n",
    "\n",
    "    loss_type = \"igr_w_heat\"\n",
    "    loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "    train_set = shape_3d.ReconDataset(\n",
    "        file_path=mesh[1] + \".ply\",\n",
    "        n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "        n_samples=10001,  # args.n_iterations,\n",
    "        grid_res=256,  # args.grid_res,\n",
    "        grid_range=1.1,  # args.grid_range,\n",
    "        sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "        sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "        n_random_samples=7500,  # args.n_random_samples,\n",
    "        resample=True,\n",
    "        compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "        scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "    )\n",
    "\n",
    "    model = Net.Network(\n",
    "        latent_size=0,  # args.latent_size,\n",
    "        in_dim=3,\n",
    "        decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "        nl=\"sine\",  # args.nl,\n",
    "        encoder_type=\"none\",  # args.encoder_type,\n",
    "        decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "        neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "        init_type=\"mfgi\",  # args.init_type,\n",
    "        sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "        n_repeat_period=30,  # args.n_repeat_period,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ######\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_data = next(iter(test_dataloader))\n",
    "    mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "    # add noise to mnfld_points\n",
    "    # mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "    mnfld_points.requires_grad_()\n",
    "    print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "    if torch.cuda.is_available():\n",
    "        map_location = torch.device(\"cuda\")\n",
    "    else:\n",
    "        map_location = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "    sdf0 = model(sites)\n",
    "\n",
    "else:\n",
    "\n",
    "    def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3) tensor of 3D query points\n",
    "            center: (3,) tensor specifying the center of the sphere\n",
    "            radius: float, radius of the sphere\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,) tensor of signed distances\n",
    "        \"\"\"\n",
    "        return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "    def sphere_sdf_with_noise(\n",
    "        points: torch.Tensor, center: torch.Tensor, radius: float, noise_amplitude=0.05\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sphere SDF with smooth directional noise added near the surface.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3)\n",
    "            center: (3,)\n",
    "            radius: float\n",
    "            noise_amplitude: float\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,)\n",
    "        \"\"\"\n",
    "        rel = points - center\n",
    "        norm = torch.norm(rel, dim=-1)  # (N,)\n",
    "        base_sdf = norm - radius  # (N,)\n",
    "\n",
    "        # Smooth periodic noise based on direction\n",
    "        unit_dir = rel / (norm.unsqueeze(-1) + 1e-9)  # (N,3)\n",
    "        noise = torch.sin(10 * unit_dir[:, 0]) * torch.sin(10 * unit_dir[:, 1]) * torch.sin(10 * unit_dir[:, 2])\n",
    "\n",
    "        # Weight noise so it mostly affects surface area\n",
    "        falloff = torch.exp(-20 * (base_sdf**2))  # (N,) ~1 near surface, ~0 far\n",
    "        sdf = base_sdf + noise_amplitude * noise * falloff\n",
    "\n",
    "        return sdf\n",
    "\n",
    "    # generate points on the sphere\n",
    "    mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "    mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "    mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()\n",
    "    sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "    # sdf0 = sphere_sdf_with_noise(sites, torch.zeros(3).to(device), 0.50, noise_amplitude=0.1)\n",
    "\n",
    "# # add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 18**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled) - 0.5) * noise_scale * 10\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "# sdf0 = model(sites)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba12786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "# print(\"Delaunay simplices shape: \", d3dsimplices.shape)\n",
    "\n",
    "# print(\"sites shape: \", sites.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polyscope.surface_mesh.SurfaceMesh at 0x7f4f1dac8e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, faces = su.cvt_extraction(sites, sdf0, d3dsimplices, True)\n",
    "ps.register_point_cloud(\"cvt extraction\", p.detach().cpu().numpy(), enabled=False)\n",
    "ps.register_surface_mesh(\"cvt extraction\", p.detach().cpu().numpy(), faces, back_face_policy=\"identical\", enabled=False)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "# ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, True, barycentric_weights=True, quaternion_slerp=True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh interpol\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, True, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf0.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"init MTET\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    faces.detach().cpu().numpy(),\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "# ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "    voroloss_optim=False,\n",
    "):\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ],\n",
    "            betas=(0.8, 0.95),\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    lambda_shl = lambda_cvt / 10\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "            if epoch % 100 == 0 and epoch <= 500:\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 5).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "            elif epoch % 100 == 0 and epoch <= 800:\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 2).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "\n",
    "            # cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)  # torch.tensor(0)  #\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "            mtet = False\n",
    "            noised_sdf = False\n",
    "            sites_sdf_grads = None\n",
    "            \n",
    "            # Create small noise in sites_sdf that fade during the training process\n",
    "            if not noised_sdf:\n",
    "                # Create small noise in sites_sdf that fade during the training process\n",
    "                # This is to avoid numerical issues with the SDF being too close to zero\n",
    "                if epoch < max_iter * 0.5:\n",
    "                    noise_amplitude = 0.02 * (1 - epoch / (max_iter * 0.5))\n",
    "                    sites_noised_sdf = torch.randn_like(sites_sdf) * noise_amplitude + sites_sdf\n",
    "                else:\n",
    "                    sites_noised_sdf = sites_sdf\n",
    "            else:\n",
    "                sites_noised_sdf = sites_sdf\n",
    "\n",
    "            if mtet:\n",
    "                print(\"Using MTET\")\n",
    "                d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "                marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "                    sites.unsqueeze(0), d3dsimplices, sites_noised_sdf.unsqueeze(0), return_tet_idx=False\n",
    "                )\n",
    "                vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "                v_vect = vertices_list[0]\n",
    "                faces = faces_list[0]\n",
    "                print(\"v_vect shape: \", v_vect.shape)\n",
    "\n",
    "            else:\n",
    "                v_vect, faces_or_clippedvert, sites_sdf_grads, tets_sdf_grads, W = su.get_clipped_mesh_numba(\n",
    "                    sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "                )\n",
    "            #  v_vect, faces_or_clippedvert = su.cvt_extraction(sites, sites_sdf, d3dsimplices, build_mesh)\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in faces_or_clippedvert for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "            if sites_sdf_grads is None:\n",
    "                sites_sdf_grads, tets_sdf_grads, W = su.sdf_space_grad_pytorch_diego_sites_tets(\n",
    "                    sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "                )\n",
    "\n",
    "            # do cvt loss on the clipped voronoi vertices positions TODO\n",
    "            cvt_loss = lf.compute_cvt_loss_CLIPPED_vertices(sites, None, None, d3dsimplices, faces_or_clippedvert)\n",
    "\n",
    "            print(\n",
    "                \"cvt_loss: \",\n",
    "                lambda_cvt / 1 * cvt_loss.item(),\n",
    "                \"chamfer_loss_mesh: \",\n",
    "                lambda_chamfer * chamfer_loss_mesh.item(),\n",
    "            )\n",
    "            sites_loss = lambda_cvt / 1 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "\n",
    "            eik_loss = lambda_cvt / 10 * lf.discrete_tet_volume_eikonal_loss(sites, sites_sdf_grads, d3dsimplices)\n",
    "            # shl = lambda_cvt / 0.1 * lf.smoothed_heaviside_loss(sites, sites_sdf, sites_sdf_grads, d3dsimplices)\n",
    "\n",
    "            # eik_loss = lambda_cvt / 1000 * lf.tet_sdf_grad_eikonal_loss(sites, tets_sdf_grads, d3dsimplices)\n",
    "            print(\"eikonal_loss: \", eik_loss.item())\n",
    "\n",
    "            shl = lambda_cvt / 1 * lf.tet_sdf_motion_mean_curvature_loss(sites, sites_sdf, W, d3dsimplices, eps_H)\n",
    "            # print(\"smoothed_heaviside_loss: \", shl.item())\n",
    "\n",
    "            # sites_eik_loss = lambda_cvt * 0.5 * torch.mean(((sites_sdf_grads**2).sum(dim=1) - 1) ** 2)\n",
    "\n",
    "            sdf_loss = eik_loss + shl  # sites_eik_loss  # +\n",
    "        else:\n",
    "            sdf_loss = 0\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            sites_loss = (\n",
    "                lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "            ) + lambda_cvt / 10000 * cvt_loss\n",
    "\n",
    "        loss = sites_loss + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        # TODO: test epoch == 300 growthrate 300%\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 3).detach()\n",
    "                print(\"Estimated eps_H: \", eps_H)\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites,\n",
    "                simplices=d3dsimplices,\n",
    "                model=sites_sdf,\n",
    "                sites_sdf_grads=sites_sdf_grads,\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "            eps_H = lf.estimate_eps_H(sites, d3dsimplices, multiplier=1.5 * 5).detach()\n",
    "            print(\"Estimated eps_H: \", eps_H)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447548a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda weights:\n",
      "lambda_cvt: 100\n",
      "lambda_sdf: 0\n",
      "lambda_min_distance: 0\n",
      "lambda_laplace: 0\n",
      "lambda_chamfer: 1000\n",
      "lambda_eikonal: 0\n",
      "lambda_domain_restriction: 100\n",
      "lambda_true_points: 0\n"
     ]
    }
   ],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "# Print the lambda weights\n",
    "print(\"Lambda weights:\")\n",
    "print(f\"lambda_cvt: {lambda_cvt}\")\n",
    "print(f\"lambda_sdf: {lambda_sdf}\")\n",
    "print(f\"lambda_min_distance: {lambda_min_distance}\") \n",
    "print(f\"lambda_laplace: {lambda_laplace}\")\n",
    "print(f\"lambda_chamfer: {lambda_chamfer}\")\n",
    "print(f\"lambda_eikonal: {lambda_eikonal}\")\n",
    "print(f\"lambda_domain_restriction: {lambda_domain_restriction}\")\n",
    "print(f\"lambda_true_points: {lambda_true_points}\")\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.10944399982690811\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 1: loss = 0.10466130077838898\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 2: loss = 0.10011604428291321\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 3: loss = 0.095824234187603\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 4: loss = 0.09174980968236923\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 5: loss = 0.0878855511546135\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 6: loss = 0.08422825485467911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 7: loss = 0.08075923472642899\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 8: loss = 0.07746898382902145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 9: loss = 0.07432038336992264\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 10: loss = 0.07131803035736084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 11: loss = 0.06846009939908981\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 12: loss = 0.06571882963180542\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 13: loss = 0.06307537853717804\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 14: loss = 0.06052614375948906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 15: loss = 0.05807426944375038\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 16: loss = 0.05572671815752983\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 17: loss = 0.053474266082048416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 18: loss = 0.051316749304533005\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 19: loss = 0.04924863949418068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 20: loss = 0.04726666584610939\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 21: loss = 0.045370906591415405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 22: loss = 0.0435485877096653\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 23: loss = 0.04179757833480835\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 24: loss = 0.04010627791285515\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 25: loss = 0.038489930331707\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 26: loss = 0.03693753480911255\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 27: loss = 0.03544916957616806\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 28: loss = 0.03403127193450928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 29: loss = 0.03268001228570938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 30: loss = 0.03139292448759079\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 31: loss = 0.030162932351231575\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 32: loss = 0.02897803485393524\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 33: loss = 0.027850383892655373\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 34: loss = 0.026782292872667313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 35: loss = 0.025775091722607613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 36: loss = 0.02480316534638405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 37: loss = 0.023882893845438957\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 38: loss = 0.02301778271794319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 39: loss = 0.022201701998710632\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 40: loss = 0.021434588357806206\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 41: loss = 0.020717227831482887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 42: loss = 0.020039765164256096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 43: loss = 0.019393807277083397\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 44: loss = 0.018777307122945786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 45: loss = 0.018193399533629417\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 46: loss = 0.01764029823243618\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 47: loss = 0.01710795983672142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 48: loss = 0.016592424362897873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 49: loss = 0.016092119738459587\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 50: loss = 0.015607491135597229\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 51: loss = 0.015137319453060627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 52: loss = 0.014681629836559296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 53: loss = 0.014247789047658443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 54: loss = 0.013839652761816978\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 55: loss = 0.013455546461045742\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 56: loss = 0.013098260387778282\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 57: loss = 0.01275884360074997\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 58: loss = 0.012434819713234901\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 59: loss = 0.01212805975228548\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 60: loss = 0.011830688454210758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 61: loss = 0.011541027575731277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 62: loss = 0.011257958598434925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 63: loss = 0.010984757915139198\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 64: loss = 0.010720900259912014\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 65: loss = 0.010466314852237701\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 66: loss = 0.010220946744084358\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 67: loss = 0.00998194981366396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 68: loss = 0.009754949249327183\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 69: loss = 0.0095403166487813\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 70: loss = 0.009337847121059895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 71: loss = 0.009143713861703873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 72: loss = 0.008955804631114006\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 73: loss = 0.008777894079685211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 74: loss = 0.00860572513192892\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 75: loss = 0.00843526516109705\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 76: loss = 0.008266961202025414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 77: loss = 0.00810309313237667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 78: loss = 0.007943176664412022\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 79: loss = 0.007788022980093956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 80: loss = 0.007639781106263399\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 81: loss = 0.007496546022593975\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 82: loss = 0.007357945665717125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 83: loss = 0.0072255670092999935\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 84: loss = 0.007095064036548138\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 85: loss = 0.006967184599488974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 86: loss = 0.00684428121894598\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 87: loss = 0.006728308275341988\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 88: loss = 0.00661954190582037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 89: loss = 0.006516742054373026\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 90: loss = 0.006414915435016155\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 91: loss = 0.006314649246633053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 92: loss = 0.006209587678313255\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 93: loss = 0.006105896085500717\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 94: loss = 0.006008967757225037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 95: loss = 0.005918420385569334\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 96: loss = 0.005837549455463886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 97: loss = 0.005759626626968384\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 98: loss = 0.00568361533805728\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 99: loss = 0.0056096212938427925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 100: loss = 0.005537741351872683\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 101: loss = 0.005468526855111122\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 102: loss = 0.0054007358849048615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 103: loss = 0.005337326787412167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 104: loss = 0.005274731665849686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 105: loss = 0.0052094412967562675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 106: loss = 0.005145172588527203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 107: loss = 0.005085309501737356\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 108: loss = 0.00502830371260643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 109: loss = 0.004971055779606104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 110: loss = 0.004913331475108862\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 111: loss = 0.004857389722019434\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 112: loss = 0.004805731121450663\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 113: loss = 0.004755307454615831\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 114: loss = 0.00470653735101223\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 115: loss = 0.004658323246985674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 116: loss = 0.0046107154339551926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 117: loss = 0.004563767928630114\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 118: loss = 0.004517210181802511\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 119: loss = 0.004474933724850416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 120: loss = 0.004435203969478607\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 121: loss = 0.004396992735564709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 122: loss = 0.004360454622656107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 123: loss = 0.004324546083807945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 124: loss = 0.004287188407033682\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 125: loss = 0.004251270089298487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 126: loss = 0.004216679371893406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 127: loss = 0.0041819228790700436\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 128: loss = 0.004150934983044863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 129: loss = 0.004120571073144674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 130: loss = 0.004091260489076376\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 131: loss = 0.004063948057591915\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 132: loss = 0.004035396035760641\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 133: loss = 0.004008639138191938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 134: loss = 0.003983754198998213\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 135: loss = 0.003958018030971289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 136: loss = 0.0039337328635156155\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 137: loss = 0.0039099217392504215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 138: loss = 0.0038853527512401342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 139: loss = 0.0038618154358118773\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 140: loss = 0.003837374970316887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 141: loss = 0.0038130239117890596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 142: loss = 0.0037860607262700796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 143: loss = 0.0037606379482895136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 144: loss = 0.0037355120293796062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 145: loss = 0.0037242204416543245\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 146: loss = 0.0036893184296786785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 147: loss = 0.003667298937216401\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 148: loss = 0.003650560276582837\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 149: loss = 0.00363112217746675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 150: loss = 0.0036081939470022917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 151: loss = 0.003587757470086217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 152: loss = 0.0035724742338061333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 153: loss = 0.00355654489248991\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 154: loss = 0.0035373696591705084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 155: loss = 0.0035183338914066553\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 156: loss = 0.0034987321123480797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 157: loss = 0.0034785636235028505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 158: loss = 0.0034617665223777294\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 159: loss = 0.0034452886320650578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 160: loss = 0.0034269574098289013\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 161: loss = 0.0034077477175742388\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 162: loss = 0.003387811593711376\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 163: loss = 0.0033681399654597044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 164: loss = 0.0033499302808195353\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 165: loss = 0.003331329207867384\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 166: loss = 0.003312168875709176\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 167: loss = 0.0032933405600488186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 168: loss = 0.003274507587775588\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 169: loss = 0.003257317002862692\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 170: loss = 0.0032404896337538958\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 171: loss = 0.0032256755512207747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 172: loss = 0.003209897084161639\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 173: loss = 0.0031954259611666203\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 174: loss = 0.0031808470375835896\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 175: loss = 0.0031670157331973314\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 176: loss = 0.0031535427551716566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 177: loss = 0.003140517510473728\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 178: loss = 0.003127145813778043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 179: loss = 0.0031134546734392643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 180: loss = 0.003100333269685507\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 181: loss = 0.00308788544498384\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 182: loss = 0.003075536573305726\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 183: loss = 0.0030625867657363415\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 184: loss = 0.003051014617085457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 185: loss = 0.0030500891152769327\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 186: loss = 0.0030323180835694075\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 187: loss = 0.0030270125716924667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 188: loss = 0.00300419214181602\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 189: loss = 0.0029987129382789135\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 190: loss = 0.0029879016801714897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 191: loss = 0.0029681087471544743\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 192: loss = 0.0029532788321375847\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 193: loss = 0.0029452452436089516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 194: loss = 0.0029372721910476685\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 195: loss = 0.002925196895375848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 196: loss = 0.00291297840885818\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 197: loss = 0.002901463769376278\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 198: loss = 0.002892740536481142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 199: loss = 0.002883000299334526\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 200: loss = 0.0028720865957438946\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 201: loss = 0.0028610082808882\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 202: loss = 0.0028512938879430294\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 203: loss = 0.0028416048735380173\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 204: loss = 0.002831813180819154\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 205: loss = 0.002821436384692788\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 206: loss = 0.002811276586726308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 207: loss = 0.002801855793222785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 208: loss = 0.002792102750390768\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 209: loss = 0.0027824307326227427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 210: loss = 0.0027733934111893177\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 211: loss = 0.0027655225712805986\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 212: loss = 0.0027621847111731768\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 213: loss = 0.0027543322648853064\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 214: loss = 0.0027393961790949106\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 215: loss = 0.002733946545049548\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 216: loss = 0.0027234379667788744\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 217: loss = 0.0027148567605763674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 218: loss = 0.00271458993665874\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 219: loss = 0.002697885036468506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 220: loss = 0.002693465445190668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 221: loss = 0.002680659992620349\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 222: loss = 0.0026756844017654657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 223: loss = 0.0026642000302672386\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 224: loss = 0.0026566602755337954\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 225: loss = 0.002650604350492358\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 226: loss = 0.0026431982405483723\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 227: loss = 0.0026345960795879364\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 228: loss = 0.0026267513167113066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 229: loss = 0.0026201631408184767\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 230: loss = 0.0026132739149034023\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 231: loss = 0.0026053127367049456\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 232: loss = 0.002599174389615655\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 233: loss = 0.0025920304469764233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 234: loss = 0.002585954964160919\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 235: loss = 0.0025793558452278376\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 236: loss = 0.0025722961872816086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 237: loss = 0.002566081704571843\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 238: loss = 0.0025600914377719164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 239: loss = 0.002554126549512148\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 240: loss = 0.0025487691164016724\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 241: loss = 0.0025431711692363024\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 242: loss = 0.0025375105906277895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 243: loss = 0.002531935926526785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 244: loss = 0.0025266995653510094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 245: loss = 0.0025211756583303213\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 246: loss = 0.002516065025702119\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 247: loss = 0.00251028616912663\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 248: loss = 0.0025050071999430656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 249: loss = 0.0024996844585984945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 250: loss = 0.002494635060429573\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 251: loss = 0.0024898035917431116\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 252: loss = 0.0024841863196343184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 253: loss = 0.0024783730041235685\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 254: loss = 0.0024725287221372128\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 255: loss = 0.0024669982958585024\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 256: loss = 0.0024611952248960733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 257: loss = 0.0024557458236813545\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 258: loss = 0.0024504773318767548\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 259: loss = 0.0024469224736094475\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 260: loss = 0.0024396274238824844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 261: loss = 0.0024352911859750748\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 262: loss = 0.002431354485452175\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 263: loss = 0.0024254873860627413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 264: loss = 0.0024231569841504097\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 265: loss = 0.002418062649667263\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 266: loss = 0.0024138507433235645\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 267: loss = 0.0024079422000795603\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 268: loss = 0.002404351718723774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 269: loss = 0.002400216180831194\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 270: loss = 0.002397517440840602\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 271: loss = 0.0023916217032819986\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 272: loss = 0.002390138804912567\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 273: loss = 0.002383196260780096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 274: loss = 0.0023804993834346533\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 275: loss = 0.0023755780421197414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 276: loss = 0.0023717039730399847\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 277: loss = 0.0023677260614931583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 278: loss = 0.0023619807325303555\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 279: loss = 0.0023577103856951\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 280: loss = 0.0023511494509875774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 281: loss = 0.0023469338193535805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 282: loss = 0.002340729581192136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 283: loss = 0.0023375293239951134\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 284: loss = 0.0023407666012644768\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 285: loss = 0.002325468696653843\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 286: loss = 0.002324520144611597\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 287: loss = 0.0023198481649160385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 288: loss = 0.0023127268068492413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 289: loss = 0.0023076410870999098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 290: loss = 0.0023041279055178165\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 291: loss = 0.0023010848090052605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 292: loss = 0.002296960446983576\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 293: loss = 0.0022927774116396904\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 294: loss = 0.0022878109011799097\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 295: loss = 0.0022840439341962337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 296: loss = 0.0022812653332948685\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 297: loss = 0.0022782846353948116\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 298: loss = 0.002274466445669532\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 299: loss = 0.002271009376272559\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 300: loss = 0.0022686750162392855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 301: loss = 0.0022623250260949135\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 302: loss = 0.002262681722640991\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 303: loss = 0.002257481450214982\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 304: loss = 0.0022563517559319735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 305: loss = 0.002250057877972722\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 306: loss = 0.002251471159979701\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 307: loss = 0.002243070164695382\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 308: loss = 0.0022412422113120556\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 309: loss = 0.0022372615057975054\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 310: loss = 0.0022318160627037287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 311: loss = 0.0022304777521640062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 312: loss = 0.0022254721261560917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 313: loss = 0.002222450217232108\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 314: loss = 0.0022201393730938435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 315: loss = 0.0022158378269523382\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 316: loss = 0.002212675753980875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 317: loss = 0.002210842212662101\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 318: loss = 0.00220697489567101\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 319: loss = 0.002203666139394045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 320: loss = 0.002201577415689826\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 321: loss = 0.002198539674282074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 322: loss = 0.0021953717805445194\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 323: loss = 0.0021928290370851755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 324: loss = 0.0021903016604483128\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 325: loss = 0.0021870869677513838\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 326: loss = 0.0021846836898475885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 327: loss = 0.0021826494485139847\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 328: loss = 0.0021814804058521986\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 329: loss = 0.002179990289732814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 330: loss = 0.002172508044168353\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 331: loss = 0.002171309432014823\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 332: loss = 0.0021675850730389357\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 333: loss = 0.0021641813218593597\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 334: loss = 0.0021640353370457888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 335: loss = 0.0021578739397227764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 336: loss = 0.002155497670173645\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 337: loss = 0.0021536042913794518\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 338: loss = 0.002147347666323185\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 339: loss = 0.0021454831585288048\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 340: loss = 0.002142246114090085\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 341: loss = 0.002142475452274084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 342: loss = 0.0021387608721852303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 343: loss = 0.0021341515239328146\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 344: loss = 0.002132343826815486\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 345: loss = 0.002129428321495652\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 346: loss = 0.0021270615980029106\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 347: loss = 0.0021248124539852142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 348: loss = 0.002121714176610112\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 349: loss = 0.0021203821524977684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 350: loss = 0.0021188396494835615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 351: loss = 0.002116032410413027\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 352: loss = 0.002115475246682763\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 353: loss = 0.002112378366291523\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 354: loss = 0.0021109769586473703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 355: loss = 0.0021097150165587664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 356: loss = 0.0021070886868983507\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 357: loss = 0.0021053801756352186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 358: loss = 0.0021035235840827227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 359: loss = 0.002101535676047206\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 360: loss = 0.0021002283319830894\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 361: loss = 0.002098564524203539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 362: loss = 0.002096794545650482\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 363: loss = 0.0020957745146006346\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 364: loss = 0.002094280906021595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 365: loss = 0.0020922264084219933\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 366: loss = 0.00209248554892838\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 367: loss = 0.0020902645774185658\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 368: loss = 0.0020877292845398188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 369: loss = 0.002086588181555271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 370: loss = 0.0020851779263466597\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 371: loss = 0.0020848875865340233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 372: loss = 0.0020853914320468903\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 373: loss = 0.002091827802360058\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 374: loss = 0.0020840694196522236\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 375: loss = 0.002079583238810301\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 376: loss = 0.0020797369070351124\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 377: loss = 0.002078268676996231\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 378: loss = 0.0020756260491907597\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 379: loss = 0.0020761596970260143\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 380: loss = 0.0020737256854772568\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 381: loss = 0.002074100309982896\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 382: loss = 0.0020738167222589254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 383: loss = 0.0020743755158036947\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 384: loss = 0.00206682994030416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 385: loss = 0.0020697915460914373\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 386: loss = 0.002067307708784938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 387: loss = 0.0020644322503358126\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 388: loss = 0.0020623819436877966\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 389: loss = 0.0020598010160028934\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 390: loss = 0.002059550490230322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 391: loss = 0.0020568324252963066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 392: loss = 0.002055942313745618\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 393: loss = 0.0020534733776003122\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 394: loss = 0.0020527609158307314\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 395: loss = 0.002050643553957343\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 396: loss = 0.0020493967458605766\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 397: loss = 0.002048154128715396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 398: loss = 0.0020461492240428925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 399: loss = 0.0020451892632991076\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 400: loss = 0.002043710555881262\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 401: loss = 0.002041709376499057\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 402: loss = 0.00204053008928895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 403: loss = 0.0020387335680425167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 404: loss = 0.002037822036072612\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 405: loss = 0.0020356529857963324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 406: loss = 0.0020341877825558186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 407: loss = 0.0020326741505414248\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 408: loss = 0.002031364245340228\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 409: loss = 0.0020305095240473747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 410: loss = 0.002028705319389701\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 411: loss = 0.0020277737639844418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 412: loss = 0.0020275393035262823\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 413: loss = 0.0020252892281860113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 414: loss = 0.0020263695623725653\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 415: loss = 0.002025214722380042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 416: loss = 0.002024785615503788\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 417: loss = 0.002027289243414998\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 418: loss = 0.002025237772613764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 419: loss = 0.0020203860476613045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 420: loss = 0.0020241597667336464\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 421: loss = 0.0020206833723932505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 422: loss = 0.0020198810379952192\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 423: loss = 0.0020411373116075993\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 424: loss = 0.0020125862210989\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 425: loss = 0.002022306201979518\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 426: loss = 0.0020219769794493914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 427: loss = 0.00201215036213398\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 428: loss = 0.002012466313317418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 429: loss = 0.002011333592236042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 430: loss = 0.0020046932622790337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 431: loss = 0.0020076085347682238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 432: loss = 0.0020070525351911783\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 433: loss = 0.0020021989475935698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 434: loss = 0.0020005449187010527\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 435: loss = 0.001999620581045747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 436: loss = 0.0019993155729025602\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 437: loss = 0.00199439050629735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 438: loss = 0.0019925564993172884\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 439: loss = 0.001991774421185255\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 440: loss = 0.00198990385979414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 441: loss = 0.001987641444429755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 442: loss = 0.0019856540020555258\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 443: loss = 0.0019850919488817453\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 444: loss = 0.0019850037060678005\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 445: loss = 0.001983791124075651\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 446: loss = 0.0019824181217700243\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 447: loss = 0.0019829447846859694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 448: loss = 0.001993949292227626\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 449: loss = 0.0019840169697999954\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 450: loss = 0.001986562041565776\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 451: loss = 0.0019842698238790035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 452: loss = 0.001980801345780492\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 453: loss = 0.001977472798898816\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 454: loss = 0.0019765307661145926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 455: loss = 0.0019762367010116577\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 456: loss = 0.001976039493456483\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 457: loss = 0.0019756576512008905\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 458: loss = 0.001973159145563841\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 459: loss = 0.001970754936337471\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 460: loss = 0.0019677444361150265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 461: loss = 0.0019661036785691977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 462: loss = 0.0019660787656903267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 463: loss = 0.001972402213141322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 464: loss = 0.0019843881018459797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 465: loss = 0.0019744543824344873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 466: loss = 0.0019653993658721447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 467: loss = 0.0019753773231059313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 468: loss = 0.0019735475070774555\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 469: loss = 0.001966099953278899\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 470: loss = 0.001968828495591879\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 471: loss = 0.0019840497989207506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 472: loss = 0.0019673879723995924\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 473: loss = 0.0019806744530797005\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 474: loss = 0.001965187257155776\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 475: loss = 0.001972453435882926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 476: loss = 0.0019716930110007524\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 477: loss = 0.0019638226367533207\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 478: loss = 0.001966145122423768\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 479: loss = 0.001962306909263134\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 480: loss = 0.001955984393134713\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 481: loss = 0.0019582302775233984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 482: loss = 0.0019544116221368313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 483: loss = 0.0019514232408255339\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 484: loss = 0.0019513097358867526\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 485: loss = 0.0019471535924822092\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 486: loss = 0.001946748816408217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 487: loss = 0.0019432702101767063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 488: loss = 0.001942636794410646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 489: loss = 0.0019394999835640192\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 490: loss = 0.0019378630677238107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 491: loss = 0.0019366764463484287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 492: loss = 0.0019338565180078149\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 493: loss = 0.0019326527835801244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 494: loss = 0.0019308244809508324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 495: loss = 0.0019291075877845287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 496: loss = 0.0019275787053629756\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 497: loss = 0.0019251947524026036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 498: loss = 0.0019242851994931698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 499: loss = 0.001922329654917121\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 500: loss = 0.0019199455855414271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 501: loss = 0.0019191772444173694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 502: loss = 0.0019179362570866942\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 503: loss = 0.0019167100545018911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 504: loss = 0.0019158672075718641\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 505: loss = 0.0019145136466249824\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 506: loss = 0.0019144831458106637\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 507: loss = 0.001912606880068779\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 508: loss = 0.0019122599624097347\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 509: loss = 0.001911610714159906\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 510: loss = 0.0019100307254120708\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 511: loss = 0.001909834099933505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 512: loss = 0.0019090402638539672\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 513: loss = 0.0019077631877735257\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 514: loss = 0.0019088274566456676\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 515: loss = 0.001908513717353344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 516: loss = 0.001908089849166572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 517: loss = 0.0019081050995737314\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 518: loss = 0.0019085039384663105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 519: loss = 0.0019080876372754574\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 520: loss = 0.0019041836494579911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 521: loss = 0.0019037219462916255\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 522: loss = 0.0019037834135815501\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 523: loss = 0.00190146139357239\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 524: loss = 0.0019021386979147792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 525: loss = 0.0018973744008690119\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 526: loss = 0.001897681737318635\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 527: loss = 0.001896354486234486\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 528: loss = 0.0018956606509163976\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 529: loss = 0.001893414999358356\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 530: loss = 0.0018940574955195189\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 531: loss = 0.0018928557401522994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 532: loss = 0.001892788684926927\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 533: loss = 0.0018914462998509407\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 534: loss = 0.0018904779572039843\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 535: loss = 0.001890001236461103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 536: loss = 0.0018893915694206953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 537: loss = 0.0018888961058109999\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 538: loss = 0.0018881033174693584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 539: loss = 0.0018871137872338295\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 540: loss = 0.0018877583788707852\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 541: loss = 0.001885647769086063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 542: loss = 0.0018853087676689029\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 543: loss = 0.0018847283208742738\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 544: loss = 0.001883020275272429\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 545: loss = 0.001882659737020731\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 546: loss = 0.0018826589221134782\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 547: loss = 0.0018824482103809714\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 548: loss = 0.0018806259613484144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 549: loss = 0.0018782977713271976\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 550: loss = 0.001877141767181456\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 551: loss = 0.0018765098648145795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 552: loss = 0.0018743263790383935\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 553: loss = 0.0018737604841589928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 554: loss = 0.001873084926046431\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 555: loss = 0.0018721160013228655\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 556: loss = 0.0018785413121804595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 557: loss = 0.0018757624784484506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 558: loss = 0.0018770107999444008\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 559: loss = 0.0018706887494772673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 560: loss = 0.001869875006377697\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 561: loss = 0.0018680408829823136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 562: loss = 0.0018672067672014236\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 563: loss = 0.0018669180572032928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 564: loss = 0.0018669541459530592\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 565: loss = 0.0018641982460394502\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 566: loss = 0.0018646373646333814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 567: loss = 0.0018647070974111557\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 568: loss = 0.0018634528387337923\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 569: loss = 0.0018614919390529394\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 570: loss = 0.0018605252262204885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 571: loss = 0.0018612368730828166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 572: loss = 0.0018593479180708528\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 573: loss = 0.0018582913326099515\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 574: loss = 0.001857907511293888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 575: loss = 0.0018569841049611568\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 576: loss = 0.0018552878173068166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 577: loss = 0.0018541580066084862\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 578: loss = 0.0018525178311392665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 579: loss = 0.0018514028051868081\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 580: loss = 0.0018500670557841659\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 581: loss = 0.0018492358503863215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 582: loss = 0.0018487762426957488\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 583: loss = 0.0018482127925381064\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 584: loss = 0.0018482988234609365\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 585: loss = 0.0018462527077645063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 586: loss = 0.0018440844723954797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 587: loss = 0.001845099264755845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 588: loss = 0.0018443404696881771\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 589: loss = 0.0018477063858881593\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 590: loss = 0.0018484366592019796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 591: loss = 0.001859378768131137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 592: loss = 0.0018476629629731178\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 593: loss = 0.0018428574549034238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 594: loss = 0.0018395268125459552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 595: loss = 0.0018389440374448895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 596: loss = 0.0018343284027650952\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 597: loss = 0.0018358780071139336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 598: loss = 0.0018362359842285514\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 599: loss = 0.0018286008853465319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 600: loss = 0.0018268682761117816\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 601: loss = 0.0018277943599969149\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 602: loss = 0.0018234532326459885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 603: loss = 0.0018205036176368594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 604: loss = 0.0018201671773567796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 605: loss = 0.0018188073299825191\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 606: loss = 0.0018162450287491083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 607: loss = 0.0018146198708564043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 608: loss = 0.0018144758651033044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 609: loss = 0.0018132742261514068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 610: loss = 0.001811760594137013\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 611: loss = 0.0018102304311469197\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 612: loss = 0.0018095637205988169\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 613: loss = 0.0018091575475409627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 614: loss = 0.0018073121318593621\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 615: loss = 0.0018060634611174464\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 616: loss = 0.0018049994250759482\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 617: loss = 0.0018040236318483949\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 618: loss = 0.0018029012717306614\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 619: loss = 0.0018016203539445996\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 620: loss = 0.0018007555045187473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 621: loss = 0.001799904159270227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 622: loss = 0.001799384132027626\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 623: loss = 0.0017983861034736037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 624: loss = 0.001797804026864469\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 625: loss = 0.0017970881890505552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 626: loss = 0.0017965040169656277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 627: loss = 0.0017958515090867877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 628: loss = 0.0017950956244021654\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 629: loss = 0.0017951057525351644\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 630: loss = 0.0017965170554816723\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 631: loss = 0.0017999258125200868\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 632: loss = 0.001793272327631712\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 633: loss = 0.0017939504468813539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 634: loss = 0.0017954549985006452\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 635: loss = 0.0017903049010783434\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 636: loss = 0.001790883601643145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 637: loss = 0.0017879760125651956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 638: loss = 0.0017885202541947365\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 639: loss = 0.0017857165075838566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 640: loss = 0.0017856000922620296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 641: loss = 0.001783194369636476\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 642: loss = 0.0017825417453423142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 643: loss = 0.0017812002915889025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 644: loss = 0.0017800495261326432\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 645: loss = 0.001779514946974814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 646: loss = 0.0017776957247406244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 647: loss = 0.0017762189963832498\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 648: loss = 0.0017751007108017802\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 649: loss = 0.0017726761288940907\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 650: loss = 0.0017715899739414454\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 651: loss = 0.0017700259340927005\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 652: loss = 0.0017684337217360735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 653: loss = 0.0017678914591670036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 654: loss = 0.0017665477935224771\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 655: loss = 0.0017661559395492077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 656: loss = 0.0017666664207354188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 657: loss = 0.0017646815394982696\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 658: loss = 0.0017630686052143574\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 659: loss = 0.0017624353058636189\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 660: loss = 0.0017619781428948045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 661: loss = 0.0017597598489373922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 662: loss = 0.0017591003561392426\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 663: loss = 0.0017582833534106612\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 664: loss = 0.0017567997565492988\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 665: loss = 0.0017565523739904165\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 666: loss = 0.0017551957862451673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 667: loss = 0.001755041303113103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 668: loss = 0.0017548538744449615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 669: loss = 0.0017551977653056383\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 670: loss = 0.0017536155646666884\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 671: loss = 0.0017533866921439767\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 672: loss = 0.0017516791122034192\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 673: loss = 0.0017529656179249287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 674: loss = 0.0017516633961349726\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 675: loss = 0.0017501788679510355\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 676: loss = 0.0017490866594016552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 677: loss = 0.0017494041239842772\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 678: loss = 0.0017494001658633351\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 679: loss = 0.0017478779191151261\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 680: loss = 0.001748505630530417\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 681: loss = 0.001746653695590794\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 682: loss = 0.001747417263686657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 683: loss = 0.001749501214362681\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 684: loss = 0.0017507957527413964\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 685: loss = 0.0017434506444260478\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 686: loss = 0.0017456059576943517\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 687: loss = 0.0017441879026591778\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 688: loss = 0.0017423751996830106\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 689: loss = 0.0017431930173188448\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 690: loss = 0.0017417361959815025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 691: loss = 0.0017414746107533574\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 692: loss = 0.0017412655288353562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 693: loss = 0.0017396642360836267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 694: loss = 0.0017403438687324524\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 695: loss = 0.001738465973176062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 696: loss = 0.0017383269732818007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 697: loss = 0.0017373299924656749\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 698: loss = 0.0017362646758556366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 699: loss = 0.0017359355697408319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 700: loss = 0.0017346348613500595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 701: loss = 0.0017335611628368497\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 702: loss = 0.001732968376018107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 703: loss = 0.001731939148157835\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 704: loss = 0.001731185708194971\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 705: loss = 0.0017305269138887525\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 706: loss = 0.0017292778939008713\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 707: loss = 0.0017290321411564946\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 708: loss = 0.0017287908121943474\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 709: loss = 0.0017321218037977815\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 710: loss = 0.001732295611873269\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 711: loss = 0.0017252727411687374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 712: loss = 0.0017266522627323866\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 713: loss = 0.0017252413090318441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 714: loss = 0.0017230850644409657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 715: loss = 0.0017232871614396572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 716: loss = 0.0017236608546227217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 717: loss = 0.0017224597977474332\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 718: loss = 0.0017205392941832542\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 719: loss = 0.0017201824812218547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 720: loss = 0.0017205384792760015\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 721: loss = 0.001718521467410028\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 722: loss = 0.0017173837404698133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 723: loss = 0.0017167775658890605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 724: loss = 0.0017159862909466028\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 725: loss = 0.001715356600470841\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 726: loss = 0.0017150515923276544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 727: loss = 0.0017151421634480357\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 728: loss = 0.0017151918727904558\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 729: loss = 0.001712413621135056\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 730: loss = 0.001712487661279738\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 731: loss = 0.0017122974386438727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 732: loss = 0.0017111708875745535\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 733: loss = 0.00171146378852427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 734: loss = 0.0017119405092671514\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 735: loss = 0.0017122499411925673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 736: loss = 0.0017116230446845293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 737: loss = 0.0017085615545511246\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 738: loss = 0.0017108850879594684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 739: loss = 0.0017086005536839366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 740: loss = 0.0017100489931181073\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 741: loss = 0.001709542004391551\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 742: loss = 0.0017096285009756684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 743: loss = 0.0017085089348256588\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 744: loss = 0.0017085138242691755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 745: loss = 0.0017071290640160441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 746: loss = 0.0017066304571926594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 747: loss = 0.0017058243975043297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 748: loss = 0.0017053481424227357\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 749: loss = 0.0017046511638909578\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 750: loss = 0.0017041390528902411\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 751: loss = 0.0017037269426509738\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 752: loss = 0.0017024565022438765\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 753: loss = 0.0017012442694976926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 754: loss = 0.001701323431916535\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 755: loss = 0.0017014216864481568\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 756: loss = 0.0017004436813294888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 757: loss = 0.0017013459000736475\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 758: loss = 0.0017007517162710428\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 759: loss = 0.0016998755745589733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 760: loss = 0.0017006292473524809\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 761: loss = 0.0016995742917060852\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 762: loss = 0.0016994375037029386\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 763: loss = 0.0016981963999569416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 764: loss = 0.001698001753538847\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 765: loss = 0.0016977716004475951\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 766: loss = 0.0016968180425465107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 767: loss = 0.0016968239797279239\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 768: loss = 0.0016964601818472147\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 769: loss = 0.0016966862604022026\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 770: loss = 0.0016979246865957975\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 771: loss = 0.001699102227576077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 772: loss = 0.0016977398190647364\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 773: loss = 0.0016936336178332567\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 774: loss = 0.0016957544721662998\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 775: loss = 0.0016931844875216484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 776: loss = 0.0016938409535214305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 777: loss = 0.0016922149807214737\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 778: loss = 0.001692784484475851\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 779: loss = 0.001691716955974698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 780: loss = 0.0016918585170060396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 781: loss = 0.0016928731929510832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 782: loss = 0.0016931204590946436\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 783: loss = 0.001692368765361607\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 784: loss = 0.0016935705207288265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 785: loss = 0.001699413056485355\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 786: loss = 0.0016979904612526298\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 787: loss = 0.0016905288212001324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 788: loss = 0.001695356797426939\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 789: loss = 0.0016912361606955528\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 790: loss = 0.0016936073079705238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 791: loss = 0.0016927989199757576\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 792: loss = 0.0016909503610804677\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 793: loss = 0.001691431156359613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 794: loss = 0.0016921557253226638\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 795: loss = 0.0016903264913707972\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 796: loss = 0.0016910829581320286\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 797: loss = 0.0016911110142245889\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 798: loss = 0.0016893759602680802\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 799: loss = 0.0016901209019124508\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 800: loss = 0.0016901365015655756\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 801: loss = 0.0016891914419829845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 802: loss = 0.0016904673539102077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 803: loss = 0.0016941179055720568\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 804: loss = 0.001692601596005261\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 805: loss = 0.0016882963245734572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 806: loss = 0.0016901716589927673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 807: loss = 0.0016879583708941936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 808: loss = 0.0016887364909052849\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 809: loss = 0.0016863574273884296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 810: loss = 0.0016849034000188112\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 811: loss = 0.0016843854682520032\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 812: loss = 0.0016823232872411609\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 813: loss = 0.0016815601848065853\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 814: loss = 0.0016814784612506628\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 815: loss = 0.00168040010612458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 816: loss = 0.001679142820648849\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 817: loss = 0.0016786149935796857\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 818: loss = 0.0016782976454123855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 819: loss = 0.0016768008936196566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 820: loss = 0.001676210667937994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 821: loss = 0.0016761631704866886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 822: loss = 0.001675592502579093\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 823: loss = 0.0016750849317759275\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 824: loss = 0.0016744291642680764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 825: loss = 0.001674014376476407\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 826: loss = 0.0016733200754970312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 827: loss = 0.001672555459663272\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 828: loss = 0.0016721788560971618\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 829: loss = 0.001672121463343501\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 830: loss = 0.0016720705898478627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 831: loss = 0.0016711342614144087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 832: loss = 0.0016701427521184087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 833: loss = 0.0016698915278539062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 834: loss = 0.0016689446056261659\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 835: loss = 0.0016689939657226205\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 836: loss = 0.0016684961738064885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 837: loss = 0.0016681939596310258\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 838: loss = 0.0016694430960342288\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 839: loss = 0.0016764997271820903\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 840: loss = 0.0016786628402769566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 841: loss = 0.0016780252335593104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 842: loss = 0.0016667299205437303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 843: loss = 0.0016756524564698339\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 844: loss = 0.00166589196305722\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 845: loss = 0.001669871504418552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 846: loss = 0.0016723291482776403\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 847: loss = 0.0016700030537322164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 848: loss = 0.0016677897656336427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 849: loss = 0.0016677659004926682\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 850: loss = 0.0016679654363542795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 851: loss = 0.0016693976940587163\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 852: loss = 0.0016679526306688786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 853: loss = 0.0016659758985042572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 854: loss = 0.0016651838086545467\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 855: loss = 0.0016648567980155349\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 856: loss = 0.0016641304828226566\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 857: loss = 0.0016643400304019451\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 858: loss = 0.0016634598141536117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 859: loss = 0.0016621793620288372\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 860: loss = 0.001661727437749505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 861: loss = 0.0016609112499281764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 862: loss = 0.001659877598285675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 863: loss = 0.0016594415064901114\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 864: loss = 0.0016588116995990276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 865: loss = 0.0016578587237745523\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 866: loss = 0.0016570648876950145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 867: loss = 0.001656062318943441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 868: loss = 0.0016553481109440327\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 869: loss = 0.0016548116691410542\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 870: loss = 0.0016541562508791685\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 871: loss = 0.0016540255164727569\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 872: loss = 0.0016534129390493035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 873: loss = 0.0016536485636606812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 874: loss = 0.001652474282309413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 875: loss = 0.0016520583303645253\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 876: loss = 0.0016514781164005399\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 877: loss = 0.001651595113798976\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 878: loss = 0.001650893478654325\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 879: loss = 0.0016504188533872366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 880: loss = 0.0016500588972121477\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 881: loss = 0.0016494507435709238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 882: loss = 0.0016492330469191074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 883: loss = 0.001648654812015593\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 884: loss = 0.0016485645901411772\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 885: loss = 0.0016476802993565798\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 886: loss = 0.0016476032324135303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 887: loss = 0.0016477397875860333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 888: loss = 0.0016476268647238612\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 889: loss = 0.0016470587579533458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 890: loss = 0.0016472659772261977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 891: loss = 0.0016479450277984142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 892: loss = 0.0016483220970258117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 893: loss = 0.0016470226692035794\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 894: loss = 0.0016461719060316682\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 895: loss = 0.0016460672486573458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 896: loss = 0.0016460478072986007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 897: loss = 0.0016461771447211504\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 898: loss = 0.0016463937936350703\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 899: loss = 0.0016460956539958715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 900: loss = 0.0016454168362542987\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 901: loss = 0.0016465901862829924\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 902: loss = 0.001647690194658935\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 903: loss = 0.0016489812405779958\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 904: loss = 0.0016594056505709887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 905: loss = 0.0016477251192554832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 906: loss = 0.0016469049733132124\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 907: loss = 0.0016455039149150252\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 908: loss = 0.00164504861459136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 909: loss = 0.001643186784349382\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 910: loss = 0.0016432148404419422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 911: loss = 0.0016419908497482538\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 912: loss = 0.0016412135446444154\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 913: loss = 0.0016406970098614693\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 914: loss = 0.001640619826503098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 915: loss = 0.0016396207502111793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 916: loss = 0.0016384876798838377\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 917: loss = 0.0016387522919103503\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 918: loss = 0.0016375177074223757\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 919: loss = 0.0016375468112528324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 920: loss = 0.0016378910513594747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 921: loss = 0.0016374874394387007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 922: loss = 0.0016368567012250423\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 923: loss = 0.0016364951152354479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 924: loss = 0.0016362193273380399\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 925: loss = 0.0016362376045435667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 926: loss = 0.001635862747207284\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 927: loss = 0.0016354291001334786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 928: loss = 0.0016350940568372607\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 929: loss = 0.0016347236232832074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 930: loss = 0.0016342914896085858\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 931: loss = 0.0016338363057002425\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 932: loss = 0.0016330834478139877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 933: loss = 0.0016329101053997874\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 934: loss = 0.0016321197617799044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 935: loss = 0.001632325816899538\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 936: loss = 0.001631779014132917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 937: loss = 0.0016321586444973946\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 938: loss = 0.0016309547936543822\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 939: loss = 0.0016312629450112581\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 940: loss = 0.001630587736144662\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 941: loss = 0.001630761194974184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 942: loss = 0.0016301870346069336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 943: loss = 0.0016302389558404684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 944: loss = 0.001629929756745696\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 945: loss = 0.0016298956470564008\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 946: loss = 0.0016293985536321998\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 947: loss = 0.0016287817852571607\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 948: loss = 0.0016282362630590796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 949: loss = 0.0016272776992991567\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 950: loss = 0.0016257807146757841\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 951: loss = 0.0016240919940173626\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 952: loss = 0.0016230003675445914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 953: loss = 0.0016215695068240166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 954: loss = 0.001620338181965053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 955: loss = 0.0016196697251871228\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 956: loss = 0.0016190102323889732\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 957: loss = 0.00161750428378582\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 958: loss = 0.0016164586413651705\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 959: loss = 0.0016167251160368323\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 960: loss = 0.001616552472114563\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 961: loss = 0.0016171057941392064\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 962: loss = 0.0016146880807355046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 963: loss = 0.001613873289898038\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 964: loss = 0.001616606255993247\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 965: loss = 0.0016115398611873388\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 966: loss = 0.0016136403428390622\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 967: loss = 0.0016133349854499102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 968: loss = 0.0016102248337119818\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 969: loss = 0.0016108532436192036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 970: loss = 0.0016171284951269627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 971: loss = 0.0016134051838889718\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 972: loss = 0.0016113783931359649\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 973: loss = 0.0016090244753286242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 974: loss = 0.0016103320522233844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 975: loss = 0.001609373022802174\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 976: loss = 0.0016085973475128412\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 977: loss = 0.001605897443369031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 978: loss = 0.001606529112905264\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 979: loss = 0.0016061841743066907\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 980: loss = 0.001605721889063716\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 981: loss = 0.0016046493547037244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 982: loss = 0.0016043019713833928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 983: loss = 0.0016042712377384305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 984: loss = 0.0016050987178459764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 985: loss = 0.0016035712324082851\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 986: loss = 0.0016024468932300806\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 987: loss = 0.001601396594196558\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 988: loss = 0.0016012171981856227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 989: loss = 0.001600122544914484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 990: loss = 0.0016005233628675342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 991: loss = 0.0015999168390408158\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 992: loss = 0.0015992074040696025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 993: loss = 0.0015998336020857096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 994: loss = 0.0015987809747457504\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 995: loss = 0.0015979951713234186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 996: loss = 0.0015978862065821886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 997: loss = 0.0015969519736245275\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 998: loss = 0.001597152091562748\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 999: loss = 0.0015961048193275928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 1000: loss = 0.0015964588383212686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Sites length:  32768\n",
      "min sites:  tensor(-1.0191, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1.0176, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "\n",
    "voroloss_optim = True\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True,  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(\n",
    "    #         sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    #     )\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=voroloss_optim\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/gargoyle1000_800_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 800\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "if voroloss_optim:\n",
    "    sdf_v = model(sites).squeeze(-1)\n",
    "\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a1aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "# d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "# b, f = su.NOT_mt_extraction(sites, sdf_v, d3dsimplices)\n",
    "# ps.register_surface_mesh(\n",
    "#     \"NOT_mt_extraction\",\n",
    "#     b,\n",
    "#     f,\n",
    "#     back_face_policy=\"identical\",\n",
    "#     enabled=False,\n",
    "# )\n",
    "# # ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0f86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zc true_Sdf shape:  torch.Size([3974, 4])\n",
      "zc optimized sdf : torch.Size([3974, 4])\n",
      "sum of zc true Sdf:  60.38836669921875\n",
      "sum of zc opti Sdf:  60.38836669921875\n",
      "Diff   of   sum:  0.0\n",
      "Mean of zc true Sdf:  0.0\n"
     ]
    }
   ],
   "source": [
    "# metric between sites sdf values and their corresponding sdf values on hotspot model\n",
    "true_Sdf = model(sites).squeeze(-1)\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "vertices_to_compute, bisectors_to_compute, used_tet = su.compute_zero_crossing_vertices_3d(\n",
    "    sites, None, None, d3dsimplices, sdf_v\n",
    ")\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "d3d = d3dsimplices[used_tet]\n",
    "zc_sdf = sdf_v[d3d]\n",
    "zc_truesdf = true_Sdf[d3d]\n",
    "print(\"zc true_Sdf shape: \", zc_truesdf.shape)\n",
    "print(\"zc optimized sdf :\", zc_sdf.shape)\n",
    "print(\"sum of zc true Sdf: \", torch.sum(zc_truesdf).item())\n",
    "print(\"sum of zc opti Sdf: \", torch.sum(zc_sdf).item())\n",
    "print(\"Diff   of   sum: \", torch.sum(zc_truesdf - zc_sdf).item())\n",
    "print(\"Mean of zc true Sdf: \", torch.mean(zc_truesdf - zc_sdf).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 223668\n",
      "Delaunay simplices shape: [[ 4096  4128     0  2080]\n",
      " [ 4096 14336  4128  9248]\n",
      " [ 4480   416   256   320]\n",
      " ...\n",
      " [29147 28155 29179 28154]\n",
      " [29147 29179 29178 28154]\n",
      " [12919 11863 12887 11895]]\n",
      "Max vertex index in simplices: 32767\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 32768\n",
      "Computing Delaunay simplices...\n",
      "Number of Delaunay simplices: 223668\n",
      "Delaunay simplices shape: [[ 4096  4128     0  2080]\n",
      " [ 5121  4096  7168     0]\n",
      " [ 4096     0  3072  2080]\n",
      " ...\n",
      " [29147 28155 29179 28154]\n",
      " [29147 29179 29178 28154]\n",
      " [12919 11863 12887 11895]]\n",
      "Max vertex index in simplices: 32767\n",
      "Min vertex index in simplices: 0\n",
      "Site index range: 32768\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final unclipped polygon mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    "    enabled=False,\n",
    ")\n",
    "\n",
    "if voroloss_optim:\n",
    "    ps.show()\n",
    "\n",
    "p, faces = su.cvt_extraction(sites, sdf_v, d3dsimplices.detach().cpu().numpy(), True)\n",
    "# ps.register_point_cloud(\"cvt extraction\", p.detach().cpu().numpy())\n",
    "ps.register_surface_mesh(\"cvt extraction final\", p.detach().cpu().numpy(), faces, back_face_policy=\"identical\")\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "ps.register_surface_mesh(\n",
    "    \"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect, back_face_policy=\"identical\"\n",
    ")\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "d3dsimplices, _ = pygdel3d.triangulate(sites_np)\n",
    "d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(\n",
    "    sites.unsqueeze(0), d3dsimplices, sdf_v.unsqueeze(0), return_tet_idx=False\n",
    ")\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "v_vect = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "\n",
    "ps.register_surface_mesh(\n",
    "    \"MTET\", v_vect.detach().cpu().numpy(), faces.detach().cpu().numpy(), back_face_policy=\"identical\"\n",
    ")\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "# su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "# su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites, sdf = train_DCCVT(\n",
    "#     sites, sdf_v, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=True\n",
    "# )\n",
    "# (\n",
    "#     v_vect,\n",
    "#     f_vect,\n",
    "#     _,\n",
    "#     _,\n",
    "#     _,\n",
    "# ) = su.get_clipped_mesh_numba(sites, None, None, False, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3720 is out of bounds for axis 0 with size 2990",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m     accuracy = np.mean(dists_ours_to_gt**\u001b[32m2\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, completeness\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m ours_pts, _ = \u001b[43msample_points_on_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_obj_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m m = mesh[\u001b[32m1\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m gt_pts, _ = sample_points_on_mesh(m + \u001b[33m\"\u001b[39m\u001b[33m.obj\u001b[39m\u001b[33m\"\u001b[39m, n_points=\u001b[32m100000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36msample_points_on_mesh\u001b[39m\u001b[34m(mesh_path, n_points)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_points_on_mesh\u001b[39m(mesh_path, n_points=\u001b[32m100000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     mesh = \u001b[43mtrimesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# normalize mesh\u001b[39;00m\n\u001b[32m     10\u001b[39m     mesh.apply_translation(-mesh.centroid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/load.py:111\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file_obj, file_type, resolver, force, allow_remote, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mTHIS FUNCTION IS DEPRECATED but there are no current plans for it to be removed.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m  Loaded geometry as trimesh classes\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# call the most general loading case into a `Scene`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m loaded = \u001b[43mload_scene\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_remote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_remote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force == \u001b[33m\"\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# new code should use `load_mesh` for this\u001b[39;00m\n\u001b[32m    121\u001b[39m     log.debug(\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`trimesh.load(force=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmesh\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)` is a compatibility wrapper for `trimesh.load_mesh`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/load.py:216\u001b[39m, in \u001b[36mload_scene\u001b[39m\u001b[34m(file_obj, file_type, resolver, allow_remote, metadata, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg.file_type \u001b[38;5;129;01min\u001b[39;00m mesh_loaders:\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# use mesh loader\u001b[39;00m\n\u001b[32m    214\u001b[39m     parsed = deepcopy(kwargs)\n\u001b[32m    215\u001b[39m     parsed.update(\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[43mmesh_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     )\n\u001b[32m    224\u001b[39m     loaded = _load_kwargs(**parsed)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg.file_type \u001b[38;5;129;01min\u001b[39;00m compressed_loaders:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# for archives, like ZIP files\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Kyushu_experiments-1/venv/lib/python3.12/site-packages/trimesh/exchange/obj.py:244\u001b[39m, in \u001b[36mload_obj\u001b[39m\u001b[34m(file_obj, resolver, group_material, skip_materials, maintain_order, metadata, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m     mask_v = np.zeros(\u001b[38;5;28mlen\u001b[39m(v), dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[43mmask_v\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m]\u001b[49m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# reconstruct the faces with the new vertex indices\u001b[39;00m\n\u001b[32m    247\u001b[39m inverse = np.zeros(\u001b[38;5;28mlen\u001b[39m(v), dtype=np.int64)\n",
      "\u001b[31mIndexError\u001b[39m: index 3720 is out of bounds for axis 0 with size 2990"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    print(mesh_path)\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT  Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours  GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours  GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT  Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "\n",
    "    # Normalize mesh (centered and scaled uniformly)\n",
    "    bbox = mesh.bounds\n",
    "    center = mesh.centroid\n",
    "    scale = np.linalg.norm(bbox[1] - bbox[0])\n",
    "    mesh.apply_translation(-center)\n",
    "    mesh.apply_scale(1.0 / scale)\n",
    "\n",
    "    # Export normalized mesh\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "_, _ = sample_points_on_mesh(\n",
    "    \"/home/wylliam/dev/Kyushu_experiments/outputs/gargoyle_unconverged/cdp1000_v0_cvt100_clipTrue_buildFalse_upsampling0_num_centroids32_target_size32_final.obj\",\n",
    "    n_points=100000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
