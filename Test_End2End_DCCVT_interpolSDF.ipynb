{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_2000.pth\"\n",
    "\n",
    "#\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "torch.manual_seed(69)\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489f6099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Octahedral grid points shape:  torch.Size([1728, 3])\n"
     ]
    }
   ],
   "source": [
    "vs = su.octahedral_grid_points(grid=8, domain=(-1.0, 1.0))\n",
    "print(\"Octahedral grid points shape: \", vs.shape)\n",
    "\n",
    "vs = vs + torch.randn_like(vs) * noise_scale\n",
    "\n",
    "# sites = vs.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "# ps_cloud = ps.register_point_cloud(\"vs\",vs.detach().cpu().numpy())\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD MODEL WITH HOTSPOT\n",
    "# import sys\n",
    "\n",
    "# sys.path.append(\"3rdparty/HotSpot\")\n",
    "# from dataset import shape_3d\n",
    "# import models.Net as Net\n",
    "\n",
    "# loss_type = \"igr_w_heat\"\n",
    "# loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "# train_set = shape_3d.ReconDataset(\n",
    "#     file_path=mesh[1] + \".ply\",\n",
    "#     n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "#     n_samples=10001,  # args.n_iterations,\n",
    "#     grid_res=256,  # args.grid_res,\n",
    "#     grid_range=1.1,  # args.grid_range,\n",
    "#     sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "#     sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "#     n_random_samples=7500,  # args.n_random_samples,\n",
    "#     resample=True,\n",
    "#     compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "#     scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "# )\n",
    "\n",
    "# model = Net.Network(\n",
    "#     latent_size=0,  # args.latent_size,\n",
    "#     in_dim=3,\n",
    "#     decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "#     nl=\"sine\",  # args.nl,\n",
    "#     encoder_type=\"none\",  # args.encoder_type,\n",
    "#     decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "#     neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "#     init_type=\"mfgi\",  # args.init_type,\n",
    "#     sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "#     n_repeat_period=30,  # args.n_repeat_period,\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# ######\n",
    "# test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "# test_data = next(iter(test_dataloader))\n",
    "# mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "# # add noise to mnfld_points\n",
    "# # mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "# mnfld_points.requires_grad_()\n",
    "# print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "# if torch.cuda.is_available():\n",
    "#     map_location = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     map_location = torch.device(\"cpu\")\n",
    "# model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "\n",
    "\n",
    "def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "    Args:\n",
    "        points: (N, 3) tensor of 3D query points\n",
    "        center: (3,) tensor specifying the center of the sphere\n",
    "        radius: float, radius of the sphere\n",
    "\n",
    "    Returns:\n",
    "        sdf: (N,) tensor of signed distances\n",
    "    \"\"\"\n",
    "    return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "\n",
    "# generate points on the sphere\n",
    "mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 315.208704 MB, Reserved: 847.249408 MB\n",
      "torch.Size([32768])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 24**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "# sdf0 = model(sites)\n",
    "\n",
    "sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "sdf0 += torch.randn_like(sdf0) * noise_scale * 2\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "\n",
    "\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n",
    "\n",
    "# print(sdf_grad0.shape)\n",
    "# print(sdf_grad0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites shape:  torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, tet_probs = su.get_clipped_mesh_numba(\n",
    "    sites, None, d3dsimplices, True, sdf0, True\n",
    ")\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaa1bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def eikonal_loss(grad_sdf: torch.Tensor) -> torch.Tensor:\n",
    "# #     \"\"\"\n",
    "# #     Eikonal regularization loss.\n",
    "\n",
    "# #     Args:\n",
    "# #         grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "# #         variant: 'a' for E1a: ½ mean((||∇φ|| - 1)²)\n",
    "# #     Returns:\n",
    "# #         A scalar tensor containing the eikonal loss.\n",
    "# #     \"\"\"\n",
    "# #     norms = torch.norm(grad_sdf, dim=1)  # (N,)\n",
    "# #     loss = 0.5 * torch.mean((norms**2 - 1.0) ** 2)\n",
    "# #     return loss\n",
    "\n",
    "\n",
    "# def motion_by_mean_curvature_loss(\n",
    "#     sdf: torch.Tensor, grad_sdf: torch.Tensor, sites: torch.Tensor, d3dsimplices: torch.Tensor, factor: float = 1.5\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Motion-by-mean-curvature smoothing loss via a smeared Heaviside function.\n",
    "\n",
    "#     Args:\n",
    "#         sdf: Tensor of shape (N,) containing φ at each site.\n",
    "#         grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "#         epsilon_H: Bandwidth ε_H for the smearing (e.g., 1.5 * average edge length).\n",
    "\n",
    "#     Returns:\n",
    "#         A scalar tensor containing the smoothing loss.\n",
    "#     \"\"\"\n",
    "#     # compute epsion_H from sites and d3dsimplices\n",
    "#     d3d = torch.tensor(d3dsimplices).to(device).detach()  # (M,4)\n",
    "#     comb = torch.combinations(torch.arange(d3d.shape[1], device=device), r=2)  # (6,2)\n",
    "#     edges = d3d[:, comb]  # (M,6,2)\n",
    "#     edges = edges.reshape(-1, 2)  # (M*6,2)\n",
    "#     edges, _ = torch.sort(edges, dim=1)  # sort each row so (a,b) == (b,a)\n",
    "#     unique_edges = torch.unique(edges, dim=0)\n",
    "#     v0, v1 = sites[unique_edges[:, 0]], sites[unique_edges[:, 1]]  # (N,3)\n",
    "#     i, j = unique_edges[:, 0], unique_edges[:, 1]  # (N,3)\n",
    "\n",
    "#     phi = sdf\n",
    "#     sign_mask = phi[i] * phi[j] < 0\n",
    "#     v0, v1 = v0[sign_mask], v1[sign_mask]  # only keep edges with opposite signs\n",
    "#     edge_lengths = torch.norm(v1 - v0, dim=1)  # (N,)\n",
    "#     epsilon_H = factor * torch.mean(edge_lengths)  # Bandwidth for the smeared Heaviside function\n",
    "\n",
    "#     # Compute the derivative of the smeared Heaviside H'(φ)\n",
    "#     mask = torch.abs(phi) <= epsilon_H\n",
    "#     H_prime = torch.zeros_like(phi)\n",
    "#     # H'(φ) = (1/(2ε_H)) * (1 + cos(π φ / ε_H)) for |φ| ≤ ε_H\n",
    "#     H_prime[mask] = (1.0 / (2.0 * epsilon_H)) * (1.0 + torch.cos(np.pi * phi[mask] / epsilon_H))\n",
    "\n",
    "#     # Compute |∇H| = |H'(φ)| * ||∇φ||\n",
    "#     norms = torch.norm(grad_sdf, dim=1)\n",
    "#     magnitude = H_prime * norms\n",
    "\n",
    "#     # Ignore very small contributions (tetrahedra already smooth enough)\n",
    "#     valid = magnitude > 1e-8\n",
    "#     if valid.any():\n",
    "#         return torch.mean(magnitude[valid])\n",
    "#     else:\n",
    "#         return torch.tensor(0.0, device=sdf.device)\n",
    "\n",
    "\n",
    "# def smoothness_loss(sites, phi, d3dsimplices, eps=1e-8, weighted=True):\n",
    "#     \"\"\"\n",
    "#     Compute ∑_{(i,j)} w_ij (φ_i - φ_j)^2 over edges from tetrahedral simplices.\n",
    "\n",
    "#     Args:\n",
    "#         sites:     (N,3) float tensor of point positions.\n",
    "#         phi:       (N,)   float tensor of SDF values at sites.\n",
    "#         simplices: (M,4)  long tensor of tetrahedron indices.\n",
    "#         eps:       small constant to avoid divide-by-zero.\n",
    "#         weighted:  if True, weight edges by 1/dist else w=1.\n",
    "#     Returns:\n",
    "#         scalar smoothing loss.\n",
    "#     \"\"\"\n",
    "#     # 1) extract all simplex edges\n",
    "#     d3d = torch.tensor(d3dsimplices).to(device).detach()  # (M,4)\n",
    "#     comb = torch.combinations(torch.arange(d3d.shape[1], device=sites.device), r=2)  # (6,2)\n",
    "#     edges = d3d[:, comb]  # (M,6,2)\n",
    "#     edges = edges.reshape(-1, 2)  # (M*6,2)\n",
    "#     edges = torch.sort(edges, dim=1)[0]  # sort lexicographically\n",
    "#     edges = torch.unique(edges, dim=0)  # keep unique undirected edges\n",
    "#     i, j = edges[:, 0], edges[:, 1]  # index pairs\n",
    "#     sign_mask = phi[i] * phi[j] < 0\n",
    "\n",
    "#     i, j = i[sign_mask], j[sign_mask]  # only keep edges with opposite signs\n",
    "\n",
    "#     if weighted:\n",
    "#         dij = (sites[i] - sites[j]).norm(dim=1)  # (E,)\n",
    "#         w = 1.0 / (dij + eps)\n",
    "#     else:\n",
    "#         w = 1.0\n",
    "\n",
    "#     diff = phi[i] - phi[j]  # (E,)\n",
    "#     loss = torch.mean(w * diff * diff)\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "):\n",
    "    voroloss_optim = True\n",
    "\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.0)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "\n",
    "            v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(\n",
    "                sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "            )\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "            sites_loss = lambda_cvt / 10 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "        else:\n",
    "            sites_loss = lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "\n",
    "        # sites_sdf_grads = su.sdf_space_grad_pytorch_diego(\n",
    "        #     sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "        # )\n",
    "\n",
    "        # print norm min max for sites_sdf_grads\n",
    "        # print(\"sites_sdf_grads norm min: \", sites_sdf_grads.norm(dim=1).min().item())\n",
    "        # print(\"sites_sdf_grads norm max: \", sites_sdf_grads.norm(dim=1).max().item())\n",
    "        # print(\"sites_sdf_grads norm mean: \", sites_sdf_grads.norm(dim=1).mean().item())\n",
    "\n",
    "        # motion_loss = (\n",
    "        #     lambda_cvt\n",
    "        #     / 1000\n",
    "        #     * motion_by_mean_curvature_loss(sites_sdf, sites_sdf_grads, sites, d3dsimplices, factor=1.5)\n",
    "        # )\n",
    "        # eik_loss = lambda_cvt / 10 * torch.mean(sites_sdf_grads - 1) ** 2  # * eikonal_loss(sdf_verts_grads)\n",
    "        # print(\"eikonal_loss: \", eik_loss.item(), \"motion_loss: \", motion_loss.item())\n",
    "        # sm_loss = lambda_cvt / 100 * smoothness_loss(sites, sites_sdf, d3dsimplices)\n",
    "        # print(\"smoothness_loss: \", sm_loss.item())\n",
    "        # sdf_loss = eik_loss + motion_loss\n",
    "        # sdf_loss = eik_loss\n",
    "\n",
    "        loss = sites_loss  # + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites, simplices=d3dsimplices, model=sites_sdf\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.11024878174066544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 1: loss = 0.10602222383022308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 2: loss = 0.10199696570634842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 3: loss = 0.09815452992916107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 4: loss = 0.09448860585689545\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 5: loss = 0.09098295867443085\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 6: loss = 0.08761227130889893\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 7: loss = 0.08435649424791336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 8: loss = 0.08119579404592514\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 9: loss = 0.07811954617500305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 10: loss = 0.07512439787387848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 11: loss = 0.07220581918954849\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 12: loss = 0.06936285644769669\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 13: loss = 0.06659214198589325\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 14: loss = 0.0638861209154129\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 15: loss = 0.061243146657943726\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 16: loss = 0.058662865310907364\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 17: loss = 0.05614938214421272\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 18: loss = 0.053708817809820175\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 19: loss = 0.05134211480617523\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 20: loss = 0.049057185649871826\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 21: loss = 0.04684733971953392\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 22: loss = 0.04471169412136078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 23: loss = 0.042647216469049454\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 24: loss = 0.04066012427210808\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 25: loss = 0.03874131292104721\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 26: loss = 0.03689279034733772\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 27: loss = 0.0351136140525341\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 28: loss = 0.03339911624789238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 29: loss = 0.03174969181418419\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 30: loss = 0.03016798384487629\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 31: loss = 0.028651004657149315\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 32: loss = 0.027194244787096977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 33: loss = 0.025797579437494278\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 34: loss = 0.024459930136799812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 35: loss = 0.023177457973361015\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 36: loss = 0.021947136148810387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 37: loss = 0.020769819617271423\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 38: loss = 0.019644813612103462\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 39: loss = 0.01856946386396885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 40: loss = 0.017541874200105667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 41: loss = 0.01655997894704342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 42: loss = 0.015621460974216461\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 43: loss = 0.014727187342941761\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 44: loss = 0.013872496783733368\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 45: loss = 0.013053487055003643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 46: loss = 0.012276426889002323\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 47: loss = 0.011537039652466774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 48: loss = 0.010830805636942387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 49: loss = 0.010155425406992435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 50: loss = 0.009510718286037445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 51: loss = 0.008897573687136173\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 52: loss = 0.008313831873238087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 53: loss = 0.007761017419397831\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 54: loss = 0.007239645812660456\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 55: loss = 0.006750114727765322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 56: loss = 0.006293110083788633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 57: loss = 0.005868490785360336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 58: loss = 0.005473944824188948\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 59: loss = 0.005106737837195396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 60: loss = 0.004764315206557512\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 61: loss = 0.0044446587562561035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 62: loss = 0.004146252758800983\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 63: loss = 0.003868408501148224\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 64: loss = 0.0036101159639656544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 65: loss = 0.003370764432474971\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 66: loss = 0.003148747840896249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 67: loss = 0.0029431493021547794\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 68: loss = 0.0027535748668015003\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 69: loss = 0.002578555606305599\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 70: loss = 0.0024169457610696554\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 71: loss = 0.0022673876956105232\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 72: loss = 0.0021282287780195475\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 73: loss = 0.00199874141253531\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 74: loss = 0.0018792817136272788\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 75: loss = 0.0017694628331810236\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 76: loss = 0.001668622950091958\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 77: loss = 0.001576113048940897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 78: loss = 0.0014911044854670763\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 79: loss = 0.0014111250638961792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 80: loss = 0.001335781766101718\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 81: loss = 0.00126519869081676\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 82: loss = 0.0011991369538009167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 83: loss = 0.0011378289200365543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 84: loss = 0.001080913352780044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 85: loss = 0.0010280049173161387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 86: loss = 0.0009791759075596929\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 87: loss = 0.0009344452992081642\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 88: loss = 0.000893639400601387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 89: loss = 0.0008563289302401245\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 90: loss = 0.0008219841402024031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 91: loss = 0.0007905904203653336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 92: loss = 0.000761970120947808\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 93: loss = 0.0007358715520240366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 94: loss = 0.0007119698566384614\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 95: loss = 0.0006900504813529551\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 96: loss = 0.0006698556244373322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 97: loss = 0.0006508755031973124\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 98: loss = 0.0006331333424896002\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 99: loss = 0.000616803765296936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 100: loss = 0.0006018408457748592\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 101: loss = 0.000588040507864207\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 102: loss = 0.000575125333853066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 103: loss = 0.0005629040533676744\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 104: loss = 0.0005512266652658582\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 105: loss = 0.0005402248352766037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 106: loss = 0.0005298226024024189\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 107: loss = 0.000520267232786864\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 108: loss = 0.0005115833482705057\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 109: loss = 0.0005037434748373926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 110: loss = 0.0004967583809047937\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 111: loss = 0.0004904763773083687\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 112: loss = 0.0004847465315833688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 113: loss = 0.0004794726555701345\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 114: loss = 0.00047466493560932577\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 115: loss = 0.00047035462921485305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 116: loss = 0.0004662936844397336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 117: loss = 0.00046256097266450524\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 118: loss = 0.0004592635959852487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 119: loss = 0.0004562338290270418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 120: loss = 0.0004534285981208086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 121: loss = 0.00045066216262057424\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 122: loss = 0.0004479986091610044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 123: loss = 0.0004454188165254891\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 124: loss = 0.0004429210093803704\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 125: loss = 0.0004405535291880369\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 126: loss = 0.000438280520029366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 127: loss = 0.00043626167462207377\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 128: loss = 0.0004344274348113686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 129: loss = 0.00043279939563944936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 130: loss = 0.00043124117655679584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 131: loss = 0.00042970068170689046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 132: loss = 0.0004282050649635494\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 133: loss = 0.0004268204211257398\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 134: loss = 0.00042556642438285053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 135: loss = 0.00042433381895534694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 136: loss = 0.00042310645221732557\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 137: loss = 0.00042196305003017187\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 138: loss = 0.0004209268372505903\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 139: loss = 0.00041992307524196804\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 140: loss = 0.0004189287719782442\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 141: loss = 0.00041801168117672205\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 142: loss = 0.0004171348991803825\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 143: loss = 0.000416305148974061\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 144: loss = 0.0004155391943641007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 145: loss = 0.0004147893632762134\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 146: loss = 0.0004140714299865067\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 147: loss = 0.0004133918264415115\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 148: loss = 0.0004127068677917123\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 149: loss = 0.00041205965681001544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 150: loss = 0.0004114375333301723\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 151: loss = 0.0004108252178411931\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 152: loss = 0.0004102508828509599\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 153: loss = 0.0004096762859262526\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 154: loss = 0.00040913603152148426\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 155: loss = 0.0004086056142114103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 156: loss = 0.00040809603524394333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 157: loss = 0.00040760450065135956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 158: loss = 0.0004071264120284468\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 159: loss = 0.00040666249697096646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 160: loss = 0.0004062099033035338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 161: loss = 0.0004057660116814077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 162: loss = 0.0004053335578646511\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 163: loss = 0.00040490154060535133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 164: loss = 0.00040448291110806167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 165: loss = 0.0004040724888909608\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 166: loss = 0.00040367632755078375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 167: loss = 0.0004032897995784879\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 168: loss = 0.00040292562334798276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 169: loss = 0.0004025581874884665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 170: loss = 0.00040220239316113293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 171: loss = 0.0004018544568680227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 172: loss = 0.00040151693974621594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 173: loss = 0.0004011930723208934\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 174: loss = 0.000400872144382447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 175: loss = 0.00040054938290268183\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 176: loss = 0.00040022225584834814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 177: loss = 0.0003999090986326337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 178: loss = 0.0003996013547293842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 179: loss = 0.0003993036225438118\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 180: loss = 0.0003989973047282547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 181: loss = 0.00039869415923021734\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 182: loss = 0.00039839636883698404\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 183: loss = 0.00039811088936403394\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 184: loss = 0.00039783582906238735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 185: loss = 0.0003975619620177895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 186: loss = 0.00039728148840367794\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 187: loss = 0.0003970038378611207\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 188: loss = 0.00039672444108873606\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 189: loss = 0.0003964414063375443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 190: loss = 0.00039616599678993225\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 191: loss = 0.0003958820307161659\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 192: loss = 0.000395611219573766\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 193: loss = 0.00039534716052003205\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 194: loss = 0.0003950841201003641\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 195: loss = 0.00039482838474214077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 196: loss = 0.0003945841162931174\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 197: loss = 0.00039433862548321486\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 198: loss = 0.0003940943570341915\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 199: loss = 0.0003938509617000818\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 200: loss = 0.0003936097491532564\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 201: loss = 0.00039337569614872336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 202: loss = 0.0003931379469577223\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 203: loss = 0.0003929025842808187\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 204: loss = 0.0003926711215171963\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 205: loss = 0.00039244708023034036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 206: loss = 0.0003922226023860276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 207: loss = 0.000391993933590129\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 208: loss = 0.0003917728899978101\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 209: loss = 0.00039155795820988715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 210: loss = 0.0003913372056558728\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 211: loss = 0.000391115783713758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 212: loss = 0.00039089989149942994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 213: loss = 0.0003906880156137049\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Epoch 214: loss = 0.0003904878976754844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 27\u001b[0m\n\u001b[1;32m      7\u001b[0m     sites \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(sites)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# import cProfile, pstats\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# #\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     sites, optimized_sites_sdf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_DCCVT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_weights\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     sites_np \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(site_file_path, sites_np)\n",
      "Cell \u001b[0;32mIn[28], line 81\u001b[0m, in \u001b[0;36mtrain_DCCVT\u001b[0;34m(sites, sites_sdf, max_iter, stop_train_threshold, upsampling, lambda_weights)\u001b[0m\n\u001b[1;32m     79\u001b[0m     sites_loss \u001b[38;5;241m=\u001b[39m lambda_cvt \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m cvt_loss \u001b[38;5;241m+\u001b[39m lambda_chamfer \u001b[38;5;241m*\u001b[39m chamfer_loss_mesh\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     sites_loss \u001b[38;5;241m=\u001b[39m lambda_chamfer \u001b[38;5;241m*\u001b[39m \u001b[43mvoroloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnfld_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# sites_sdf_grads = su.sdf_space_grad_pytorch_diego(\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#     sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# sdf_loss = eik_loss + motion_loss\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# sdf_loss = eik_loss\u001b[39;00m\n\u001b[1;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m sites_loss  \u001b[38;5;66;03m# + sdf_loss\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Kyushu_experiments/sdfpred_utils/loss_functions.py:591\u001b[0m, in \u001b[0;36mVoroloss_opt.__call__\u001b[0;34m(self, points, spoints)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# WARNING: fecthing for knn\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 591\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mknn_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    592\u001b[0m point_to_voronoi_center \u001b[38;5;241m=\u001b[39m points \u001b[38;5;241m-\u001b[39m spoints[indices[:, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    593\u001b[0m voronoi_edge \u001b[38;5;241m=\u001b[39m spoints[indices[:, \u001b[38;5;241m1\u001b[39m:]] \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m    594\u001b[0m     spoints[indices[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/pytorch3d/ops/knn.py:187\u001b[0m, in \u001b[0;36mknn_points\u001b[0;34m(p1, p2, lengths1, lengths2, norm, K, version, return_nn, return_sorted)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lengths2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     lengths2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((p1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), P2, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mp1\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 187\u001b[0m p1_dists, p1_idx \u001b[38;5;241m=\u001b[39m \u001b[43m_knn_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sorted\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m p2_nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_nn:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/pytorch3d/ops/knn.py:76\u001b[0m, in \u001b[0;36m_knn_points.forward\u001b[0;34m(ctx, p1, p2, lengths1, lengths2, K, version, norm, return_sorted)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# sort KNN in ascending order if K > 1\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m K \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m return_sorted:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths2\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m K:\n\u001b[1;32m     77\u001b[0m         P1 \u001b[38;5;241m=\u001b[39m p1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     78\u001b[0m         mask \u001b[38;5;241m=\u001b[39m lengths2[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(K, device\u001b[38;5;241m=\u001b[39mdists\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # #\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/sphere1000_200_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 200\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Computing Delaunay simplices...\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_obj_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(dists_ours_to_gt\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, completeness\n\u001b[0;32m---> 34\u001b[0m ours_pts, _ \u001b[38;5;241m=\u001b[39m sample_points_on_mesh(\u001b[43moutput_obj_file\u001b[49m, n_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m     35\u001b[0m m \u001b[38;5;241m=\u001b[39m mesh[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmesh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m gt_pts, _ \u001b[38;5;241m=\u001b[39m sample_points_on_mesh(m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_obj_file' is not defined"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT → Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours → GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours → GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT → Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "\n",
    "    # Normalize mesh (centered and scaled uniformly)\n",
    "    bbox = mesh.bounds\n",
    "    center = mesh.centroid\n",
    "    scale = np.linalg.norm(bbox[1] - bbox[0])\n",
    "    mesh.apply_translation(-center)\n",
    "    mesh.apply_scale(1.0 / scale)\n",
    "\n",
    "    # Export normalized mesh\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "_, _ = sample_points_on_mesh(\n",
    "    \"/home/wylliam/dev/Kyushu_experiments/outputs/gargoyle_unconverged/cdp1000_v0_cvt100_clipTrue_buildFalse_upsampling0_num_centroids32_target_size32_final.obj\",\n",
    "    n_points=100000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
