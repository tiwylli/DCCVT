{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "torch.manual_seed(69)\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_2000.pth\"\n",
    "\n",
    "#\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n",
      "torch.float32\n",
      "torch.Size([32768, 3])\n",
      "Allocated: 432.168448 MB, Reserved: 448.790528 MB\n",
      "torch.Size([32768])\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "\n",
    "import sys\n",
    "\n",
    "if mesh[0] != \"sphere\":\n",
    "    sys.path.append(\"3rdparty/HotSpot\")\n",
    "    from dataset import shape_3d\n",
    "    import models.Net as Net\n",
    "\n",
    "    loss_type = \"igr_w_heat\"\n",
    "    loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "    train_set = shape_3d.ReconDataset(\n",
    "        file_path=mesh[1] + \".ply\",\n",
    "        n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "        n_samples=10001,  # args.n_iterations,\n",
    "        grid_res=256,  # args.grid_res,\n",
    "        grid_range=1.1,  # args.grid_range,\n",
    "        sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "        sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "        n_random_samples=7500,  # args.n_random_samples,\n",
    "        resample=True,\n",
    "        compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "        scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "    )\n",
    "\n",
    "    model = Net.Network(\n",
    "        latent_size=0,  # args.latent_size,\n",
    "        in_dim=3,\n",
    "        decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "        nl=\"sine\",  # args.nl,\n",
    "        encoder_type=\"none\",  # args.encoder_type,\n",
    "        decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "        neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "        init_type=\"mfgi\",  # args.init_type,\n",
    "        sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "        n_repeat_period=30,  # args.n_repeat_period,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ######\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_data = next(iter(test_dataloader))\n",
    "    mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "    # add noise to mnfld_points\n",
    "    # mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "    mnfld_points.requires_grad_()\n",
    "    print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "    if torch.cuda.is_available():\n",
    "        map_location = torch.device(\"cuda\")\n",
    "    else:\n",
    "        map_location = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "    sdf0 = model(sites)\n",
    "\n",
    "else:\n",
    "\n",
    "    def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3) tensor of 3D query points\n",
    "            center: (3,) tensor specifying the center of the sphere\n",
    "            radius: float, radius of the sphere\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,) tensor of signed distances\n",
    "        \"\"\"\n",
    "        return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "    def sphere_sdf_with_noise(\n",
    "        points: torch.Tensor, center: torch.Tensor, radius: float, noise_amplitude=0.05\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sphere SDF with smooth directional noise added near the surface.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3)\n",
    "            center: (3,)\n",
    "            radius: float\n",
    "            noise_amplitude: float\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,)\n",
    "        \"\"\"\n",
    "        rel = points - center\n",
    "        norm = torch.norm(rel, dim=-1)  # (N,)\n",
    "        base_sdf = norm - radius  # (N,)\n",
    "\n",
    "        # Smooth periodic noise based on direction\n",
    "        unit_dir = rel / (norm.unsqueeze(-1) + 1e-9)  # (N,3)\n",
    "        noise = torch.sin(10 * unit_dir[:, 0]) * torch.sin(10 * unit_dir[:, 1]) * torch.sin(10 * unit_dir[:, 2])\n",
    "\n",
    "        # Weight noise so it mostly affects surface area\n",
    "        falloff = torch.exp(-20 * (base_sdf**2))  # (N,) ~1 near surface, ~0 far\n",
    "        sdf = base_sdf + noise_amplitude * noise * falloff\n",
    "\n",
    "        return sdf\n",
    "\n",
    "    # generate points on the sphere\n",
    "    mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "    mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "    mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()\n",
    "    # sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "    sdf0 = sphere_sdf_with_noise(sites, torch.zeros(3).to(device), 0.50, noise_amplitude=0.1)\n",
    "\n",
    "##add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 24**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites shape:  torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, tet_probs = su.get_clipped_mesh_numba(\n",
    "    sites, None, d3dsimplices, True, sdf0, True\n",
    ")\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa1bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eikonal_loss(sites, grad_sdf, tets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Eikonal regularization loss.\n",
    "\n",
    "    Args:\n",
    "        grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "        variant: 'a' for E1a: ½ mean((||∇φ|| - 1)²)\n",
    "    Returns:\n",
    "        A scalar tensor containing the eikonal loss.\n",
    "    \"\"\"\n",
    "    grad_a = grad_sdf[tets[:, 0]]  # (M,3)\n",
    "    grad_b = grad_sdf[tets[:, 1]]  # (M,3)\n",
    "    grad_c = grad_sdf[tets[:, 2]]  # (M,3)\n",
    "    grad_d = grad_sdf[tets[:, 3]]  # (M,3\n",
    "\n",
    "    grad_avg = (grad_a + grad_b + grad_c + grad_d) / 4.0\n",
    "    grad_norm2 = (grad_avg**2).sum(dim=-1)\n",
    "\n",
    "    a = sites[tets[:, 0]]\n",
    "    b = sites[tets[:, 1]]\n",
    "    c = sites[tets[:, 2]]\n",
    "    d = sites[tets[:, 3]]\n",
    "\n",
    "    volume = su.volume_tetrahedron(a, b, c, d)\n",
    "\n",
    "    loss = 0.5 * torch.mean(volume * (grad_norm2 - 1) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def heaviside_derivative(phi: torch.Tensor, eps_H: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Derivative H'(φ̂) of the smoothed Heaviside function.\n",
    "    \"\"\"\n",
    "    H_prime = torch.zeros_like(phi)\n",
    "\n",
    "    inside = (phi >= -eps_H) & (phi <= eps_H)\n",
    "    H_prime[inside] = 1 / (2 * eps_H) + (1 / (2 * math.pi * eps_H)) * torch.cos(math.pi * phi[inside] / eps_H)\n",
    "\n",
    "    return H_prime\n",
    "\n",
    "\n",
    "def smoothed_heaviside_loss(sites, sdf, grad_sdf, tets, eps_H=0.07, eps_grad=1e-8):\n",
    "    \"\"\"\n",
    "    Fast approximation of E2 = ∑_t |∇H(φ̂)| * Volume(t)\n",
    "    using average of chain-rule gradients.\n",
    "    \"\"\"\n",
    "    # 1. Compute ∇φ at each site\n",
    "    grad_phi = grad_sdf  # su.sdf_space_grad_pytorch_diego(sites, sdf, tets)  # (N,3)\n",
    "\n",
    "    # 2. Compute H'(φ̂) at each site\n",
    "    H_prime = heaviside_derivative(sdf, eps_H)  # (N,)\n",
    "\n",
    "    # 3. Multiply: ∇H = H'(φ̂) · ∇φ\n",
    "    grad_H = grad_phi * H_prime[:, None]  # (N,3)\n",
    "\n",
    "    # 4. Gather per tet: average over 4 sites\n",
    "    g0 = grad_H[tets[:, 0]]\n",
    "    g1 = grad_H[tets[:, 1]]\n",
    "    g2 = grad_H[tets[:, 2]]\n",
    "    g3 = grad_H[tets[:, 3]]\n",
    "    grad_H_avg = (g0 + g1 + g2 + g3) / 4  # (M,3)\n",
    "\n",
    "    # 5. Norm of gradient\n",
    "    grad_norm = grad_H_avg.norm(dim=1)  # (M,)\n",
    "\n",
    "    # 6. Compute volume\n",
    "    a = sites[tets[:, 0]]\n",
    "    b = sites[tets[:, 1]]\n",
    "    c = sites[tets[:, 2]]\n",
    "    d = sites[tets[:, 3]]\n",
    "    volume = su.volume_tetrahedron(a, b, c, d)  # (M,)\n",
    "\n",
    "    # 7. Mask small gradients\n",
    "    mask = grad_norm >= eps_grad\n",
    "\n",
    "    return torch.mean(volume[mask] * grad_norm[mask])\n",
    "\n",
    "\n",
    "# def motion_by_mean_curvature_loss(\n",
    "#     sdf: torch.Tensor, grad_sdf: torch.Tensor, sites: torch.Tensor, d3dsimplices: torch.Tensor, factor: float = 1.5\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Motion-by-mean-curvature smoothing loss via a smeared Heaviside function.\n",
    "\n",
    "#     Args:\n",
    "#         sdf: Tensor of shape (N,) containing φ at each site.\n",
    "#         grad_sdf: Tensor of shape (N, 3) containing ∇φ at each site.\n",
    "#         epsilon_H: Bandwidth ε_H for the smearing (e.g., 1.5 * average edge length).\n",
    "\n",
    "#     Returns:\n",
    "#         A scalar tensor containing the smoothing loss.\n",
    "#     \"\"\"\n",
    "#     # compute epsion_H from sites and d3dsimplices\n",
    "#     d3d = torch.tensor(d3dsimplices).to(device).detach()  # (M,4)\n",
    "#     comb = torch.combinations(torch.arange(d3d.shape[1], device=device), r=2)  # (6,2)\n",
    "#     edges = d3d[:, comb]  # (M,6,2)\n",
    "#     edges = edges.reshape(-1, 2)  # (M*6,2)\n",
    "#     edges, _ = torch.sort(edges, dim=1)  # sort each row so (a,b) == (b,a)\n",
    "#     unique_edges = torch.unique(edges, dim=0)\n",
    "#     v0, v1 = sites[unique_edges[:, 0]], sites[unique_edges[:, 1]]  # (N,3)\n",
    "#     i, j = unique_edges[:, 0], unique_edges[:, 1]  # (N,3)\n",
    "\n",
    "#     phi = sdf\n",
    "#     sign_mask = phi[i] * phi[j] < 0\n",
    "#     v0, v1 = v0[sign_mask], v1[sign_mask]  # only keep edges with opposite signs\n",
    "#     edge_lengths = torch.norm(v1 - v0, dim=1)  # (N,)\n",
    "#     epsilon_H = factor * torch.mean(edge_lengths)  # Bandwidth for the smeared Heaviside function\n",
    "\n",
    "#     # Compute the derivative of the smeared Heaviside H'(φ)\n",
    "#     mask = torch.abs(phi) <= epsilon_H\n",
    "#     H_prime = torch.zeros_like(phi)\n",
    "#     # H'(φ) = (1/(2ε_H)) * (1 + cos(π φ / ε_H)) for |φ| ≤ ε_H\n",
    "#     H_prime[mask] = (1.0 / (2.0 * epsilon_H)) * (1.0 + torch.cos(np.pi * phi[mask] / epsilon_H))\n",
    "\n",
    "#     # Compute |∇H| = |H'(φ)| * ||∇φ||\n",
    "#     norms = torch.norm(grad_sdf, dim=1)\n",
    "#     magnitude = H_prime * norms\n",
    "\n",
    "#     # Ignore very small contributions (tetrahedra already smooth enough)\n",
    "#     valid = magnitude > 1e-8\n",
    "#     if valid.any():\n",
    "#         return torch.mean(magnitude[valid])\n",
    "#     else:\n",
    "#         return torch.tensor(0.0, device=sdf.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "    voroloss_optim=False,\n",
    "):\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.0)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "\n",
    "            v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(\n",
    "                sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "            )\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "            sites_loss = lambda_cvt / 10 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "\n",
    "            sites_sdf_grads = su.sdf_space_grad_pytorch_diego(\n",
    "                sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "            )\n",
    "\n",
    "            eik_loss = lambda_cvt / 10 * eikonal_loss(sites, sites_sdf_grads, d3dsimplices)\n",
    "            print(\"eikonal_loss: \", eik_loss.item(), \"motion_loss: \")\n",
    "            shl = lambda_cvt / 1 * smoothed_heaviside_loss(sites, sites_sdf, sites_sdf_grads, d3dsimplices)\n",
    "            print(\"smoothed_heaviside_loss: \", shl.item())\n",
    "            sdf_loss = eik_loss + shl\n",
    "        else:\n",
    "            sites_loss = lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "\n",
    "        loss = sites_loss + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites, simplices=d3dsimplices, model=sites_sdf\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eikonal_loss:  1.0085632801055908 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015408656559884548\n",
      "Epoch 0: loss = 2.449723243713379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.7305145859718323 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01544200349599123\n",
      "Epoch 1: loss = 2.1410975456237793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5419363379478455 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015513308346271515\n",
      "Epoch 2: loss = 1.871730089187622\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.9431252479553223 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015533480793237686\n",
      "Epoch 3: loss = 2.168600082397461\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5182637572288513 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01550684217363596\n",
      "Epoch 4: loss = 1.7220542430877686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.47663378715515137 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01556705217808485\n",
      "Epoch 5: loss = 1.6009565591812134\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.2839350998401642 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01559501513838768\n",
      "Epoch 6: loss = 1.4539064168930054\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.27270349860191345 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01553663332015276\n",
      "Epoch 7: loss = 1.4359631538391113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.27349555492401123 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015538832172751427\n",
      "Epoch 8: loss = 1.4305980205535889\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.2474459856748581 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015578986145555973\n",
      "Epoch 9: loss = 1.408602237701416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.21079681813716888 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01558801718056202\n",
      "Epoch 10: loss = 1.328560471534729\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.18700739741325378 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015617646276950836\n",
      "Epoch 11: loss = 1.2396764755249023\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1695607602596283 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015619088895618916\n",
      "Epoch 12: loss = 1.2394075393676758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.15760838985443115 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015607470646500587\n",
      "Epoch 13: loss = 1.1235957145690918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.25951921939849854 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015614445321261883\n",
      "Epoch 14: loss = 1.2492144107818604\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.21416105329990387 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0156480073928833\n",
      "Epoch 15: loss = 1.2111403942108154\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1424463391304016 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015668703243136406\n",
      "Epoch 16: loss = 1.1671234369277954\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.15369147062301636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015656832605600357\n",
      "Epoch 17: loss = 1.123141884803772\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1587689220905304 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015711287036538124\n",
      "Epoch 18: loss = 1.1321513652801514\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.15102002024650574 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01568775624036789\n",
      "Epoch 19: loss = 1.111372947692871\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1436440795660019 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015687979757785797\n",
      "Epoch 20: loss = 1.0153796672821045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1384606659412384 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015686852857470512\n",
      "Epoch 21: loss = 1.0300331115722656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09115071594715118 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01567050628364086\n",
      "Epoch 22: loss = 0.9507474303245544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08751124143600464 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01564382202923298\n",
      "Epoch 23: loss = 1.0107030868530273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08249056339263916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015630319714546204\n",
      "Epoch 24: loss = 0.9956270456314087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07340925186872482 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015626922249794006\n",
      "Epoch 25: loss = 0.9623117446899414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08576436340808868 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015598657540977001\n",
      "Epoch 26: loss = 1.0217819213867188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08263939619064331 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015581300482153893\n",
      "Epoch 27: loss = 0.9749459028244019\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09550248086452484 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015561556443572044\n",
      "Epoch 28: loss = 0.9863206744194031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09157238900661469 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015533868223428726\n",
      "Epoch 29: loss = 1.007216453552246\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09023866057395935 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015523933805525303\n",
      "Epoch 30: loss = 1.0109623670578003\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.12386225163936615 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015441264025866985\n",
      "Epoch 31: loss = 1.0553263425827026\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.11904682964086533 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015425366349518299\n",
      "Epoch 32: loss = 1.0605547428131104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.11322840303182602 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015424264594912529\n",
      "Epoch 33: loss = 0.9975491762161255\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10493691265583038 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015377475880086422\n",
      "Epoch 34: loss = 0.971023440361023\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09995192289352417 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015372666530311108\n",
      "Epoch 35: loss = 0.9451847076416016\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07688150554895401 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015352897346019745\n",
      "Epoch 36: loss = 0.941051721572876\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10671783983707428 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015365034341812134\n",
      "Epoch 37: loss = 0.9839645028114319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09985877573490143 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01532843615859747\n",
      "Epoch 38: loss = 1.023543119430542\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09995383024215698 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015300128608942032\n",
      "Epoch 39: loss = 0.9967293739318848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0964646190404892 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015314202755689621\n",
      "Epoch 40: loss = 0.9770005941390991\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09462285041809082 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015296027064323425\n",
      "Epoch 41: loss = 0.9487069249153137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09124448895454407 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01529079582542181\n",
      "Epoch 42: loss = 0.9433833360671997\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08799438178539276 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015270560048520565\n",
      "Epoch 43: loss = 0.953188955783844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08546574413776398 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01529997494071722\n",
      "Epoch 44: loss = 0.9767926931381226\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09867135435342789 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015293296426534653\n",
      "Epoch 45: loss = 0.9322875738143921\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08997520804405212 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015298391692340374\n",
      "Epoch 46: loss = 0.9318403601646423\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08677471429109573 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015270021744072437\n",
      "Epoch 47: loss = 0.919494092464447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08712078630924225 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01526446733623743\n",
      "Epoch 48: loss = 0.9006948471069336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08487581461668015 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015233601443469524\n",
      "Epoch 49: loss = 0.884489119052887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08189563453197479 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015216105617582798\n",
      "Epoch 50: loss = 0.8474137783050537\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07959330081939697 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015203546732664108\n",
      "Epoch 51: loss = 0.8542341589927673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0720742791891098 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015170233324170113\n",
      "Epoch 52: loss = 0.8204277157783508\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06996936351060867 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015146262012422085\n",
      "Epoch 53: loss = 0.779398500919342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07315823435783386 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015138066373765469\n",
      "Epoch 54: loss = 0.77931809425354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07467091828584671 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015127669088542461\n",
      "Epoch 55: loss = 0.7746990323066711\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07214516401290894 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015107939951121807\n",
      "Epoch 56: loss = 0.8052954077720642\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06990616768598557 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015082890167832375\n",
      "Epoch 57: loss = 0.8340387344360352\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0955405905842781 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015063651837408543\n",
      "Epoch 58: loss = 0.878667414188385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06202711910009384 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01503180805593729\n",
      "Epoch 59: loss = 0.8259329199790955\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.065972238779068 motion_loss: \n",
      "smoothed_heaviside_loss:  0.015005373395979404\n",
      "Epoch 60: loss = 0.8509703278541565\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06451292335987091 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01497567817568779\n",
      "Epoch 61: loss = 0.8633277416229248\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.061478693038225174 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014968154020607471\n",
      "Epoch 62: loss = 0.8329340815544128\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.060139477252960205 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01495227962732315\n",
      "Epoch 63: loss = 0.8599131107330322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0582052543759346 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014934326522052288\n",
      "Epoch 64: loss = 0.8907963037490845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.053422655910253525 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014908816665410995\n",
      "Epoch 65: loss = 0.9109285473823547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05403021350502968 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0149007523432374\n",
      "Epoch 66: loss = 0.9012539982795715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05502566322684288 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014892936684191227\n",
      "Epoch 67: loss = 0.8483823537826538\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05477514490485191 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0148551594465971\n",
      "Epoch 68: loss = 0.8800206184387207\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.3523576259613037 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01479924563318491\n",
      "Epoch 69: loss = 1.1615381240844727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.053116269409656525 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014772100374102592\n",
      "Epoch 70: loss = 0.8474156260490417\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.053411610424518585 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014718337915837765\n",
      "Epoch 71: loss = 0.8806288242340088\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.052310459315776825 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014661919325590134\n",
      "Epoch 72: loss = 0.8793050050735474\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.050573866814374924 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014595327898859978\n",
      "Epoch 73: loss = 0.8554102778434753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04904758557677269 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01456289179623127\n",
      "Epoch 74: loss = 0.8382787108421326\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04864530637860298 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01453191228210926\n",
      "Epoch 75: loss = 0.83015376329422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.11042170971632004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014488574117422104\n",
      "Epoch 76: loss = 0.9392942786216736\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10568040609359741 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014457051642239094\n",
      "Epoch 77: loss = 0.8751716017723083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10067682713270187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014441220089793205\n",
      "Epoch 78: loss = 0.8633837103843689\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10001976042985916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014418532140552998\n",
      "Epoch 79: loss = 0.8882935047149658\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.045057378709316254 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0143760796636343\n",
      "Epoch 80: loss = 0.8522042632102966\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.044581346213817596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014335949905216694\n",
      "Epoch 81: loss = 0.8494076132774353\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04504324495792389 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014300293289124966\n",
      "Epoch 82: loss = 0.8223706483840942\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04327958822250366 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014281976968050003\n",
      "Epoch 83: loss = 0.829688310623169\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04313096031546593 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01425644475966692\n",
      "Epoch 84: loss = 0.7855525016784668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.045649684965610504 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014210743829607964\n",
      "Epoch 85: loss = 0.7975539565086365\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04417899623513222 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014200668781995773\n",
      "Epoch 86: loss = 0.7892473936080933\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.042559150606393814 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014141682535409927\n",
      "Epoch 87: loss = 0.8138410449028015\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0395025871694088 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014091350138187408\n",
      "Epoch 88: loss = 0.7561517357826233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.038245152682065964 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014048686251044273\n",
      "Epoch 89: loss = 0.7553442716598511\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04004598408937454 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014044202864170074\n",
      "Epoch 90: loss = 0.7595952749252319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03608136624097824 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014038356021046638\n",
      "Epoch 91: loss = 0.7394300103187561\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039049237966537476 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014008183032274246\n",
      "Epoch 92: loss = 0.797983705997467\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04150617495179176 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013970891945064068\n",
      "Epoch 93: loss = 0.7923979163169861\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05557742342352867 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013929519802331924\n",
      "Epoch 94: loss = 0.8300597071647644\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.051819927990436554 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013886273838579655\n",
      "Epoch 95: loss = 0.8030964136123657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.048786792904138565 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013863395899534225\n",
      "Epoch 96: loss = 0.8083449602127075\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04597065970301628 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013840310275554657\n",
      "Epoch 97: loss = 0.7883409857749939\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04286552965641022 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01382757630199194\n",
      "Epoch 98: loss = 0.7720828056335449\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039237163960933685 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013809255324304104\n",
      "Epoch 99: loss = 0.7348533868789673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03979282081127167 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013783229514956474\n",
      "Epoch 100: loss = 0.7322129011154175\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03773128241300583 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013764116913080215\n",
      "Epoch 101: loss = 0.7197175025939941\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0355130098760128 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013714206404983997\n",
      "Epoch 102: loss = 0.7185208201408386\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03572418913245201 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013678285293281078\n",
      "Epoch 103: loss = 0.7005813717842102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04479376971721649 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013643674552440643\n",
      "Epoch 104: loss = 0.7232771515846252\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.040662866085767746 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013622647151350975\n",
      "Epoch 105: loss = 0.7253077030181885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03671405091881752 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013604884035885334\n",
      "Epoch 106: loss = 0.6802818775177002\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0338442325592041 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013598965480923653\n",
      "Epoch 107: loss = 0.705180287361145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06794759631156921 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01355210691690445\n",
      "Epoch 108: loss = 0.758495032787323\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06330926716327667 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013514280319213867\n",
      "Epoch 109: loss = 0.710676372051239\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05977009981870651 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01347307302057743\n",
      "Epoch 110: loss = 0.7213476896286011\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.057646673172712326 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013449086807668209\n",
      "Epoch 111: loss = 0.7538046836853027\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.054092299193143845 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01341213472187519\n",
      "Epoch 112: loss = 0.7761776447296143\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.053287357091903687 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013388119637966156\n",
      "Epoch 113: loss = 0.7400248050689697\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05352424830198288 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013366223312914371\n",
      "Epoch 114: loss = 0.7210167050361633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.049171172082424164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013323143124580383\n",
      "Epoch 115: loss = 0.7460735440254211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.048272643238306046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013303800486028194\n",
      "Epoch 116: loss = 0.7493188381195068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04637840390205383 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013268728740513325\n",
      "Epoch 117: loss = 0.7584288716316223\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.044514965265989304 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013259924948215485\n",
      "Epoch 118: loss = 0.7089951634407043\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.046877965331077576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013247165828943253\n",
      "Epoch 119: loss = 0.7251289486885071\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04522448033094406 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013182111084461212\n",
      "Epoch 120: loss = 0.7415584325790405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04148203507065773 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013160837814211845\n",
      "Epoch 121: loss = 0.7473074793815613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039283864200115204 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013113053515553474\n",
      "Epoch 122: loss = 0.7241511344909668\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03769386187195778 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01311323419213295\n",
      "Epoch 123: loss = 0.7401516437530518\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03702538460493088 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013081066310405731\n",
      "Epoch 124: loss = 0.739684522151947\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03575493022799492 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013051541522145271\n",
      "Epoch 125: loss = 0.7078378200531006\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0446847528219223 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013052561320364475\n",
      "Epoch 126: loss = 0.6992291212081909\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04031640291213989 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013030151836574078\n",
      "Epoch 127: loss = 0.7439530491828918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.038943007588386536 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013004605658352375\n",
      "Epoch 128: loss = 0.7155910134315491\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04024006798863411 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012989433482289314\n",
      "Epoch 129: loss = 0.7330033779144287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031744662672281265 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012966975569725037\n",
      "Epoch 130: loss = 0.7144813537597656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.030798759311437607 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012933106161653996\n",
      "Epoch 131: loss = 0.7412835955619812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.030027925968170166 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012909293174743652\n",
      "Epoch 132: loss = 0.7389650940895081\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026127152144908905 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012870642356574535\n",
      "Epoch 133: loss = 0.7403892278671265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02236485667526722 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012854328379034996\n",
      "Epoch 134: loss = 0.6965766549110413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02931654267013073 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012822187505662441\n",
      "Epoch 135: loss = 0.696954607963562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03274077922105789 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01279471069574356\n",
      "Epoch 136: loss = 0.6872881650924683\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031760986894369125 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01277144905179739\n",
      "Epoch 137: loss = 0.7088550925254822\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031977150589227676 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012750036083161831\n",
      "Epoch 138: loss = 0.7194898128509521\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039865266531705856 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012734494172036648\n",
      "Epoch 139: loss = 0.7114217281341553\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031166359782218933 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01268948707729578\n",
      "Epoch 140: loss = 0.6783329844474792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.029565993696451187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012676822021603584\n",
      "Epoch 141: loss = 0.7020304203033447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026540791615843773 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01265210472047329\n",
      "Epoch 142: loss = 0.7172535061836243\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0251733660697937 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012613659724593163\n",
      "Epoch 143: loss = 0.7044687867164612\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02454889938235283 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012578953057527542\n",
      "Epoch 144: loss = 0.6913414001464844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.023634810000658035 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012562300078570843\n",
      "Epoch 145: loss = 0.6757774949073792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.023101532831788063 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012540346011519432\n",
      "Epoch 146: loss = 0.6399979591369629\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022315695881843567 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012508347630500793\n",
      "Epoch 147: loss = 0.6600061655044556\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02195272408425808 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012497340328991413\n",
      "Epoch 148: loss = 0.6867246627807617\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02042306773364544 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01248482707887888\n",
      "Epoch 149: loss = 0.6807335615158081\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020714107900857925 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012477625161409378\n",
      "Epoch 150: loss = 0.6847904920578003\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020807575434446335 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012454768642783165\n",
      "Epoch 151: loss = 0.6565325260162354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01578925922513008 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012436304241418839\n",
      "Epoch 152: loss = 0.71282958984375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014072570949792862 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012418213300406933\n",
      "Epoch 153: loss = 0.6994009613990784\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01371216494590044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012417232617735863\n",
      "Epoch 154: loss = 0.6954426169395447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013291846960783005 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012371212244033813\n",
      "Epoch 155: loss = 0.7096057534217834\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01289227232336998 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012357988394796848\n",
      "Epoch 156: loss = 0.7321293950080872\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01265955250710249 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012334979139268398\n",
      "Epoch 157: loss = 0.7339738607406616\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012613577768206596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012326061725616455\n",
      "Epoch 158: loss = 0.7468428015708923\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012961499392986298 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012292376719415188\n",
      "Epoch 159: loss = 0.733195424079895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012430882081389427 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01226455345749855\n",
      "Epoch 160: loss = 0.7258381247520447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012225992046296597 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012220900505781174\n",
      "Epoch 161: loss = 0.6910136938095093\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011957116425037384 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012186484411358833\n",
      "Epoch 162: loss = 0.7320356965065002\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014253247529268265 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012169244699180126\n",
      "Epoch 163: loss = 0.7236874103546143\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014039169996976852 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012158950790762901\n",
      "Epoch 164: loss = 0.6917880773544312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011620847508311272 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012123829685151577\n",
      "Epoch 165: loss = 0.7520931959152222\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01120226364582777 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012088202871382236\n",
      "Epoch 166: loss = 0.7326521277427673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011025968939065933 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012051001191139221\n",
      "Epoch 167: loss = 0.7361710667610168\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010766285471618176 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01202230341732502\n",
      "Epoch 168: loss = 0.7382836937904358\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010667141526937485 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012017911300063133\n",
      "Epoch 169: loss = 0.6901540160179138\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010590558871626854 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0119741540402174\n",
      "Epoch 170: loss = 0.6776483058929443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010893684811890125 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011967194266617298\n",
      "Epoch 171: loss = 0.713087797164917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010822409763932228 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011952994391322136\n",
      "Epoch 172: loss = 0.6937711834907532\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010997585952281952 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011931966058909893\n",
      "Epoch 173: loss = 0.6721226572990417\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011285463348031044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011919153854250908\n",
      "Epoch 174: loss = 0.6836915612220764\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019597195088863373 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011912677437067032\n",
      "Epoch 175: loss = 0.7091752886772156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01887156069278717 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011893102899193764\n",
      "Epoch 176: loss = 0.6505658626556396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017713064327836037 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011872995644807816\n",
      "Epoch 177: loss = 0.653489351272583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016973506659269333 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011849462985992432\n",
      "Epoch 178: loss = 0.6706781387329102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016237085685133934 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011804857291281223\n",
      "Epoch 179: loss = 0.7112082242965698\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015822427347302437 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011778413318097591\n",
      "Epoch 180: loss = 0.674748957157135\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01480584405362606 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011765156872570515\n",
      "Epoch 181: loss = 0.666006863117218\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015346021391451359 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011747874319553375\n",
      "Epoch 182: loss = 0.6606523990631104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01502083707600832 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0117246825248003\n",
      "Epoch 183: loss = 0.6813806295394897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014401808381080627 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011686231009662151\n",
      "Epoch 184: loss = 0.7130258083343506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03228025138378143 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01168876327574253\n",
      "Epoch 185: loss = 0.7157150506973267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.033680543303489685 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011664431542158127\n",
      "Epoch 186: loss = 0.7569775581359863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03110034577548504 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01163447741419077\n",
      "Epoch 187: loss = 0.742366373538971\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02892015501856804 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011594780720770359\n",
      "Epoch 188: loss = 0.7593895196914673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02684883587062359 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011573308147490025\n",
      "Epoch 189: loss = 0.7302110195159912\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02559218928217888 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01153292041271925\n",
      "Epoch 190: loss = 0.7471455931663513\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.023868175223469734 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011517757549881935\n",
      "Epoch 191: loss = 0.7215988636016846\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04080992564558983 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01148448046296835\n",
      "Epoch 192: loss = 0.7525402307510376\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031463779509067535 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011477235704660416\n",
      "Epoch 193: loss = 0.7387480735778809\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02994588389992714 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011453635059297085\n",
      "Epoch 194: loss = 0.7215277552604675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.030042048543691635 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01141440961509943\n",
      "Epoch 195: loss = 0.7245346307754517\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.028128430247306824 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011404933407902718\n",
      "Epoch 196: loss = 0.7462080121040344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0281688179820776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011369513347744942\n",
      "Epoch 197: loss = 0.7655691504478455\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026752285659313202 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011360498145222664\n",
      "Epoch 198: loss = 0.7275886535644531\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02060183510184288 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01133737526834011\n",
      "Epoch 199: loss = 0.7472707033157349\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021192103624343872 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011310621164739132\n",
      "Epoch 200: loss = 0.7232319712638855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01815255917608738 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011287636123597622\n",
      "Epoch 201: loss = 0.7205911874771118\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020524030551314354 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011277293786406517\n",
      "Epoch 202: loss = 0.721676230430603\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017116626724600792 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011264877393841743\n",
      "Epoch 203: loss = 0.686695396900177\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016381360590457916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011232776567339897\n",
      "Epoch 204: loss = 0.7029637098312378\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01568756252527237 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011212886311113834\n",
      "Epoch 205: loss = 0.6606823801994324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015237400308251381 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011214391328394413\n",
      "Epoch 206: loss = 0.7037621140480042\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014618417248129845 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011176172643899918\n",
      "Epoch 207: loss = 0.6965470314025879\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014335880987346172 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011161440052092075\n",
      "Epoch 208: loss = 0.6853660941123962\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013389019295573235 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011145318858325481\n",
      "Epoch 209: loss = 0.7056739926338196\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009147228673100471 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01113087311387062\n",
      "Epoch 210: loss = 0.6637171506881714\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012592737562954426 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011116404086351395\n",
      "Epoch 211: loss = 0.6972174644470215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012097133323550224 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011090340092778206\n",
      "Epoch 212: loss = 0.70334392786026\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01120419055223465 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01107688620686531\n",
      "Epoch 213: loss = 0.6870901584625244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01044481061398983 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011048364453017712\n",
      "Epoch 214: loss = 0.6602239012718201\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010118736885488033 motion_loss: \n",
      "smoothed_heaviside_loss:  0.011023402214050293\n",
      "Epoch 215: loss = 0.6621752977371216\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009875084273517132 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01099251490086317\n",
      "Epoch 216: loss = 0.6487350463867188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008529366925358772 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010973675176501274\n",
      "Epoch 217: loss = 0.6689231991767883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006674574222415686 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010939622297883034\n",
      "Epoch 218: loss = 0.6436229348182678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012311413884162903 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010939689353108406\n",
      "Epoch 219: loss = 0.667584240436554\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006145038641989231 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010932503268122673\n",
      "Epoch 220: loss = 0.7052250504493713\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006172453984618187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010922382585704327\n",
      "Epoch 221: loss = 0.6739166975021362\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006936514284461737 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01090058870613575\n",
      "Epoch 222: loss = 0.640523374080658\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007129200734198093 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010876525193452835\n",
      "Epoch 223: loss = 0.6338702440261841\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06177522614598274 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010861903429031372\n",
      "Epoch 224: loss = 0.7133541703224182\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06441795080900192 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010848620906472206\n",
      "Epoch 225: loss = 0.7130757570266724\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07549859583377838 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010825698263943195\n",
      "Epoch 226: loss = 0.742468535900116\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07512465864419937 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010819710791110992\n",
      "Epoch 227: loss = 0.7219800353050232\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09147558361291885 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010789967142045498\n",
      "Epoch 228: loss = 0.6783009171485901\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07301664352416992 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010762207210063934\n",
      "Epoch 229: loss = 0.6578428745269775\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06537710130214691 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01073025818914175\n",
      "Epoch 230: loss = 0.6954231858253479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08215770125389099 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010720064863562584\n",
      "Epoch 231: loss = 0.6857473850250244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07003787159919739 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010706535540521145\n",
      "Epoch 232: loss = 0.7050198316574097\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06638846546411514 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010662144981324673\n",
      "Epoch 233: loss = 0.675356924533844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04111570864915848 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010643103159964085\n",
      "Epoch 234: loss = 0.6810813546180725\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04457825422286987 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010617394931614399\n",
      "Epoch 235: loss = 0.6311634182929993\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04641547054052353 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010623380541801453\n",
      "Epoch 236: loss = 0.6130324602127075\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.046595633029937744 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010585881769657135\n",
      "Epoch 237: loss = 0.6255035400390625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.035176344215869904 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01057625375688076\n",
      "Epoch 238: loss = 0.6446508169174194\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.057085514068603516 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01056284923106432\n",
      "Epoch 239: loss = 0.6418445110321045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05574699491262436 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010539975948631763\n",
      "Epoch 240: loss = 0.6550666093826294\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05280696600675583 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010500157251954079\n",
      "Epoch 241: loss = 0.6430972218513489\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07129722088575363 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010474739596247673\n",
      "Epoch 242: loss = 0.6357750296592712\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07502079010009766 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010455841198563576\n",
      "Epoch 243: loss = 0.6679330468177795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05592264235019684 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01042503584176302\n",
      "Epoch 244: loss = 0.6533530950546265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05396382883191109 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010401120409369469\n",
      "Epoch 245: loss = 0.6225447058677673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05156451463699341 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010400021448731422\n",
      "Epoch 246: loss = 0.6325989961624146\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04448337107896805 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010372032411396503\n",
      "Epoch 247: loss = 0.5785683989524841\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.038816291838884354 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010332428850233555\n",
      "Epoch 248: loss = 0.5566631555557251\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03796360641717911 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01031514722853899\n",
      "Epoch 249: loss = 0.5966416001319885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03427576273679733 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010293999686837196\n",
      "Epoch 250: loss = 0.565232515335083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03127804398536682 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010308941826224327\n",
      "Epoch 251: loss = 0.672231912612915\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.028869524598121643 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0102952616289258\n",
      "Epoch 252: loss = 0.6571735739707947\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02959534339606762 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01026963721960783\n",
      "Epoch 253: loss = 0.6351604461669922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02628777176141739 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010249857790768147\n",
      "Epoch 254: loss = 0.5927197337150574\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.025003941729664803 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010233928449451923\n",
      "Epoch 255: loss = 0.5953751802444458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02425369620323181 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010218008421361446\n",
      "Epoch 256: loss = 0.620026171207428\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022714268416166306 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010197840631008148\n",
      "Epoch 257: loss = 0.6000912189483643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021312091499567032 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010194002650678158\n",
      "Epoch 258: loss = 0.5865464210510254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02177254855632782 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01016827579587698\n",
      "Epoch 259: loss = 0.5803946852684021\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02164727821946144 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010147458873689175\n",
      "Epoch 260: loss = 0.5870465040206909\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02088078111410141 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010123935528099537\n",
      "Epoch 261: loss = 0.6092084050178528\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020414233207702637 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010126395151019096\n",
      "Epoch 262: loss = 0.5760992765426636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019594617187976837 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010105520486831665\n",
      "Epoch 263: loss = 0.5805710554122925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0186622254550457 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0100962333381176\n",
      "Epoch 264: loss = 0.6419650316238403\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01789955049753189 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010074933059513569\n",
      "Epoch 265: loss = 0.6108572483062744\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.018444791436195374 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010052638128399849\n",
      "Epoch 266: loss = 0.6176987290382385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01789121702313423 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010037615895271301\n",
      "Epoch 267: loss = 0.6014442443847656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01676863804459572 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010031701065599918\n",
      "Epoch 268: loss = 0.5963765382766724\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016228197142481804 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010016190819442272\n",
      "Epoch 269: loss = 0.5719877481460571\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015624476596713066 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010007171891629696\n",
      "Epoch 270: loss = 0.5735659003257751\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015276571735739708 motion_loss: \n",
      "smoothed_heaviside_loss:  0.010006804950535297\n",
      "Epoch 271: loss = 0.5730326771736145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0148922773078084 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009990286082029343\n",
      "Epoch 272: loss = 0.5743924379348755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014622289687395096 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009973142296075821\n",
      "Epoch 273: loss = 0.5721619725227356\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014509562402963638 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009965036995708942\n",
      "Epoch 274: loss = 0.5920955538749695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013977164402604103 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00994078628718853\n",
      "Epoch 275: loss = 0.5805721282958984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013338346034288406 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00992799736559391\n",
      "Epoch 276: loss = 0.5744179487228394\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013243877328932285 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00994139350950718\n",
      "Epoch 277: loss = 0.5761671662330627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013025565072894096 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009913330897688866\n",
      "Epoch 278: loss = 0.5826603174209595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013037169352173805 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009912410750985146\n",
      "Epoch 279: loss = 0.5992327928543091\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01374630257487297 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00988955982029438\n",
      "Epoch 280: loss = 0.5960625410079956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015864713117480278 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009874419309198856\n",
      "Epoch 281: loss = 0.6065769791603088\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015527037903666496 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00985746830701828\n",
      "Epoch 282: loss = 0.6543717384338379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012320213951170444 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009848258458077908\n",
      "Epoch 283: loss = 0.6268656849861145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011923843994736671 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009831523522734642\n",
      "Epoch 284: loss = 0.6264505386352539\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01180245727300644 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00979380402714014\n",
      "Epoch 285: loss = 0.6212533712387085\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010698607191443443 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009767834097146988\n",
      "Epoch 286: loss = 0.620838463306427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009914976544678211 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009766077622771263\n",
      "Epoch 287: loss = 0.593506932258606\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010091902688145638 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00973164290189743\n",
      "Epoch 288: loss = 0.579552948474884\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010719647631049156 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009703059680759907\n",
      "Epoch 289: loss = 0.5936793088912964\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010808318853378296 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009679616428911686\n",
      "Epoch 290: loss = 0.5994445085525513\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010414104908704758 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009670667350292206\n",
      "Epoch 291: loss = 0.5920903086662292\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01000305823981762 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009663303382694721\n",
      "Epoch 292: loss = 0.5965878367424011\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009659712202847004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00966405589133501\n",
      "Epoch 293: loss = 0.5943475961685181\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010202373377978802 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009649098850786686\n",
      "Epoch 294: loss = 0.5655781626701355\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009873644448816776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009646194986999035\n",
      "Epoch 295: loss = 0.5541160106658936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009021174162626266 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009621163830161095\n",
      "Epoch 296: loss = 0.5851238965988159\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0101275984197855 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009593607857823372\n",
      "Epoch 297: loss = 0.595043957233429\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009983758442103863 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009567742235958576\n",
      "Epoch 298: loss = 0.5698065161705017\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009880801662802696 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009540809318423271\n",
      "Epoch 299: loss = 0.5666418075561523\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009689421392977238 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009522239677608013\n",
      "Epoch 300: loss = 0.5674867630004883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00949001032859087 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009528263472020626\n",
      "Epoch 301: loss = 0.579159677028656\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009354975074529648 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009508026763796806\n",
      "Epoch 302: loss = 0.5692349076271057\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009187038987874985 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0094756456092\n",
      "Epoch 303: loss = 0.5826461911201477\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008978071622550488 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009439520537853241\n",
      "Epoch 304: loss = 0.6233667135238647\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009201956912875175 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009420948103070259\n",
      "Epoch 305: loss = 0.6302890777587891\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009297830052673817 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009410091675817966\n",
      "Epoch 306: loss = 0.584492564201355\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009307021275162697 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009401452727615833\n",
      "Epoch 307: loss = 0.566167950630188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008921224623918533 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009394058026373386\n",
      "Epoch 308: loss = 0.5582019686698914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008916838094592094 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009399774484336376\n",
      "Epoch 309: loss = 0.5799401998519897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009377703070640564 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009373334236443043\n",
      "Epoch 310: loss = 0.5759249925613403\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008999154902994633 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009347544051706791\n",
      "Epoch 311: loss = 0.610478401184082\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01263323612511158 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009331487119197845\n",
      "Epoch 312: loss = 0.5985826253890991\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012540124356746674 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009324463084340096\n",
      "Epoch 313: loss = 0.6010163426399231\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01338752917945385 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009299654513597488\n",
      "Epoch 314: loss = 0.5719050168991089\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013391368091106415 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009276572614908218\n",
      "Epoch 315: loss = 0.5945073962211609\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012995311990380287 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009261658415198326\n",
      "Epoch 316: loss = 0.5793238878250122\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011044792830944061 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009241691790521145\n",
      "Epoch 317: loss = 0.5453196167945862\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0106262918561697 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009243606589734554\n",
      "Epoch 318: loss = 0.5633926391601562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009931243024766445 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009238746017217636\n",
      "Epoch 319: loss = 0.5893481373786926\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012049494311213493 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009219841100275517\n",
      "Epoch 320: loss = 0.6372522115707397\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011866232380270958 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009205002337694168\n",
      "Epoch 321: loss = 0.6064541339874268\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010788812302052975 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009198975749313831\n",
      "Epoch 322: loss = 0.5682969689369202\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010819151066243649 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009186447598040104\n",
      "Epoch 323: loss = 0.5685387849807739\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010842186398804188 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009197868406772614\n",
      "Epoch 324: loss = 0.5927467942237854\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012960666790604591 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009192397817969322\n",
      "Epoch 325: loss = 0.6144896745681763\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013297703117132187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009178871288895607\n",
      "Epoch 326: loss = 0.626651406288147\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016675980761647224 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009160439483821392\n",
      "Epoch 327: loss = 0.6117377281188965\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017285631969571114 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009134633466601372\n",
      "Epoch 328: loss = 0.6752469539642334\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016349874436855316 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009120345115661621\n",
      "Epoch 329: loss = 0.6100395321846008\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01564164273440838 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00909388530999422\n",
      "Epoch 330: loss = 0.5908246040344238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014563788659870625 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009090505540370941\n",
      "Epoch 331: loss = 0.607183039188385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014001288451254368 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009097395464777946\n",
      "Epoch 332: loss = 0.6120280623435974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01388582494109869 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009083044715225697\n",
      "Epoch 333: loss = 0.5959583520889282\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014022971503436565 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009077193215489388\n",
      "Epoch 334: loss = 0.5978193879127502\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01368267834186554 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00907808542251587\n",
      "Epoch 335: loss = 0.625711977481842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012957878410816193 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009057960473001003\n",
      "Epoch 336: loss = 0.6277451515197754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012457474134862423 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009030244313180447\n",
      "Epoch 337: loss = 0.5983250141143799\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012137006968259811 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00902743823826313\n",
      "Epoch 338: loss = 0.5378648638725281\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012989440932869911 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009025922045111656\n",
      "Epoch 339: loss = 0.5747911930084229\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01180221512913704 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009011365473270416\n",
      "Epoch 340: loss = 0.6100034117698669\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011973030865192413 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00901721604168415\n",
      "Epoch 341: loss = 0.5979121327400208\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011286022141575813 motion_loss: \n",
      "smoothed_heaviside_loss:  0.009010675363242626\n",
      "Epoch 342: loss = 0.6253467798233032\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011481461115181446 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008985050953924656\n",
      "Epoch 343: loss = 0.6335847973823547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009819312021136284 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00896997470408678\n",
      "Epoch 344: loss = 0.5819847583770752\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.027416449040174484 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008988604880869389\n",
      "Epoch 345: loss = 0.5845189094543457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026575876399874687 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008971505798399448\n",
      "Epoch 346: loss = 0.6244868636131287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02713209204375744 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008970001712441444\n",
      "Epoch 347: loss = 0.604720413684845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022245027124881744 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008951779454946518\n",
      "Epoch 348: loss = 0.6113632321357727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016789115965366364 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008924894034862518\n",
      "Epoch 349: loss = 0.5722960233688354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016093600541353226 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008895093575119972\n",
      "Epoch 350: loss = 0.5630561709403992\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04444505646824837 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008882150053977966\n",
      "Epoch 351: loss = 0.5920612812042236\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.048343196511268616 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008868086151778698\n",
      "Epoch 352: loss = 0.6596784591674805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04135286062955856 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008846824057400227\n",
      "Epoch 353: loss = 0.6110295653343201\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03698696941137314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00884166918694973\n",
      "Epoch 354: loss = 0.6127825379371643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.034397877752780914 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00882376916706562\n",
      "Epoch 355: loss = 0.6282644271850586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03361733630299568 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008807595819234848\n",
      "Epoch 356: loss = 0.6500636339187622\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031030194833874702 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008802670054137707\n",
      "Epoch 357: loss = 0.5625147223472595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.029395654797554016 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008779949508607388\n",
      "Epoch 358: loss = 0.5811154246330261\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.027525603771209717 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008757028728723526\n",
      "Epoch 359: loss = 0.616855800151825\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02564731054008007 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008730043657124043\n",
      "Epoch 360: loss = 0.5958709120750427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02449752949178219 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008730397559702396\n",
      "Epoch 361: loss = 0.5917626619338989\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02372552454471588 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008716014213860035\n",
      "Epoch 362: loss = 0.5679133534431458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02346980571746826 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008688366040587425\n",
      "Epoch 363: loss = 0.5970433354377747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020540669560432434 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008660617284476757\n",
      "Epoch 364: loss = 0.5779227614402771\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0206083282828331 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008658948354423046\n",
      "Epoch 365: loss = 0.6069368720054626\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019595662131905556 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008642317727208138\n",
      "Epoch 366: loss = 0.5781667232513428\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014582565985620022 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008636224083602428\n",
      "Epoch 367: loss = 0.5619449019432068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015182124450802803 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008647294715046883\n",
      "Epoch 368: loss = 0.5565061569213867\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017140472307801247 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008634721860289574\n",
      "Epoch 369: loss = 0.5264053344726562\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01383092999458313 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008632111363112926\n",
      "Epoch 370: loss = 0.5091295838356018\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01333317719399929 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008617290295660496\n",
      "Epoch 371: loss = 0.543453574180603\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013595093041658401 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008606130257248878\n",
      "Epoch 372: loss = 0.5423380136489868\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.013054721988737583 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008583765476942062\n",
      "Epoch 373: loss = 0.5641310214996338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017691051587462425 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008570432662963867\n",
      "Epoch 374: loss = 0.5930428504943848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017449533566832542 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008555233478546143\n",
      "Epoch 375: loss = 0.5935795903205872\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017456483095884323 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008549279533326626\n",
      "Epoch 376: loss = 0.572486937046051\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015907172113656998 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008541540242731571\n",
      "Epoch 377: loss = 0.5690523386001587\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015777412801980972 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008529022336006165\n",
      "Epoch 378: loss = 0.5503630042076111\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.018009724095463753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008553494699299335\n",
      "Epoch 379: loss = 0.5595752596855164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015646813437342644 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008538736961781979\n",
      "Epoch 380: loss = 0.5391973257064819\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017679885029792786 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008526988327503204\n",
      "Epoch 381: loss = 0.5914541482925415\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.018090758472681046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008514279499650002\n",
      "Epoch 382: loss = 0.5913942456245422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017415059730410576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008493700064718723\n",
      "Epoch 383: loss = 0.5783965587615967\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022349990904331207 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008476149290800095\n",
      "Epoch 384: loss = 0.6305812001228333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0346667543053627 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008461548946797848\n",
      "Epoch 385: loss = 0.6505679488182068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.032735083252191544 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008446002379059792\n",
      "Epoch 386: loss = 0.6069178581237793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031655583530664444 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008429828099906445\n",
      "Epoch 387: loss = 0.6010047197341919\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031228644773364067 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00840746983885765\n",
      "Epoch 388: loss = 0.5693763494491577\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.029911741614341736 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008404664695262909\n",
      "Epoch 389: loss = 0.5518050789833069\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.029953492805361748 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008394304662942886\n",
      "Epoch 390: loss = 0.5395697951316833\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.028696350753307343 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008370542898774147\n",
      "Epoch 391: loss = 0.559523344039917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.027702845633029938 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00836397148668766\n",
      "Epoch 392: loss = 0.5476416349411011\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02758544124662876 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008339297957718372\n",
      "Epoch 393: loss = 0.551739513874054\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026098914444446564 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008342948742210865\n",
      "Epoch 394: loss = 0.5289204120635986\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.024482274428009987 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008332333527505398\n",
      "Epoch 395: loss = 0.5534362196922302\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022768940776586533 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008351939730346203\n",
      "Epoch 396: loss = 0.5729283690452576\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021243054419755936 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008339855819940567\n",
      "Epoch 397: loss = 0.5498907566070557\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020488446578383446 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008345463313162327\n",
      "Epoch 398: loss = 0.5521005392074585\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02275601215660572 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00835353136062622\n",
      "Epoch 399: loss = 0.5462256073951721\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022082991898059845 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00833821576088667\n",
      "Epoch 400: loss = 0.5274466872215271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011695701628923416 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00834661815315485\n",
      "Epoch 401: loss = 0.5511754751205444\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007906777784228325 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008339734748005867\n",
      "Epoch 402: loss = 0.5430600047111511\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007714012172073126 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008327682502567768\n",
      "Epoch 403: loss = 0.49413347244262695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0076000322587788105 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008318965323269367\n",
      "Epoch 404: loss = 0.5279686450958252\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009188173338770866 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008327354677021503\n",
      "Epoch 405: loss = 0.521020770072937\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008702288381755352 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00831128004938364\n",
      "Epoch 406: loss = 0.5456646084785461\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008232831954956055 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008288449607789516\n",
      "Epoch 407: loss = 0.5495674014091492\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008164533413946629 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008278537541627884\n",
      "Epoch 408: loss = 0.5979987978935242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006501935888081789 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008262643590569496\n",
      "Epoch 409: loss = 0.5132662653923035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006690789945423603 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008247610181570053\n",
      "Epoch 410: loss = 0.5466843247413635\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007711694575846195 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008225572295486927\n",
      "Epoch 411: loss = 0.5717596411705017\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007825501263141632 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008234329521656036\n",
      "Epoch 412: loss = 0.5760544538497925\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007863773964345455 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008229588158428669\n",
      "Epoch 413: loss = 0.5625745058059692\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00879422016441822 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008219580166041851\n",
      "Epoch 414: loss = 0.5418042540550232\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008738518692553043 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008200284093618393\n",
      "Epoch 415: loss = 0.5680708885192871\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008876629173755646 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008194483816623688\n",
      "Epoch 416: loss = 0.6165045499801636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0070397560484707355 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008176029659807682\n",
      "Epoch 417: loss = 0.6003108620643616\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006351140793412924 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008157058618962765\n",
      "Epoch 418: loss = 0.5814550518989563\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0058120074681937695 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008133599534630775\n",
      "Epoch 419: loss = 0.6061724424362183\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005676897242665291 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008128603920340538\n",
      "Epoch 420: loss = 0.5979759693145752\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005474993959069252 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008120207116007805\n",
      "Epoch 421: loss = 0.6252971887588501\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005216134712100029 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008100377395749092\n",
      "Epoch 422: loss = 0.6061558127403259\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0051058996468782425 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008097006939351559\n",
      "Epoch 423: loss = 0.6021611094474792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0049059768207371235 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008085382170975208\n",
      "Epoch 424: loss = 0.6110101342201233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00496412580832839 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008060364052653313\n",
      "Epoch 425: loss = 0.6002424359321594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006151371635496616 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008063459768891335\n",
      "Epoch 426: loss = 0.5867518186569214\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006376833654940128 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008089794777333736\n",
      "Epoch 427: loss = 0.5494539141654968\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007268535438925028 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008102625608444214\n",
      "Epoch 428: loss = 0.5456035137176514\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006919918581843376 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008105816319584846\n",
      "Epoch 429: loss = 0.5905053615570068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006523818708956242 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008106111548841\n",
      "Epoch 430: loss = 0.6063958406448364\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006063494831323624 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008091271854937077\n",
      "Epoch 431: loss = 0.6104448437690735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0057895430363714695 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008079794235527515\n",
      "Epoch 432: loss = 0.5782540440559387\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005657379049807787 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008056567050516605\n",
      "Epoch 433: loss = 0.5852209329605103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005416641943156719 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008050681091845036\n",
      "Epoch 434: loss = 0.6630102396011353\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005110194906592369 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008040687069296837\n",
      "Epoch 435: loss = 0.5888425707817078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004751869477331638 motion_loss: \n",
      "smoothed_heaviside_loss:  0.008030652068555355\n",
      "Epoch 436: loss = 0.5402875542640686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004505263175815344 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00801155623048544\n",
      "Epoch 437: loss = 0.5907381176948547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004986978601664305 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00798702985048294\n",
      "Epoch 438: loss = 0.5691920518875122\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0049047768115997314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007991901598870754\n",
      "Epoch 439: loss = 0.5864233374595642\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004739223048090935 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007989351637661457\n",
      "Epoch 440: loss = 0.6089035868644714\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004615785554051399 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007975462824106216\n",
      "Epoch 441: loss = 0.5635295510292053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004484057426452637 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007966500706970692\n",
      "Epoch 442: loss = 0.5907180905342102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004345923196524382 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007937487214803696\n",
      "Epoch 443: loss = 0.5474817156791687\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004242648370563984 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00792099628597498\n",
      "Epoch 444: loss = 0.5871537923812866\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004100121092051268 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007920986972749233\n",
      "Epoch 445: loss = 0.5847543478012085\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003987831994891167 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007922878488898277\n",
      "Epoch 446: loss = 0.5375121235847473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003919415641576052 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007915182039141655\n",
      "Epoch 447: loss = 0.5292713642120361\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0038348506204783916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007917216047644615\n",
      "Epoch 448: loss = 0.5403472781181335\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003860367927700281 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007916675880551338\n",
      "Epoch 449: loss = 0.5641037225723267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0037985011003911495 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007902324199676514\n",
      "Epoch 450: loss = 0.5617223978042603\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00402034493163228 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007885683327913284\n",
      "Epoch 451: loss = 0.5426227450370789\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0036914364900439978 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007870284840464592\n",
      "Epoch 452: loss = 0.5171421766281128\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003637429093942046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007857752963900566\n",
      "Epoch 453: loss = 0.5385417342185974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00391424959525466 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007868695072829723\n",
      "Epoch 454: loss = 0.5265705585479736\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0038164937868714333 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007859308272600174\n",
      "Epoch 455: loss = 0.5014516115188599\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0037379427812993526 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007833272218704224\n",
      "Epoch 456: loss = 0.5228055119514465\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0036867959424853325 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007820483297109604\n",
      "Epoch 457: loss = 0.5173815488815308\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0036242385394871235 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007825042121112347\n",
      "Epoch 458: loss = 0.5201073884963989\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003598130075260997 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007825784385204315\n",
      "Epoch 459: loss = 0.5226780772209167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0035066998098045588 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00783167127519846\n",
      "Epoch 460: loss = 0.5152637362480164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0032012027222663164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007834061980247498\n",
      "Epoch 461: loss = 0.6043939590454102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0031830554362386465 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007827059365808964\n",
      "Epoch 462: loss = 0.5130614042282104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003102777525782585 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007811781018972397\n",
      "Epoch 463: loss = 0.5218245983123779\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030718708876520395 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007816368713974953\n",
      "Epoch 464: loss = 0.5122327208518982\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003044215962290764 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007834079675376415\n",
      "Epoch 465: loss = 0.48250722885131836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003025817684829235 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007822534069418907\n",
      "Epoch 466: loss = 0.48240581154823303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030092019587755203 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007806309964507818\n",
      "Epoch 467: loss = 0.5190614461898804\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002951189875602722 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007784135174006224\n",
      "Epoch 468: loss = 0.5136551856994629\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002974356524646282 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007772330194711685\n",
      "Epoch 469: loss = 0.5195993781089783\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030496602412313223 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0077708750031888485\n",
      "Epoch 470: loss = 0.5345955491065979\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030383868142962456 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007760361768305302\n",
      "Epoch 471: loss = 0.5334877967834473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030204993672668934 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007753363810479641\n",
      "Epoch 472: loss = 0.5582857728004456\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002939938334748149 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007746492512524128\n",
      "Epoch 473: loss = 0.5434792637825012\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002907081739977002 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007728680036962032\n",
      "Epoch 474: loss = 0.5390977263450623\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002802200149744749 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007709125988185406\n",
      "Epoch 475: loss = 0.5355589389801025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002729464787989855 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007692620623856783\n",
      "Epoch 476: loss = 0.5315951108932495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026339225005358458 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007684937212616205\n",
      "Epoch 477: loss = 0.5791727900505066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026662785094231367 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007678181864321232\n",
      "Epoch 478: loss = 0.5534915328025818\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002636607736349106 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007684882264584303\n",
      "Epoch 479: loss = 0.5702226161956787\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026416326873004436 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0076714130118489265\n",
      "Epoch 480: loss = 0.5451716780662537\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002675962168723345 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007649701554328203\n",
      "Epoch 481: loss = 0.5455536842346191\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026700771413743496 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0076342555694282055\n",
      "Epoch 482: loss = 0.5389627814292908\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026530351024121046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007666546385735273\n",
      "Epoch 483: loss = 0.5593452453613281\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026760478504002094 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007643769029527903\n",
      "Epoch 484: loss = 0.5236184597015381\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0026609329506754875 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0076374635100364685\n",
      "Epoch 485: loss = 0.5138949155807495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0029014735482633114 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007617710158228874\n",
      "Epoch 486: loss = 0.5399632453918457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0025674747303128242 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007609919644892216\n",
      "Epoch 487: loss = 0.5211275815963745\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024782682303339243 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007609556429088116\n",
      "Epoch 488: loss = 0.536916971206665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002467954996973276 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007603098638355732\n",
      "Epoch 489: loss = 0.5402297377586365\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002357564400881529 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007594494614750147\n",
      "Epoch 490: loss = 0.5287078022956848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00229963893070817 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007582623977214098\n",
      "Epoch 491: loss = 0.5626960396766663\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00240046507678926 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0075653670355677605\n",
      "Epoch 492: loss = 0.5349721312522888\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0023413056042045355 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00756154116243124\n",
      "Epoch 493: loss = 0.5366633534431458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019409745000302792 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007558648008853197\n",
      "Epoch 494: loss = 0.5193436741828918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019136258633807302 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007544524967670441\n",
      "Epoch 495: loss = 0.5170292258262634\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019160653464496136 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0075703575275838375\n",
      "Epoch 496: loss = 0.5347527265548706\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018934825202450156 motion_loss: \n",
      "smoothed_heaviside_loss:  0.00755658745765686\n",
      "Epoch 497: loss = 0.5323983430862427\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018732219468802214 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007556996773928404\n",
      "Epoch 498: loss = 0.5383272767066956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018184281652793288 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007552058435976505\n",
      "Epoch 499: loss = 0.530756413936615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001792578725144267 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007536009885370731\n",
      "Epoch 500: loss = 0.5098908543586731\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017781525384634733 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007513001095503569\n",
      "Epoch 501: loss = 0.5150898098945618\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017650292720645666 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007519013714045286\n",
      "Epoch 502: loss = 0.4975545108318329\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017418036004528403 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007511687930673361\n",
      "Epoch 503: loss = 0.5244051218032837\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018415122758597136 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007486678194254637\n",
      "Epoch 504: loss = 0.5345078110694885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001798375160433352 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007461623288691044\n",
      "Epoch 505: loss = 0.5493766665458679\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017789191333577037 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0074556125327944756\n",
      "Epoch 506: loss = 0.5518963932991028\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018138381419703364 motion_loss: \n",
      "smoothed_heaviside_loss:  0.007454941049218178\n",
      "Epoch 507: loss = 0.5476292371749878\n",
      "-----------------\n",
      "Learning rate:  0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m      7\u001b[0m     sites \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(sites)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# import cProfile, pstats\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# #\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     sites, optimized_sites_sdf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_DCCVT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_weights\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     sites_np \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(site_file_path, sites_np)\n",
      "Cell \u001b[0;32mIn[6], line 59\u001b[0m, in \u001b[0;36mtrain_DCCVT\u001b[0;34m(sites, sites_sdf, max_iter, stop_train_threshold, upsampling, lambda_weights, voroloss_optim)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m voroloss_optim:\n\u001b[1;32m     58\u001b[0m     sites_np \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 59\u001b[0m     d3dsimplices \u001b[38;5;241m=\u001b[39m \u001b[43mdiffvoronoi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_delaunay_simplices\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msites_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     d3dsimplices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(d3dsimplices)\n\u001b[1;32m     61\u001b[0m     cvt_loss \u001b[38;5;241m=\u001b[39m lf\u001b[38;5;241m.\u001b[39mcompute_cvt_loss_vectorized_delaunay(sites, \u001b[38;5;28;01mNone\u001b[39;00m, d3dsimplices)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # #\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([32768])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/bunny1000_500_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Computing Delaunay simplices...\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites, sdf = train_DCCVT(\n",
    "#     sites, sdf_v, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=True\n",
    "# )\n",
    "# (\n",
    "#     v_vect,\n",
    "#     f_vect,\n",
    "#     _,\n",
    "#     _,\n",
    "#     _,\n",
    "# ) = su.get_clipped_mesh_numba(sites, None, None, False, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5067aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer Accuracy (Ours → GT): 0.008214\n",
      "Chamfer Completeness (GT → Ours): 0.019737\n",
      "Chamfer Distance (symmetric): 0.027951\n"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT → Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours → GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours → GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT → Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "\n",
    "    # Normalize mesh (centered and scaled uniformly)\n",
    "    bbox = mesh.bounds\n",
    "    center = mesh.centroid\n",
    "    scale = np.linalg.norm(bbox[1] - bbox[0])\n",
    "    mesh.apply_translation(-center)\n",
    "    mesh.apply_scale(1.0 / scale)\n",
    "\n",
    "    # Export normalized mesh\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "_, _ = sample_points_on_mesh(\n",
    "    \"/home/wylliam/dev/Kyushu_experiments/outputs/gargoyle_unconverged/cdp1000_v0_cvt100_clipTrue_buildFalse_upsampling0_num_centroids32_target_size32_final.obj\",\n",
    "    n_points=100000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
