{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "torch.manual_seed(69)\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "# lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT_interpolSDF/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"sphere\"]\n",
    "\n",
    "# mesh = [\"gargoyle\", \"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "mesh = [\"gargoyle_unconverged\", \"/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model_2000.pth\"\n",
    "\n",
    "#\n",
    "# mesh = [\"chair\", \"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"bunny\", \"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([4096, 3])\n",
      "Sites:  tensor([-1.0027, -1.0065, -0.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 575.64.03\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 16**3\n",
    "grid = 32  # 128\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.005\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids ** (1 / 3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "# add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "print(\"Sites: \", sites[0])\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n",
      "torch.float32\n",
      "torch.Size([4096, 3])\n",
      "Allocated: 63.913472 MB, Reserved: 65.011712 MB\n",
      "torch.Size([4096])\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WITH HOTSPOT\n",
    "\n",
    "import sys\n",
    "\n",
    "if mesh[0] != \"sphere\":\n",
    "    sys.path.append(\"3rdparty/HotSpot\")\n",
    "    from dataset import shape_3d\n",
    "    import models.Net as Net\n",
    "\n",
    "    loss_type = \"igr_w_heat\"\n",
    "    loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "    train_set = shape_3d.ReconDataset(\n",
    "        file_path=mesh[1] + \".ply\",\n",
    "        n_points=grid * grid * 150,  # 15000, #args.n_points,\n",
    "        n_samples=10001,  # args.n_iterations,\n",
    "        grid_res=256,  # args.grid_res,\n",
    "        grid_range=1.1,  # args.grid_range,\n",
    "        sample_type=\"uniform_central_gaussian\",  # args.nonmnfld_sample_type,\n",
    "        sampling_std=0.5,  # args.nonmnfld_sample_std,\n",
    "        n_random_samples=7500,  # args.n_random_samples,\n",
    "        resample=True,\n",
    "        compute_sal_dist_gt=(True if \"sal\" in loss_type and loss_weights[5] > 0 else False),\n",
    "        scale_method=\"mean\",  # \"mean\" #args.pcd_scale_method,\n",
    "    )\n",
    "\n",
    "    model = Net.Network(\n",
    "        latent_size=0,  # args.latent_size,\n",
    "        in_dim=3,\n",
    "        decoder_hidden_dim=128,  # args.decoder_hidden_dim,\n",
    "        nl=\"sine\",  # args.nl,\n",
    "        encoder_type=\"none\",  # args.encoder_type,\n",
    "        decoder_n_hidden_layers=5,  # args.decoder_n_hidden_layers,\n",
    "        neuron_type=\"quadratic\",  # args.neuron_type,\n",
    "        init_type=\"mfgi\",  # args.init_type,\n",
    "        sphere_init_params=[1.6, 0.1],  # args.sphere_init_params,\n",
    "        n_repeat_period=30,  # args.n_repeat_period,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ######\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_data = next(iter(test_dataloader))\n",
    "    mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "\n",
    "    # add noise to mnfld_points\n",
    "    # mnfld_points += torch.randn_like(mnfld_points) * noise_scale * 2\n",
    "\n",
    "    mnfld_points.requires_grad_()\n",
    "    print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "    if torch.cuda.is_available():\n",
    "        map_location = torch.device(\"cuda\")\n",
    "    else:\n",
    "        map_location = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))\n",
    "    sdf0 = model(sites)\n",
    "\n",
    "else:\n",
    "\n",
    "    def sphere_sdf(points: torch.Tensor, center: torch.Tensor, radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the SDF of a sphere at given 3D points.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3) tensor of 3D query points\n",
    "            center: (3,) tensor specifying the center of the sphere\n",
    "            radius: float, radius of the sphere\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,) tensor of signed distances\n",
    "        \"\"\"\n",
    "        return torch.norm(points - center, dim=-1) - radius\n",
    "\n",
    "    def sphere_sdf_with_noise(\n",
    "        points: torch.Tensor, center: torch.Tensor, radius: float, noise_amplitude=0.05\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sphere SDF with smooth directional noise added near the surface.\n",
    "\n",
    "        Args:\n",
    "            points: (N, 3)\n",
    "            center: (3,)\n",
    "            radius: float\n",
    "            noise_amplitude: float\n",
    "\n",
    "        Returns:\n",
    "            sdf: (N,)\n",
    "        \"\"\"\n",
    "        rel = points - center\n",
    "        norm = torch.norm(rel, dim=-1)  # (N,)\n",
    "        base_sdf = norm - radius  # (N,)\n",
    "\n",
    "        # Smooth periodic noise based on direction\n",
    "        unit_dir = rel / (norm.unsqueeze(-1) + 1e-9)  # (N,3)\n",
    "        noise = torch.sin(10 * unit_dir[:, 0]) * torch.sin(10 * unit_dir[:, 1]) * torch.sin(10 * unit_dir[:, 2])\n",
    "\n",
    "        # Weight noise so it mostly affects surface area\n",
    "        falloff = torch.exp(-20 * (base_sdf**2))  # (N,) ~1 near surface, ~0 far\n",
    "        sdf = base_sdf + noise_amplitude * noise * falloff\n",
    "\n",
    "        return sdf\n",
    "\n",
    "    # generate points on the sphere\n",
    "    mnfld_points = torch.randn(grid * grid * 150, 3, device=device)\n",
    "    mnfld_points = mnfld_points / torch.norm(mnfld_points, dim=-1, keepdim=True) * 0.5\n",
    "    mnfld_points = mnfld_points.unsqueeze(0).requires_grad_()\n",
    "    # sdf0 = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "    sdf0 = sphere_sdf_with_noise(sites, torch.zeros(3).to(device), 0.50, noise_amplitude=0.1)\n",
    "\n",
    "##add mnfld points with random noise to sites\n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = 24**3 - (num_centroids)\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(sites.shape)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "sdf0 = sdf0.detach().squeeze(-1).requires_grad_()\n",
    "print(sdf0.shape)\n",
    "print(sdf0.is_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites shape:  torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "sites_np = sites.detach().cpu().numpy()\n",
    "d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "print(\"sites shape: \", sites.shape)\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\", sites.detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf0.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.00005, 0.00005),\n",
    ")\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\", mnfld_points.squeeze(0).detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(sites, None, d3dsimplices, False, sdf0, True)\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf unclipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_vert = ps.register_point_cloud(\"sdf unclipped initial verts\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "v_vect, f_vect, sdf_verts, sdf_verts_grads, tet_probs = su.get_clipped_mesh_numba(\n",
    "    sites, None, d3dsimplices, True, sdf0, True\n",
    ")\n",
    "ps_mesh = ps.register_surface_mesh(\n",
    "    \"sdf clipped initial mesh\",\n",
    "    v_vect.detach().cpu().numpy(),\n",
    "    f_vect,\n",
    "    back_face_policy=\"identical\",\n",
    ")\n",
    "ps_cloud = ps.register_point_cloud(\"active sites\", tet_probs[2].reshape(-1, 3).detach().cpu().numpy(), enabled=False)\n",
    "ps_cloud.add_vector_quantity(\"site step dir\", tet_probs[0].reshape(-1, 3).detach().cpu().numpy())\n",
    "# ps_vert.add_vector_quantity(\"verts step dir\", tet_probs[1].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "\n",
    "\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "voroloss = lf.Voroloss_opt().to(device)\n",
    "\n",
    "\n",
    "def train_DCCVT(\n",
    "    sites,\n",
    "    sites_sdf,\n",
    "    max_iter=100,\n",
    "    stop_train_threshold=1e-6,\n",
    "    upsampling=0,\n",
    "    lambda_weights=[0.1, 1.0, 0.1, 0.1, 1.0, 1.0, 0.1],\n",
    "    voroloss_optim=False,\n",
    "):\n",
    "    if not voroloss_optim:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{\"params\": [sites], \"lr\": lr_sites * 0.1}])\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.0)\n",
    "\n",
    "    # optimizer_sites = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "    # optimizer_sdf = torch.optim.SGD([{'params': [sites_sdf], 'lr': lr_sites}])\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "\n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # if mesh[0] == \"sphere\":\n",
    "        #     # generate sphere sdf\n",
    "        #     sites_sdf = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "        if not voroloss_optim:\n",
    "            sites_np = sites.detach().cpu().numpy()\n",
    "            d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims * sites_np.shape[0]))\n",
    "            d3dsimplices = np.array(d3dsimplices)\n",
    "            cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "\n",
    "            build_mesh = False\n",
    "            clip = True\n",
    "\n",
    "            v_vect, f_vect, sdf_verts, sdf_verts_grads, _ = su.get_clipped_mesh_numba(\n",
    "                sites, None, d3dsimplices, clip, sites_sdf, build_mesh\n",
    "            )\n",
    "\n",
    "            if build_mesh:\n",
    "                triangle_faces = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "                triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "                hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=mnfld_points.shape[0])\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "            else:\n",
    "                chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), v_vect.unsqueeze(0))\n",
    "\n",
    "            sites_loss = lambda_cvt / 10 * cvt_loss + lambda_chamfer * chamfer_loss_mesh\n",
    "\n",
    "            sites_sdf_grads = su.sdf_space_grad_pytorch_diego(\n",
    "                sites, sites_sdf, torch.tensor(d3dsimplices).to(device).detach()\n",
    "            )\n",
    "            eik_loss = lambda_cvt / 10 * lf.discrete_tet_volume_eikonal_loss(sites, sites_sdf_grads, d3dsimplices)\n",
    "            print(\"eikonal_loss: \", eik_loss.item(), \"motion_loss: \")\n",
    "            shl = lambda_cvt / 0.1 * lf.smoothed_heaviside_loss(sites, sites_sdf, sites_sdf_grads, d3dsimplices)\n",
    "            print(\"smoothed_heaviside_loss: \", shl.item())\n",
    "            sdf_loss = eik_loss + shl\n",
    "        else:\n",
    "            sites_loss = lambda_chamfer * voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "\n",
    "        loss = sites_loss + sdf_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "        # print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        loss.backward()\n",
    "        # print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(sites_sdf, 1.0)\n",
    "        # torch.nn.utils.clip_grad_norm_(sites, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # sites_sdf += (sites_sdf_grads*(sites-sites_positions)).sum(dim=1)\n",
    "\n",
    "        # scheduler.step()\n",
    "        print(\"Learning rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "\n",
    "        if upsampled < upsampling and epoch / (max_iter * 0.80) > upsampled / upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \", len(sites))\n",
    "            if len(sites) * 1.09 > grid**3:\n",
    "                print(\"Skipping upsampling, too many sites, sites length: \", len(sites), \"grid size: \", grid**3)\n",
    "                upsampled = upsampling\n",
    "                sites = sites.detach().requires_grad_(True)\n",
    "                sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    [\n",
    "                        {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                        {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                    ]\n",
    "                )\n",
    "                # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "                continue\n",
    "            # sites, sites_sdf = su.upsampling_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            # sites, sites_sdf = su.upsampling_curvature_vectorized_sites_sites_sdf(sites, tri=None, vor=None, simplices=d3dsimplices, model=sites_sdf)\n",
    "            sites, sites_sdf = su.upsampling_adaptive_vectorized_sites_sites_sdf(\n",
    "                sites, simplices=d3dsimplices, model=sites_sdf\n",
    "            )\n",
    "\n",
    "            # sites, sites_sdf = su.upsampling_chamfer_vectorized_sites_sites_sdf(\n",
    "            #     sites, d3dsimplices, sites_sdf, mnfld_points\n",
    "            # )\n",
    "\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            sites_sdf = sites_sdf.detach().requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                [\n",
    "                    {\"params\": [sites], \"lr\": lr_sites * 0.1},\n",
    "                    {\"params\": [sites_sdf], \"lr\": lr_sites * 0.1},\n",
    "                ]\n",
    "            )\n",
    "            # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "            upsampled += 1.0\n",
    "            print(\"sites shape AFTER: \", sites.shape)\n",
    "            print(\"sites sdf shape AFTER: \", sites_sdf.shape)\n",
    "\n",
    "        if epoch % (max_iter / 10) == 0 or epoch == max_iter:\n",
    "            # print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            # print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            # save model and sites\n",
    "            # ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "\n",
    "            # ps.register_point_cloud('sampled points end', hs_p.detach().cpu().numpy())\n",
    "            # ps.register_point_cloud(\"sampled points end\", v_vect.detach().cpu().numpy(), enabled=False)\n",
    "\n",
    "            # if f_vect is not None:\n",
    "            #     ps_mesh = ps.register_surface_mesh(\n",
    "            #         f\"{epoch} sdf clipped pmesh\",\n",
    "            #         v_vect.detach().cpu().numpy(),\n",
    "            #         f_vect,\n",
    "            #         back_face_policy=\"identical\",\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "            #     ps_mesh.add_vector_quantity(\n",
    "            #         f\"{epoch} sdf verts grads\",\n",
    "            #         sdf_verts_grads.detach().cpu().numpy(),\n",
    "            #         enabled=False,\n",
    "            #     )\n",
    "\n",
    "            site_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            # model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            sdf_file_path = (\n",
    "                f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "            )\n",
    "            torch.save(sites_sdf, sdf_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return sites, sites_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "# lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100, 0, 0, 0, 1000, 0, 100, 0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eikonal_loss:  2.1532649993896484 motion_loss: \n",
      "smoothed_heaviside_loss:  0.8108399510383606\n",
      "Epoch 0: loss = 12.363930702209473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  2.7300760746002197 motion_loss: \n",
      "smoothed_heaviside_loss:  0.8167316913604736\n",
      "Epoch 1: loss = 12.310060501098633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  4096\n",
      "tensor(0.1248, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1498, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1098, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 300\n",
      "Before upsampling, number of sites: 4096 amount added: 1200\n",
      "sites shape AFTER:  torch.Size([5296, 3])\n",
      "sites sdf shape AFTER:  torch.Size([5296])\n",
      "eikonal_loss:  1.979844093322754 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22219528257846832\n",
      "Epoch 2: loss = 8.368945121765137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  1.5269033908843994 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22225923836231232\n",
      "Epoch 3: loss = 7.49868106842041\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  1.4250121116638184 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22305135428905487\n",
      "Epoch 4: loss = 7.562898635864258\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  1.6813395023345947 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22316080331802368\n",
      "Epoch 5: loss = 7.016665458679199\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  1.3025808334350586 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22385983169078827\n",
      "Epoch 6: loss = 7.032307147979736\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5648305416107178 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2241389900445938\n",
      "Epoch 7: loss = 5.944117069244385\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.36537501215934753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22521774470806122\n",
      "Epoch 8: loss = 5.85061502456665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.27360451221466064 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22646699845790863\n",
      "Epoch 9: loss = 5.5967864990234375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.16206024587154388 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22739802300930023\n",
      "Epoch 10: loss = 6.029897689819336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1538427770137787 motion_loss: \n",
      "smoothed_heaviside_loss:  0.22884872555732727\n",
      "Epoch 11: loss = 5.911341667175293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1475466936826706 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2297590672969818\n",
      "Epoch 12: loss = 5.698555946350098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.12118060141801834 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23124909400939941\n",
      "Epoch 13: loss = 5.623691558837891\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.14119265973567963 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23208045959472656\n",
      "Epoch 14: loss = 5.419327735900879\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1284235268831253 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23300522565841675\n",
      "Epoch 15: loss = 5.367281436920166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.13219618797302246 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2345527857542038\n",
      "Epoch 16: loss = 5.115696907043457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.1212492287158966 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23527169227600098\n",
      "Epoch 17: loss = 5.114360809326172\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10907846689224243 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23619844019412994\n",
      "Epoch 18: loss = 4.925989151000977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.11293759942054749 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23800933361053467\n",
      "Epoch 19: loss = 5.0149736404418945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10974209010601044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23948542773723602\n",
      "Epoch 20: loss = 4.657858848571777\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.101725272834301 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24010194838047028\n",
      "Epoch 21: loss = 4.72014856338501\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06479022651910782 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23997843265533447\n",
      "Epoch 22: loss = 4.619284629821777\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05210358276963234 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24047093093395233\n",
      "Epoch 23: loss = 4.348024368286133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05081724375486374 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24167044460773468\n",
      "Epoch 24: loss = 4.256598472595215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04857413470745087 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24239061772823334\n",
      "Epoch 25: loss = 4.389859676361084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04490501433610916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24233561754226685\n",
      "Epoch 26: loss = 4.657279014587402\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0433960035443306 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24368590116500854\n",
      "Epoch 27: loss = 4.490522861480713\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0377696231007576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2444341629743576\n",
      "Epoch 28: loss = 4.560677528381348\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.036363959312438965 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24551938474178314\n",
      "Epoch 29: loss = 4.2354736328125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03536762297153473 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24556124210357666\n",
      "Epoch 30: loss = 4.331998825073242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03393043577671051 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24617832899093628\n",
      "Epoch 31: loss = 4.282529354095459\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03236688673496246 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24591252207756042\n",
      "Epoch 32: loss = 4.0918049812316895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.033407069742679596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24592521786689758\n",
      "Epoch 33: loss = 4.115610122680664\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.032545581459999084 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24569281935691833\n",
      "Epoch 34: loss = 4.125240325927734\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03152800351381302 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24566493928432465\n",
      "Epoch 35: loss = 3.9322094917297363\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.030621666461229324 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24539534747600555\n",
      "Epoch 36: loss = 3.896207809448242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04341442883014679 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24518010020256042\n",
      "Epoch 37: loss = 4.0640950202941895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04187050834298134 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24549077451229095\n",
      "Epoch 38: loss = 3.9323782920837402\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04185561090707779 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2460218369960785\n",
      "Epoch 39: loss = 3.8684043884277344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04194512590765953 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24598228931427002\n",
      "Epoch 40: loss = 3.946614980697632\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04140498489141464 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24587193131446838\n",
      "Epoch 41: loss = 3.8761227130889893\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0404449999332428 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2454327791929245\n",
      "Epoch 42: loss = 3.6797258853912354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039554446935653687 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2460550218820572\n",
      "Epoch 43: loss = 3.540541887283325\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03878866136074066 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24592900276184082\n",
      "Epoch 44: loss = 3.6236414909362793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03798116371035576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24538807570934296\n",
      "Epoch 45: loss = 3.691025733947754\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03745383396744728 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2451283186674118\n",
      "Epoch 46: loss = 3.486705780029297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.15740835666656494 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24535024166107178\n",
      "Epoch 47: loss = 3.871657371520996\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03819120302796364 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24524694681167603\n",
      "Epoch 48: loss = 3.6459641456604004\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.026896866038441658 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24563062191009521\n",
      "Epoch 49: loss = 3.68380069732666\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.027719110250473022 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24575279653072357\n",
      "Epoch 50: loss = 3.935734272003174\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03881299868226051 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24601520597934723\n",
      "Epoch 51: loss = 3.9108707904815674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03876221179962158 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24571429193019867\n",
      "Epoch 52: loss = 3.983536958694458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.038214776664972305 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2449004352092743\n",
      "Epoch 53: loss = 4.1100335121154785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03714402765035629 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24475276470184326\n",
      "Epoch 54: loss = 4.084348678588867\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03633619472384453 motion_loss: \n",
      "smoothed_heaviside_loss:  0.245244562625885\n",
      "Epoch 55: loss = 3.95306658744812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.036350809037685394 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24480833113193512\n",
      "Epoch 56: loss = 4.0610857009887695\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.035546205937862396 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24475573003292084\n",
      "Epoch 57: loss = 3.9671096801757812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.034392502158880234 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2445869892835617\n",
      "Epoch 58: loss = 3.8861420154571533\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03430099040269852 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24365048110485077\n",
      "Epoch 59: loss = 3.985936164855957\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.034000199288129807 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24364937841892242\n",
      "Epoch 60: loss = 4.08519172668457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03489380329847336 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24361799657344818\n",
      "Epoch 61: loss = 4.240218639373779\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03468792513012886 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24305804073810577\n",
      "Epoch 62: loss = 4.1413068771362305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03669152036309242 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2421586960554123\n",
      "Epoch 63: loss = 4.253448963165283\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03764916583895683 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24231396615505219\n",
      "Epoch 64: loss = 4.09960412979126\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06992990523576736 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24167168140411377\n",
      "Epoch 65: loss = 4.0074920654296875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06985818594694138 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24122004210948944\n",
      "Epoch 66: loss = 4.160006523132324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06890183687210083 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24059651792049408\n",
      "Epoch 67: loss = 4.213639259338379\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.19525396823883057 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23966218531131744\n",
      "Epoch 68: loss = 4.047647476196289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5594639778137207 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2398451268672943\n",
      "Epoch 69: loss = 4.369792938232422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5522016286849976 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2395457923412323\n",
      "Epoch 70: loss = 4.101964473724365\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.5044212937355042 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23963706195354462\n",
      "Epoch 71: loss = 4.296689987182617\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.4530366063117981 motion_loss: \n",
      "smoothed_heaviside_loss:  0.24020420014858246\n",
      "Epoch 72: loss = 4.161327838897705\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.40976476669311523 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23981846868991852\n",
      "Epoch 73: loss = 4.1487202644348145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.33127862215042114 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2395036816596985\n",
      "Epoch 74: loss = 3.894148349761963\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.3194391131401062 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23938289284706116\n",
      "Epoch 75: loss = 3.8490686416625977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.282792866230011 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23865029215812683\n",
      "Epoch 76: loss = 3.7631173133850098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10238136351108551 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23837979137897491\n",
      "Epoch 77: loss = 3.9701499938964844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10958129167556763 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23830820620059967\n",
      "Epoch 78: loss = 3.9724321365356445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.10706925392150879 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23781973123550415\n",
      "Epoch 79: loss = 3.786567211151123\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.12500107288360596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.2371453046798706\n",
      "Epoch 80: loss = 3.7517404556274414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.12060971558094025 motion_loss: \n",
      "smoothed_heaviside_loss:  0.23675471544265747\n",
      "Epoch 81: loss = 3.599501609802246\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  5296\n",
      "tensor(0.1083, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.1299, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0953, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 529\n",
      "Before upsampling, number of sites: 5296 amount added: 2116\n",
      "sites shape AFTER:  torch.Size([7412, 3])\n",
      "sites sdf shape AFTER:  torch.Size([7412])\n",
      "eikonal_loss:  0.12962809205055237 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09857345372438431\n",
      "Epoch 82: loss = 2.8561513423919678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.12210202217102051 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09870479255914688\n",
      "Epoch 83: loss = 2.7521703243255615\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.125497967004776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09858320653438568\n",
      "Epoch 84: loss = 2.613975763320923\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.09527955204248428 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09853369742631912\n",
      "Epoch 85: loss = 2.6898062229156494\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08016299456357956 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09809734672307968\n",
      "Epoch 86: loss = 2.8138551712036133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.08574512600898743 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09763121604919434\n",
      "Epoch 87: loss = 2.9214227199554443\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07556226849555969 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09751390665769577\n",
      "Epoch 88: loss = 2.9412121772766113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06982796639204025 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09709939360618591\n",
      "Epoch 89: loss = 2.8770029544830322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.075175940990448 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0969938263297081\n",
      "Epoch 90: loss = 2.7798666954040527\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07197022438049316 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09671308845281601\n",
      "Epoch 91: loss = 2.525561809539795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07323803007602692 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09660634398460388\n",
      "Epoch 92: loss = 2.5698699951171875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07227711379528046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09624117612838745\n",
      "Epoch 93: loss = 2.498224973678589\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05042162537574768 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09604288637638092\n",
      "Epoch 94: loss = 2.5174405574798584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05000900477170944 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09565694630146027\n",
      "Epoch 95: loss = 2.5993459224700928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04696517810225487 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09530819207429886\n",
      "Epoch 96: loss = 2.541591167449951\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07590575516223907 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0953330397605896\n",
      "Epoch 97: loss = 2.6626780033111572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04435794800519943 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09493876993656158\n",
      "Epoch 98: loss = 2.598933458328247\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039169009774923325 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09509347379207611\n",
      "Epoch 99: loss = 2.677612781524658\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0361907072365284 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09491803497076035\n",
      "Epoch 100: loss = 2.7737350463867188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.034118425101041794 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09479045867919922\n",
      "Epoch 101: loss = 2.7222111225128174\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03226401284337044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09506803005933762\n",
      "Epoch 102: loss = 2.560494899749756\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.030347470194101334 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09473484754562378\n",
      "Epoch 103: loss = 2.452497720718384\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05429090932011604 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09448099136352539\n",
      "Epoch 104: loss = 2.479703187942505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05595105141401291 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09426775574684143\n",
      "Epoch 105: loss = 2.6296567916870117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05432974174618721 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09419216215610504\n",
      "Epoch 106: loss = 2.390474796295166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04888331517577171 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09420467913150787\n",
      "Epoch 107: loss = 2.2402336597442627\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04257364571094513 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09411650896072388\n",
      "Epoch 108: loss = 2.243532657623291\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04104689508676529 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09431606531143188\n",
      "Epoch 109: loss = 2.1907100677490234\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03913344442844391 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09426283836364746\n",
      "Epoch 110: loss = 2.2927653789520264\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03758695349097252 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09434519708156586\n",
      "Epoch 111: loss = 2.336130380630493\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.035989128053188324 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09430242329835892\n",
      "Epoch 112: loss = 2.2730326652526855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.034454233944416046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09393153339624405\n",
      "Epoch 113: loss = 2.30387282371521\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.028574306517839432 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09396698325872421\n",
      "Epoch 114: loss = 2.38377046585083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.028375111520290375 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09373287111520767\n",
      "Epoch 115: loss = 2.380073070526123\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02667762152850628 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09354269504547119\n",
      "Epoch 116: loss = 2.4495999813079834\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.024636633694171906 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09359057992696762\n",
      "Epoch 117: loss = 2.4770021438598633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.039190150797367096 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09377414733171463\n",
      "Epoch 118: loss = 2.4103620052337646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.037127234041690826 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09380612522363663\n",
      "Epoch 119: loss = 2.282977819442749\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021323496475815773 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09407322853803635\n",
      "Epoch 120: loss = 2.3439252376556396\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021087709814310074 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09396673738956451\n",
      "Epoch 121: loss = 2.369054079055786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021547410637140274 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09408480674028397\n",
      "Epoch 122: loss = 2.3611040115356445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021637456491589546 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09390223026275635\n",
      "Epoch 123: loss = 2.3273770809173584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021594811230897903 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09384801238775253\n",
      "Epoch 124: loss = 2.2838363647460938\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.023006509989500046 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09357679635286331\n",
      "Epoch 125: loss = 2.273918867111206\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02249911241233349 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0934847816824913\n",
      "Epoch 126: loss = 2.274348497390747\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022610129788517952 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0933365598320961\n",
      "Epoch 127: loss = 2.256530523300171\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03169671446084976 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09348058700561523\n",
      "Epoch 128: loss = 2.257542371749878\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.022252658382058144 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09336712956428528\n",
      "Epoch 129: loss = 2.2136781215667725\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.021724138408899307 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0932217687368393\n",
      "Epoch 130: loss = 2.1595914363861084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01988168992102146 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09321355074644089\n",
      "Epoch 131: loss = 2.194742202758789\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019513770937919617 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09314662218093872\n",
      "Epoch 132: loss = 2.258232355117798\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019261933863162994 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09308023005723953\n",
      "Epoch 133: loss = 2.3663580417633057\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.018825769424438477 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09306124597787857\n",
      "Epoch 134: loss = 2.3813421726226807\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01808149740099907 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09284918010234833\n",
      "Epoch 135: loss = 2.2754409313201904\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017810696735978127 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09286369383335114\n",
      "Epoch 136: loss = 2.2487311363220215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017267679795622826 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09296245127916336\n",
      "Epoch 137: loss = 2.133758544921875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01699528843164444 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09313926100730896\n",
      "Epoch 138: loss = 2.290163040161133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.016258640214800835 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09310686588287354\n",
      "Epoch 139: loss = 2.2533202171325684\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.023961907252669334 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09297427535057068\n",
      "Epoch 140: loss = 2.1325724124908447\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.02153613790869713 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09307217597961426\n",
      "Epoch 141: loss = 2.061154842376709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01926390267908573 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09323994815349579\n",
      "Epoch 142: loss = 2.0222508907318115\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01989809051156044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09308374673128128\n",
      "Epoch 143: loss = 1.9984318017959595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.019327903166413307 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09302463382482529\n",
      "Epoch 144: loss = 1.9863228797912598\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.020094633102416992 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09283487498760223\n",
      "Epoch 145: loss = 2.1575636863708496\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.031696245074272156 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09288942068815231\n",
      "Epoch 146: loss = 2.0271294116973877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07542592287063599 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09297151118516922\n",
      "Epoch 147: loss = 2.1804544925689697\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.07353486120700836 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09288907796144485\n",
      "Epoch 148: loss = 2.0420825481414795\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06515657901763916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0927056074142456\n",
      "Epoch 149: loss = 2.0462656021118164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.05996934697031975 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09267523884773254\n",
      "Epoch 150: loss = 2.043221950531006\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.04432909935712814 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09255488961935043\n",
      "Epoch 151: loss = 2.1042330265045166\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03828606754541397 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09246306866407394\n",
      "Epoch 152: loss = 2.0718226432800293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03755202144384384 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09253960102796555\n",
      "Epoch 153: loss = 2.011286497116089\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0335736945271492 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09268250316381454\n",
      "Epoch 154: loss = 2.0005035400390625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.03805840015411377 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09261894226074219\n",
      "Epoch 155: loss = 1.9461274147033691\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01832345873117447 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09252908825874329\n",
      "Epoch 156: loss = 1.940066933631897\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.017207667231559753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09250421822071075\n",
      "Epoch 157: loss = 2.047322988510132\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015857910737395287 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09244770556688309\n",
      "Epoch 158: loss = 1.9208685159683228\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.014662595465779305 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09226939082145691\n",
      "Epoch 159: loss = 1.939070224761963\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01413538958877325 motion_loss: \n",
      "smoothed_heaviside_loss:  0.09239568561315536\n",
      "Epoch 160: loss = 2.138505697250366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008768465369939804 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0922112986445427\n",
      "Epoch 161: loss = 2.05464243888855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  7412\n",
      "tensor(0.0799, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0958, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0703, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 741\n",
      "Before upsampling, number of sites: 7412 amount added: 2964\n",
      "sites shape AFTER:  torch.Size([10376, 3])\n",
      "sites sdf shape AFTER:  torch.Size([10376])\n",
      "eikonal_loss:  0.006349781062453985 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05534201115369797\n",
      "Epoch 162: loss = 1.6536941528320312\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0043690442107617855 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05531271919608116\n",
      "Epoch 163: loss = 1.5011169910430908\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0039454554207623005 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05537744611501694\n",
      "Epoch 164: loss = 1.6633100509643555\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003912382759153843 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05492079257965088\n",
      "Epoch 165: loss = 1.5914472341537476\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0034138045739382505 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05451573058962822\n",
      "Epoch 166: loss = 1.5877968072891235\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011866436339914799 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05436060205101967\n",
      "Epoch 167: loss = 1.707885503768921\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009486634284257889 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05402323976159096\n",
      "Epoch 168: loss = 1.7127478122711182\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0035083494149148464 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05394699051976204\n",
      "Epoch 169: loss = 1.598785161972046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003757886588573456 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05358855053782463\n",
      "Epoch 170: loss = 1.6797428131103516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003383571282029152 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05339118093252182\n",
      "Epoch 171: loss = 1.6351287364959717\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0033533009700477123 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053383953869342804\n",
      "Epoch 172: loss = 1.6720436811447144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0031819548457860947 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053305499255657196\n",
      "Epoch 173: loss = 1.6016985177993774\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0032203029841184616 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05356105417013168\n",
      "Epoch 174: loss = 1.5199698209762573\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003871930530294776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05357455462217331\n",
      "Epoch 175: loss = 1.636396884918213\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.06180942803621292 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05342915281653404\n",
      "Epoch 176: loss = 1.8169264793395996\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0996054857969284 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05349034443497658\n",
      "Epoch 177: loss = 1.754668116569519\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.043497234582901 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05340162292122841\n",
      "Epoch 178: loss = 1.894132375717163\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0040769027546048164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05348305031657219\n",
      "Epoch 179: loss = 1.8747498989105225\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0036341024097055197 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05347263813018799\n",
      "Epoch 180: loss = 1.9097145795822144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0038155389484018087 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053389742970466614\n",
      "Epoch 181: loss = 1.9818389415740967\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004627193324267864 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053965937346220016\n",
      "Epoch 182: loss = 2.0520317554473877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005519394297152758 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05392369255423546\n",
      "Epoch 183: loss = 1.9507805109024048\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005071915220469236 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05389834940433502\n",
      "Epoch 184: loss = 1.9761486053466797\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00397110590711236 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0537249892950058\n",
      "Epoch 185: loss = 1.9291133880615234\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003902055323123932 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05355767160654068\n",
      "Epoch 186: loss = 1.911517858505249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003802267834544182 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05359680578112602\n",
      "Epoch 187: loss = 1.806687593460083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005413116421550512 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05366840213537216\n",
      "Epoch 188: loss = 1.665169596672058\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005302716977894306 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053581446409225464\n",
      "Epoch 189: loss = 1.7762229442596436\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006130623631179333 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05348363518714905\n",
      "Epoch 190: loss = 1.8044943809509277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0064939456060528755 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053383734077215195\n",
      "Epoch 191: loss = 1.9381508827209473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0056582968682050705 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05326560512185097\n",
      "Epoch 192: loss = 1.7894282341003418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006185257341712713 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05306721478700638\n",
      "Epoch 193: loss = 1.9245468378067017\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006517614237964153 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05306868255138397\n",
      "Epoch 194: loss = 1.8681578636169434\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007525033317506313 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05291277542710304\n",
      "Epoch 195: loss = 1.9340991973876953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007430802099406719 motion_loss: \n",
      "smoothed_heaviside_loss:  0.052778150886297226\n",
      "Epoch 196: loss = 1.8816810846328735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008743993937969208 motion_loss: \n",
      "smoothed_heaviside_loss:  0.052871860563755035\n",
      "Epoch 197: loss = 1.8234519958496094\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007672534324228764 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05283786356449127\n",
      "Epoch 198: loss = 1.8404306173324585\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007702212315052748 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05278946831822395\n",
      "Epoch 199: loss = 1.8091187477111816\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010943684726953506 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05305662378668785\n",
      "Epoch 200: loss = 1.986220121383667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010309792123734951 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0531514547765255\n",
      "Epoch 201: loss = 2.147716999053955\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009817766025662422 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053242526948451996\n",
      "Epoch 202: loss = 2.003387451171875\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0074827647767961025 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05345901474356651\n",
      "Epoch 203: loss = 1.9700713157653809\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0070866793394088745 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05333133041858673\n",
      "Epoch 204: loss = 2.0460996627807617\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006935493554919958 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05326598510146141\n",
      "Epoch 205: loss = 2.0676143169403076\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011221030727028847 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053453296422958374\n",
      "Epoch 206: loss = 2.0806069374084473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.01033530943095684 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05344262346625328\n",
      "Epoch 207: loss = 1.9772377014160156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006083638407289982 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053255774080753326\n",
      "Epoch 208: loss = 1.8573509454727173\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006957361474633217 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053146906197071075\n",
      "Epoch 209: loss = 1.7988475561141968\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006586579605937004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053006429225206375\n",
      "Epoch 210: loss = 1.7908174991607666\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005723949521780014 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053057171404361725\n",
      "Epoch 211: loss = 1.6312583684921265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0056711649522185326 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053030576556921005\n",
      "Epoch 212: loss = 1.7991423606872559\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005663418676704168 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053146667778491974\n",
      "Epoch 213: loss = 1.9074084758758545\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006218384485691786 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053129181265830994\n",
      "Epoch 214: loss = 2.0138306617736816\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006779555231332779 motion_loss: \n",
      "smoothed_heaviside_loss:  0.052905939519405365\n",
      "Epoch 215: loss = 2.0222902297973633\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0068153138272464275 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053040288388729095\n",
      "Epoch 216: loss = 2.0846028327941895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007649327628314495 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0529106929898262\n",
      "Epoch 217: loss = 1.8850632905960083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007649990264326334 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05315907299518585\n",
      "Epoch 218: loss = 1.9280633926391602\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006693071685731411 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05318852886557579\n",
      "Epoch 219: loss = 1.9572187662124634\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005836985539644957 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05316319316625595\n",
      "Epoch 220: loss = 2.0229315757751465\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006833821069449186 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05312633141875267\n",
      "Epoch 221: loss = 1.8666408061981201\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004532474558800459 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053152475506067276\n",
      "Epoch 222: loss = 1.9814906120300293\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004517459776252508 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053171440958976746\n",
      "Epoch 223: loss = 1.9291030168533325\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004334166646003723 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053482577204704285\n",
      "Epoch 224: loss = 1.865949034690857\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.015940634533762932 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053389180451631546\n",
      "Epoch 225: loss = 1.8986963033676147\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.011615963652729988 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0536714605987072\n",
      "Epoch 226: loss = 1.9461724758148193\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012361668981611729 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053508210927248\n",
      "Epoch 227: loss = 1.995560884475708\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012491529807448387 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05363517999649048\n",
      "Epoch 228: loss = 1.9329941272735596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012434554286301136 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05353376269340515\n",
      "Epoch 229: loss = 1.9269300699234009\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012498804368078709 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05367901548743248\n",
      "Epoch 230: loss = 2.009718418121338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009270555339753628 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053547076880931854\n",
      "Epoch 231: loss = 1.9368433952331543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008450167253613472 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05351276695728302\n",
      "Epoch 232: loss = 1.9798017740249634\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00734357675537467 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05344082787632942\n",
      "Epoch 233: loss = 1.8763413429260254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00658531254157424 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053316354751586914\n",
      "Epoch 234: loss = 1.8306450843811035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009493524208664894 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05387808009982109\n",
      "Epoch 235: loss = 1.8515771627426147\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.007449135184288025 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05377872660756111\n",
      "Epoch 236: loss = 1.7917639017105103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004406079649925232 motion_loss: \n",
      "smoothed_heaviside_loss:  0.053675223141908646\n",
      "Epoch 237: loss = 1.6736699342727661\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003936229273676872 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05362868681550026\n",
      "Epoch 238: loss = 1.840874195098877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003722678404301405 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05368052050471306\n",
      "Epoch 239: loss = 1.93415367603302\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00341656431555748 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05365508049726486\n",
      "Epoch 240: loss = 2.068885087966919\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003663855604827404 motion_loss: \n",
      "smoothed_heaviside_loss:  0.05355415120720863\n",
      "Epoch 241: loss = 2.085470676422119\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  10376\n",
      "tensor(0.0284, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0341, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0250, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 1037\n",
      "Before upsampling, number of sites: 10376 amount added: 4148\n",
      "sites shape AFTER:  torch.Size([14524, 3])\n",
      "sites sdf shape AFTER:  torch.Size([14524])\n",
      "eikonal_loss:  0.003210504073649645 motion_loss: \n",
      "smoothed_heaviside_loss:  0.036166682839393616\n",
      "Epoch 242: loss = 1.5031663179397583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0025009552482515574 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03595229983329773\n",
      "Epoch 243: loss = 1.432039499282837\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0027514477260410786 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03578425198793411\n",
      "Epoch 244: loss = 1.3473968505859375\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002059678314253688 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03550415486097336\n",
      "Epoch 245: loss = 1.4952213764190674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015504484763368964 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03553874045610428\n",
      "Epoch 246: loss = 1.5804911851882935\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003304979996755719 motion_loss: \n",
      "smoothed_heaviside_loss:  0.035355355590581894\n",
      "Epoch 247: loss = 1.5327643156051636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0029945969581604004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03520365431904793\n",
      "Epoch 248: loss = 1.601643443107605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003123547649011016 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03488487750291824\n",
      "Epoch 249: loss = 1.564905047416687\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001266958424821496 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03493142127990723\n",
      "Epoch 250: loss = 1.55482816696167\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012479890137910843 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03479814901947975\n",
      "Epoch 251: loss = 1.4416673183441162\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011660729069262743 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034845899790525436\n",
      "Epoch 252: loss = 1.538739562034607\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011211475357413292 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0348602831363678\n",
      "Epoch 253: loss = 1.7674163579940796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010949292918667197 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034953050315380096\n",
      "Epoch 254: loss = 1.9044047594070435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009778800886124372 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03484201431274414\n",
      "Epoch 255: loss = 1.7868337631225586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009041826706379652 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034707050770521164\n",
      "Epoch 256: loss = 1.7384929656982422\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008938070968724787 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03466714546084404\n",
      "Epoch 257: loss = 1.7089864015579224\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000801444984972477 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0346653051674366\n",
      "Epoch 258: loss = 1.667005181312561\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008985082968138158 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03460352122783661\n",
      "Epoch 259: loss = 1.6160639524459839\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009219563798978925 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03468974679708481\n",
      "Epoch 260: loss = 1.6578481197357178\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013288452755659819 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03479067236185074\n",
      "Epoch 261: loss = 1.5426914691925049\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013451739214360714 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034687262028455734\n",
      "Epoch 262: loss = 1.6879842281341553\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012850073399022222 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03465760499238968\n",
      "Epoch 263: loss = 1.7487561702728271\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013072829460725188 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0345974899828434\n",
      "Epoch 264: loss = 1.8554214239120483\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011888855369761586 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03457815572619438\n",
      "Epoch 265: loss = 1.6164989471435547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00103929138276726 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03449643403291702\n",
      "Epoch 266: loss = 1.6885638236999512\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009925627382472157 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03446364030241966\n",
      "Epoch 267: loss = 1.6741138696670532\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008572883671149611 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034554921090602875\n",
      "Epoch 268: loss = 1.5642790794372559\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007717364933341742 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0344788022339344\n",
      "Epoch 269: loss = 1.6767733097076416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000804195529781282 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034402452409267426\n",
      "Epoch 270: loss = 1.6983420848846436\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007945167599245906 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03444991633296013\n",
      "Epoch 271: loss = 1.8021310567855835\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007853860151953995 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03447531536221504\n",
      "Epoch 272: loss = 1.7145984172821045\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007993682520464063 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03443589806556702\n",
      "Epoch 273: loss = 1.7259331941604614\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001027632737532258 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034548353403806686\n",
      "Epoch 274: loss = 1.7560476064682007\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008990033529698849 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03479749336838722\n",
      "Epoch 275: loss = 1.641808271408081\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007705297321081161 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03469356149435043\n",
      "Epoch 276: loss = 1.7671743631362915\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009629824198782444 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03460711985826492\n",
      "Epoch 277: loss = 1.8060657978057861\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001087226439267397 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034629907459020615\n",
      "Epoch 278: loss = 1.7792035341262817\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008607844356447458 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034635674208402634\n",
      "Epoch 279: loss = 1.916483998298645\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010308267083019018 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03452417254447937\n",
      "Epoch 280: loss = 1.740033507347107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001125088194385171 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034509506076574326\n",
      "Epoch 281: loss = 1.6523158550262451\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001064589130692184 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03446962311863899\n",
      "Epoch 282: loss = 1.8129053115844727\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001754456665366888 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03436052054166794\n",
      "Epoch 283: loss = 1.7756139039993286\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018072874518111348 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03451386094093323\n",
      "Epoch 284: loss = 1.8335707187652588\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001767309382557869 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03447869047522545\n",
      "Epoch 285: loss = 1.7709174156188965\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002507601398974657 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034289006143808365\n",
      "Epoch 286: loss = 1.634527325630188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0023856903426349163 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03438239544630051\n",
      "Epoch 287: loss = 1.8545801639556885\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024936790578067303 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03431682661175728\n",
      "Epoch 288: loss = 1.7873730659484863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004732612520456314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03427470847964287\n",
      "Epoch 289: loss = 1.7160636186599731\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003834923030808568 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034339290112257004\n",
      "Epoch 290: loss = 1.8197336196899414\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0036864562425762415 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03450183942914009\n",
      "Epoch 291: loss = 1.8255970478057861\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004091689363121986 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0345352366566658\n",
      "Epoch 292: loss = 1.627690076828003\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0038039402570575476 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03446974977850914\n",
      "Epoch 293: loss = 1.909044623374939\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0033667506650090218 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03450590372085571\n",
      "Epoch 294: loss = 1.8020588159561157\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00207290006801486 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034641191363334656\n",
      "Epoch 295: loss = 1.709898591041565\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0025549682322889566 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034628819674253464\n",
      "Epoch 296: loss = 1.6154730319976807\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024977107532322407 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03461906686425209\n",
      "Epoch 297: loss = 1.7173885107040405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002900631632655859 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034822676330804825\n",
      "Epoch 298: loss = 1.5588576793670654\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0033642633352428675 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03488479554653168\n",
      "Epoch 299: loss = 1.624152660369873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002419406548142433 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03484388813376427\n",
      "Epoch 300: loss = 1.6210342645645142\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002353628631681204 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03501288965344429\n",
      "Epoch 301: loss = 1.6906542778015137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001985427923500538 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03509628027677536\n",
      "Epoch 302: loss = 1.7108266353607178\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0020055104978382587 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03503910079598427\n",
      "Epoch 303: loss = 1.8396077156066895\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018695573089644313 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03514188528060913\n",
      "Epoch 304: loss = 1.7689542770385742\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017705969512462616 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03505788743495941\n",
      "Epoch 305: loss = 1.6557673215866089\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015391166089102626 motion_loss: \n",
      "smoothed_heaviside_loss:  0.034987930208444595\n",
      "Epoch 306: loss = 1.683919072151184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017089121975004673 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03493476286530495\n",
      "Epoch 307: loss = 1.7099169492721558\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0021504724863916636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.035035327076911926\n",
      "Epoch 308: loss = 1.5359928607940674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0020017281640321016 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03513164818286896\n",
      "Epoch 309: loss = 1.5559121370315552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002010672353208065 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03510062396526337\n",
      "Epoch 310: loss = 1.6083327531814575\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001912798616103828 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03519785404205322\n",
      "Epoch 311: loss = 1.6997655630111694\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019718424882739782 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03513447567820549\n",
      "Epoch 312: loss = 1.5720844268798828\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019540945068001747 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03523015230894089\n",
      "Epoch 313: loss = 1.5824519395828247\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018758843652904034 motion_loss: \n",
      "smoothed_heaviside_loss:  0.035091646015644073\n",
      "Epoch 314: loss = 1.5243091583251953\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015964475460350513 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0351693369448185\n",
      "Epoch 315: loss = 1.5712169408798218\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015778238885104656 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03526623547077179\n",
      "Epoch 316: loss = 1.5920169353485107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017951062181964517 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0353589691221714\n",
      "Epoch 317: loss = 1.565556287765503\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018366456497460604 motion_loss: \n",
      "smoothed_heaviside_loss:  0.035510506480932236\n",
      "Epoch 318: loss = 1.548629879951477\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019463312346488237 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03579523041844368\n",
      "Epoch 319: loss = 1.7260961532592773\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019282690482214093 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03571324050426483\n",
      "Epoch 320: loss = 1.774504542350769\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0020639479625970125 motion_loss: \n",
      "smoothed_heaviside_loss:  0.03566823527216911\n",
      "Epoch 321: loss = 1.759636640548706\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  14524\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0215, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0158, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 1452\n",
      "Before upsampling, number of sites: 14524 amount added: 5808\n",
      "sites shape AFTER:  torch.Size([20332, 3])\n",
      "sites sdf shape AFTER:  torch.Size([20332])\n",
      "eikonal_loss:  0.001281963661313057 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02493968792259693\n",
      "Epoch 322: loss = 1.290899395942688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001093124970793724 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02486972138285637\n",
      "Epoch 323: loss = 1.2735806703567505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001057124580256641 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024641012772917747\n",
      "Epoch 324: loss = 1.1473585367202759\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002718085888773203 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024701811373233795\n",
      "Epoch 325: loss = 1.163495659828186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002869785763323307 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0245281383395195\n",
      "Epoch 326: loss = 1.2386025190353394\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003108479082584381 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024302631616592407\n",
      "Epoch 327: loss = 1.3000500202178955\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0030810991302132607 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02433050237596035\n",
      "Epoch 328: loss = 1.297296166419983\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002871470060199499 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024316251277923584\n",
      "Epoch 329: loss = 1.3534770011901855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0031247707083821297 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024183152243494987\n",
      "Epoch 330: loss = 1.5492138862609863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011050347238779068 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024100180715322495\n",
      "Epoch 331: loss = 1.3867062330245972\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013404232449829578 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024044718593358994\n",
      "Epoch 332: loss = 1.3878531455993652\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001329599879682064 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023993778973817825\n",
      "Epoch 333: loss = 1.439855933189392\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001167107606306672 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02391873672604561\n",
      "Epoch 334: loss = 1.440871238708496\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011557559482753277 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023872429504990578\n",
      "Epoch 335: loss = 1.3915051221847534\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009838273981586099 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02401866391301155\n",
      "Epoch 336: loss = 1.338938593864441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010167427826672792 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024038055911660194\n",
      "Epoch 337: loss = 1.396880030632019\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006051498930901289 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023977482691407204\n",
      "Epoch 338: loss = 1.3731642961502075\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013084301026538014 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0239398293197155\n",
      "Epoch 339: loss = 1.3260589838027954\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00188348104711622 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023997511714696884\n",
      "Epoch 340: loss = 1.460813045501709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011802613735198975 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023994874209165573\n",
      "Epoch 341: loss = 1.5716346502304077\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013137925416231155 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02395031601190567\n",
      "Epoch 342: loss = 1.3705912828445435\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011794574093073606 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023953452706336975\n",
      "Epoch 343: loss = 1.4586008787155151\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012832910288125277 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02392497844994068\n",
      "Epoch 344: loss = 1.4298350811004639\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008700412581674755 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023872533813118935\n",
      "Epoch 345: loss = 1.5746766328811646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008448650478385389 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023816365748643875\n",
      "Epoch 346: loss = 1.516821026802063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000676381983794272 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023746026679873466\n",
      "Epoch 347: loss = 1.3730669021606445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006057995487935841 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02371680550277233\n",
      "Epoch 348: loss = 1.3846598863601685\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001335314940661192 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023853834718465805\n",
      "Epoch 349: loss = 1.3771339654922485\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014068374875932932 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02374098263680935\n",
      "Epoch 350: loss = 1.3159042596817017\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001306663965806365 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02368953451514244\n",
      "Epoch 351: loss = 1.3941611051559448\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001247958978638053 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023688625544309616\n",
      "Epoch 352: loss = 1.4035286903381348\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009629853069782257 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023611225187778473\n",
      "Epoch 353: loss = 1.37972092628479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004197251051664352 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02377486601471901\n",
      "Epoch 354: loss = 1.3600332736968994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003353055566549301 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023886989802122116\n",
      "Epoch 355: loss = 1.2979665994644165\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0037348000332713127 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02388669364154339\n",
      "Epoch 356: loss = 1.2560762166976929\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012333504855632782 motion_loss: \n",
      "smoothed_heaviside_loss:  0.023820871487259865\n",
      "Epoch 357: loss = 1.4194869995117188\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005179629195481539 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02399972639977932\n",
      "Epoch 358: loss = 1.2384135723114014\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00834638997912407 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024082232266664505\n",
      "Epoch 359: loss = 1.2479474544525146\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008844523690640926 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024099109694361687\n",
      "Epoch 360: loss = 1.2535756826400757\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009887825697660446 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024121249094605446\n",
      "Epoch 361: loss = 1.4363758563995361\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.010770689696073532 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024165932089090347\n",
      "Epoch 362: loss = 1.381535291671753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.009073383174836636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02428591623902321\n",
      "Epoch 363: loss = 1.257290005683899\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.012232938781380653 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024225689470767975\n",
      "Epoch 364: loss = 1.379077672958374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008776159025728703 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024250097572803497\n",
      "Epoch 365: loss = 1.216407060623169\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.008819585666060448 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02431807667016983\n",
      "Epoch 366: loss = 1.336090326309204\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00861208513379097 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024228116497397423\n",
      "Epoch 367: loss = 1.2679299116134644\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006584290415048599 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02421514503657818\n",
      "Epoch 368: loss = 1.3319802284240723\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0027237702161073685 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024264020845294\n",
      "Epoch 369: loss = 1.2412923574447632\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0029543882701545954 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02421639673411846\n",
      "Epoch 370: loss = 1.1662253141403198\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005843895487487316 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024194899946451187\n",
      "Epoch 371: loss = 1.2529047727584839\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004716762341558933 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024163002148270607\n",
      "Epoch 372: loss = 1.247646450996399\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0028566927649080753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024177998304367065\n",
      "Epoch 373: loss = 1.258514165878296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0025660963729023933 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02414446696639061\n",
      "Epoch 374: loss = 1.420191764831543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002196465851739049 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02423795871436596\n",
      "Epoch 375: loss = 1.4701411724090576\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005430974997580051 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024178365245461464\n",
      "Epoch 376: loss = 1.2784267663955688\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004492178559303284 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024172762408852577\n",
      "Epoch 377: loss = 1.2697335481643677\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004492389503866434 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024251000955700874\n",
      "Epoch 378: loss = 1.2844717502593994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.006464589852839708 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024247661232948303\n",
      "Epoch 379: loss = 1.3985289335250854\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005768013186752796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024490490555763245\n",
      "Epoch 380: loss = 1.4364534616470337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0056094881147146225 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024527307599782944\n",
      "Epoch 381: loss = 1.3256826400756836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.005199214909225702 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024481365457177162\n",
      "Epoch 382: loss = 1.2478598356246948\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004927501548081636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02443300560116768\n",
      "Epoch 383: loss = 1.2786107063293457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004590736702084541 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024399513378739357\n",
      "Epoch 384: loss = 1.1809039115905762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0044709136709570885 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024378100410103798\n",
      "Epoch 385: loss = 1.1838083267211914\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004338468890637159 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024379221722483635\n",
      "Epoch 386: loss = 1.0799590349197388\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003863054094836116 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02438322827219963\n",
      "Epoch 387: loss = 1.186143398284912\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0032412600703537464 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024482406675815582\n",
      "Epoch 388: loss = 1.204143762588501\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002847458003088832 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024473367258906364\n",
      "Epoch 389: loss = 1.1284509897232056\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002639075741171837 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02451663464307785\n",
      "Epoch 390: loss = 1.2034177780151367\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024679999332875013 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024454256519675255\n",
      "Epoch 391: loss = 1.1191898584365845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014179461868479848 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024429596960544586\n",
      "Epoch 392: loss = 1.0507903099060059\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014727102825418115 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02456953190267086\n",
      "Epoch 393: loss = 1.1588695049285889\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014061513356864452 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024515002965927124\n",
      "Epoch 394: loss = 1.1744216680526733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001457826467230916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024452704936265945\n",
      "Epoch 395: loss = 1.1226704120635986\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001872039632871747 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0245185736566782\n",
      "Epoch 396: loss = 1.1945606470108032\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017282236367464066 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024641670286655426\n",
      "Epoch 397: loss = 1.157930612564087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015990842366591096 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024631304666399956\n",
      "Epoch 398: loss = 1.1523040533065796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017043726984411478 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024670429527759552\n",
      "Epoch 399: loss = 1.1981191635131836\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001778926933184266 motion_loss: \n",
      "smoothed_heaviside_loss:  0.02461480349302292\n",
      "Epoch 400: loss = 1.292060375213623\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017848541028797626 motion_loss: \n",
      "smoothed_heaviside_loss:  0.024619344621896744\n",
      "Epoch 401: loss = 1.222622036933899\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  20332\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0165, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 2033\n",
      "Before upsampling, number of sites: 20332 amount added: 8132\n",
      "sites shape AFTER:  torch.Size([28464, 3])\n",
      "sites sdf shape AFTER:  torch.Size([28464])\n",
      "eikonal_loss:  0.001329253427684307 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017802737653255463\n",
      "Epoch 402: loss = 0.969794750213623\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0016367667121812701 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017676901072263718\n",
      "Epoch 403: loss = 0.9028171300888062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017905397107824683 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017660904675722122\n",
      "Epoch 404: loss = 1.042775273323059\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017703113844618201 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017456112429499626\n",
      "Epoch 405: loss = 0.951507031917572\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001782833831384778 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017388999462127686\n",
      "Epoch 406: loss = 0.9826205968856812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015871895011514425 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01727517694234848\n",
      "Epoch 407: loss = 0.9560253024101257\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014695783611387014 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017254961654543877\n",
      "Epoch 408: loss = 1.00619637966156\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013023926876485348 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017354261130094528\n",
      "Epoch 409: loss = 1.0265321731567383\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001473067793995142 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017300231382250786\n",
      "Epoch 410: loss = 1.0658072233200073\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009849664056673646 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017319899052381516\n",
      "Epoch 411: loss = 0.9859738349914551\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006937490543350577 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017240285873413086\n",
      "Epoch 412: loss = 1.0026596784591675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007135897176340222 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017267044633626938\n",
      "Epoch 413: loss = 0.9881706237792969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006312262848950922 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017343977466225624\n",
      "Epoch 414: loss = 0.9717400074005127\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00072450190782547 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01732777990400791\n",
      "Epoch 415: loss = 1.0765984058380127\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007395469001494348 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017354177311062813\n",
      "Epoch 416: loss = 1.0374116897583008\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000765511766076088 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017342496663331985\n",
      "Epoch 417: loss = 1.0148380994796753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007157977088354528 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017300376668572426\n",
      "Epoch 418: loss = 0.9826159477233887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009791450574994087 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017312094569206238\n",
      "Epoch 419: loss = 1.0436002016067505\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009143884526565671 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017286507412791252\n",
      "Epoch 420: loss = 1.0783884525299072\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008503667777404189 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017260489985346794\n",
      "Epoch 421: loss = 1.0830625295639038\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007722255541011691 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017156722024083138\n",
      "Epoch 422: loss = 1.1094967126846313\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001113465754315257 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017132114619016647\n",
      "Epoch 423: loss = 1.1056429147720337\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011704993667080998 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017118658870458603\n",
      "Epoch 424: loss = 1.0017694234848022\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009562374907545745 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01715398207306862\n",
      "Epoch 425: loss = 1.090872049331665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007942387601360679 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017122846096754074\n",
      "Epoch 426: loss = 1.0668314695358276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007980652153491974 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017221465706825256\n",
      "Epoch 427: loss = 0.9670425057411194\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007888847030699253 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017183635383844376\n",
      "Epoch 428: loss = 1.0322552919387817\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007761892047710717 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017225876450538635\n",
      "Epoch 429: loss = 1.0082303285598755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006927248323336244 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0171664971858263\n",
      "Epoch 430: loss = 1.0025938749313354\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006796808447688818 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017139816656708717\n",
      "Epoch 431: loss = 1.0023435354232788\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006397138931788504 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017184967175126076\n",
      "Epoch 432: loss = 1.033037543296814\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006422735750675201 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017278876155614853\n",
      "Epoch 433: loss = 1.0110211372375488\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007261272403411567 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017274757847189903\n",
      "Epoch 434: loss = 0.9549600481987\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006746058934368193 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017251702025532722\n",
      "Epoch 435: loss = 0.9523294568061829\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006200044881552458 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017296096310019493\n",
      "Epoch 436: loss = 1.0209782123565674\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005909469909965992 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01727338880300522\n",
      "Epoch 437: loss = 0.9902151823043823\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001292085275053978 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017302099615335464\n",
      "Epoch 438: loss = 0.8956364393234253\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008401382947340608 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017284706234931946\n",
      "Epoch 439: loss = 0.8492652177810669\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010294680250808597 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017282042652368546\n",
      "Epoch 440: loss = 0.8774290680885315\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014158671256154776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017244791612029076\n",
      "Epoch 441: loss = 1.0418986082077026\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009709189762361348 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017322657629847527\n",
      "Epoch 442: loss = 0.9823232293128967\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012261117808520794 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01729997992515564\n",
      "Epoch 443: loss = 0.9774742126464844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010905605740845203 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017441879957914352\n",
      "Epoch 444: loss = 1.0494948625564575\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.003963617607951164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017531100660562515\n",
      "Epoch 445: loss = 1.016903281211853\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008191516972146928 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017514392733573914\n",
      "Epoch 446: loss = 0.9636661410331726\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010559253860265017 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017454521730542183\n",
      "Epoch 447: loss = 1.0372593402862549\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001005647238343954 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01737283356487751\n",
      "Epoch 448: loss = 0.8991940021514893\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010698550613597035 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017517924308776855\n",
      "Epoch 449: loss = 0.8647852540016174\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001225824817083776 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017476359382271767\n",
      "Epoch 450: loss = 0.8734022378921509\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011508101597428322 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017439741641283035\n",
      "Epoch 451: loss = 0.8976991176605225\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015404950827360153 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017499592155218124\n",
      "Epoch 452: loss = 0.8615756034851074\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001361183705739677 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017546599730849266\n",
      "Epoch 453: loss = 0.8713307976722717\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001436080550774932 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017603106796741486\n",
      "Epoch 454: loss = 0.8839675784111023\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008952436619438231 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0175789725035429\n",
      "Epoch 455: loss = 0.8798136711120605\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009072140092030168 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017521478235721588\n",
      "Epoch 456: loss = 1.003478765487671\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010452150600031018 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01746288873255253\n",
      "Epoch 457: loss = 1.0322785377502441\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008844765834510326 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017417578026652336\n",
      "Epoch 458: loss = 0.9987126588821411\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008226420031860471 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017391635105013847\n",
      "Epoch 459: loss = 1.0030736923217773\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009625516249798238 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017328493297100067\n",
      "Epoch 460: loss = 0.9341602921485901\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009749326854944229 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017305904999375343\n",
      "Epoch 461: loss = 0.9741494059562683\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008444104460068047 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017367197200655937\n",
      "Epoch 462: loss = 0.9040435552597046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007988826837390661 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017383690923452377\n",
      "Epoch 463: loss = 0.9237760305404663\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011727990349754691 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017443114891648293\n",
      "Epoch 464: loss = 0.900178849697113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010245824232697487 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017411958426237106\n",
      "Epoch 465: loss = 0.8148811459541321\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00374360429123044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01737639680504799\n",
      "Epoch 466: loss = 0.8731847405433655\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001067228033207357 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017319371923804283\n",
      "Epoch 467: loss = 0.8170175552368164\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001750594237819314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017428183928132057\n",
      "Epoch 468: loss = 0.8560532331466675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018888984341174364 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01746048778295517\n",
      "Epoch 469: loss = 0.8354482054710388\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018753153271973133 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017455223947763443\n",
      "Epoch 470: loss = 0.8477200865745544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0016111816512420774 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017331700772047043\n",
      "Epoch 471: loss = 0.8399211168289185\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015451525105163455 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017362242564558983\n",
      "Epoch 472: loss = 0.8456963300704956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013438928872346878 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01736832596361637\n",
      "Epoch 473: loss = 0.829073429107666\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013558311620727181 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01734861359000206\n",
      "Epoch 474: loss = 0.8440432548522949\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013301605358719826 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017342867329716682\n",
      "Epoch 475: loss = 0.7756772637367249\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011784349335357547 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017352130264043808\n",
      "Epoch 476: loss = 0.758425235748291\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001289907144382596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017309334129095078\n",
      "Epoch 477: loss = 0.7445054054260254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0027347097638994455 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017283163964748383\n",
      "Epoch 478: loss = 0.7270991802215576\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015457943081855774 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01725359447300434\n",
      "Epoch 479: loss = 0.6922678351402283\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011898361844941974 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01725325919687748\n",
      "Epoch 480: loss = 0.7854095101356506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013698094990104437 motion_loss: \n",
      "smoothed_heaviside_loss:  0.017346316948533058\n",
      "Epoch 481: loss = 0.7518115043640137\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  28464\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MedianBackward0>) tensor(0.0133, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0097, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Hybrid upsampling regime\n",
      "Number of candidates in hybrid regime: 2846\n",
      "Before upsampling, number of sites: 28464 amount added: 11384\n",
      "sites shape AFTER:  torch.Size([39848, 3])\n",
      "sites sdf shape AFTER:  torch.Size([39848])\n",
      "eikonal_loss:  0.0007305719191208482 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01258890051394701\n",
      "Epoch 482: loss = 0.5633650422096252\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005640247836709023 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012515080161392689\n",
      "Epoch 483: loss = 0.5896272659301758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006802528514526784 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012464606203138828\n",
      "Epoch 484: loss = 0.618990957736969\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006290681194514036 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012314975261688232\n",
      "Epoch 485: loss = 0.6856911182403564\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005324610392563045 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012288782745599747\n",
      "Epoch 486: loss = 0.7469682693481445\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005244515486992896 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012273592874407768\n",
      "Epoch 487: loss = 0.7226908802986145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004876782186329365 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012223146855831146\n",
      "Epoch 488: loss = 0.6677253246307373\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005150585784576833 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012148158624768257\n",
      "Epoch 489: loss = 0.6771722435951233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005571098881773651 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012190548703074455\n",
      "Epoch 490: loss = 0.6906194090843201\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005177845596335828 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012145751155912876\n",
      "Epoch 491: loss = 0.7013338208198547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004948783898726106 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012111109681427479\n",
      "Epoch 492: loss = 0.7222341895103455\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00048345813411287963 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012063582427799702\n",
      "Epoch 493: loss = 0.757098913192749\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043716636719182134 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0120319789275527\n",
      "Epoch 494: loss = 0.7232319712638855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004090427537448704 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012011861428618431\n",
      "Epoch 495: loss = 0.8094341158866882\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00041844218503683805 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012030999176204205\n",
      "Epoch 496: loss = 0.7767956852912903\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004136508796364069 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012111454270780087\n",
      "Epoch 497: loss = 0.7380394339561462\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038367678644135594 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012095375917851925\n",
      "Epoch 498: loss = 0.7560603022575378\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038904050597921014 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0120851444080472\n",
      "Epoch 499: loss = 0.7656252980232239\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005061010597273707 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012149528600275517\n",
      "Epoch 500: loss = 0.7976450324058533\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00046993186697363853 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012125502340495586\n",
      "Epoch 501: loss = 0.7814936637878418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00047167669981718063 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01211845874786377\n",
      "Epoch 502: loss = 0.8020481467247009\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00044636736856773496 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012146111577749252\n",
      "Epoch 503: loss = 0.8024287223815918\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005552169750444591 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012176377698779106\n",
      "Epoch 504: loss = 0.7685791254043579\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006878410931676626 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012157179415225983\n",
      "Epoch 505: loss = 0.8042461276054382\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013989484868943691 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012225351296365261\n",
      "Epoch 506: loss = 0.8069566488265991\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006728682783432305 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012271265499293804\n",
      "Epoch 507: loss = 0.8183044195175171\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006375540979206562 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012256852351129055\n",
      "Epoch 508: loss = 0.8444563150405884\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000952324946410954 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012218543328344822\n",
      "Epoch 509: loss = 0.8107893466949463\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006489790976047516 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012281014584004879\n",
      "Epoch 510: loss = 0.8318406939506531\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005610509542748332 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012292184866964817\n",
      "Epoch 511: loss = 0.7852230668067932\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000640638405457139 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012340875342488289\n",
      "Epoch 512: loss = 0.9070196747779846\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006334696081466973 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012333192862570286\n",
      "Epoch 513: loss = 0.8723572492599487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006793022621423006 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012351003475487232\n",
      "Epoch 514: loss = 0.8467646837234497\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000663877697661519 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012325753457844257\n",
      "Epoch 515: loss = 0.8794678449630737\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006286711432039738 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012295456603169441\n",
      "Epoch 516: loss = 0.8643324375152588\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005884261918254197 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01226511038839817\n",
      "Epoch 517: loss = 0.75567626953125\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006851028883829713 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012316359207034111\n",
      "Epoch 518: loss = 0.7432278394699097\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000698998337611556 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012323341332376003\n",
      "Epoch 519: loss = 0.7434314489364624\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006578211905434728 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01241602748632431\n",
      "Epoch 520: loss = 0.7619551420211792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007226715097203851 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012427625246345997\n",
      "Epoch 521: loss = 0.7356529235839844\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000541813438758254 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012415152974426746\n",
      "Epoch 522: loss = 0.6985453367233276\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00147916073910892 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012426065281033516\n",
      "Epoch 523: loss = 0.7232972383499146\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004518385045230389 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012630578130483627\n",
      "Epoch 524: loss = 0.706071138381958\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00229734112508595 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012599275447428226\n",
      "Epoch 525: loss = 0.7459878325462341\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002383475424721837 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012636908330023289\n",
      "Epoch 526: loss = 0.7556408643722534\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017029349692165852 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012615147046744823\n",
      "Epoch 527: loss = 0.6635869145393372\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010492528090253472 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012567206285893917\n",
      "Epoch 528: loss = 0.6808325052261353\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008696504519321024 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012598192319273949\n",
      "Epoch 529: loss = 0.6782512664794922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00108812446705997 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012589113786816597\n",
      "Epoch 530: loss = 0.7332383990287781\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011809137649834156 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012557653710246086\n",
      "Epoch 531: loss = 0.6931020617485046\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011421228991821408 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012507433071732521\n",
      "Epoch 532: loss = 0.7330272197723389\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000987221603281796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01248941570520401\n",
      "Epoch 533: loss = 0.7267645597457886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000953244511038065 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01246208231896162\n",
      "Epoch 534: loss = 0.7508639097213745\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008362071239389479 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012548832222819328\n",
      "Epoch 535: loss = 0.6840046048164368\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008735175943002105 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012512417510151863\n",
      "Epoch 536: loss = 0.7317895293235779\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008607304189354181 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012482093647122383\n",
      "Epoch 537: loss = 0.6800979375839233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002211405197158456 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01255637127906084\n",
      "Epoch 538: loss = 0.6647458076477051\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010764100588858128 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012531567364931107\n",
      "Epoch 539: loss = 0.6862142086029053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0023822118528187275 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012479474768042564\n",
      "Epoch 540: loss = 0.6875079870223999\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004619056358933449 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012468142434954643\n",
      "Epoch 541: loss = 0.7174592614173889\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004246214404702187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012519439682364464\n",
      "Epoch 542: loss = 0.7623715400695801\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014622451271861792 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012488339096307755\n",
      "Epoch 543: loss = 0.7434781789779663\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001262420671992004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012554525397717953\n",
      "Epoch 544: loss = 0.7852437496185303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012171922717243433 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012538622133433819\n",
      "Epoch 545: loss = 0.7475205659866333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009682421805337071 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012462415732443333\n",
      "Epoch 546: loss = 0.7477709650993347\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008360762731172144 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012541944161057472\n",
      "Epoch 547: loss = 0.7419252991676331\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010898263426497579 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012536459602415562\n",
      "Epoch 548: loss = 0.680635929107666\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009561540209688246 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012487899512052536\n",
      "Epoch 549: loss = 0.6739623546600342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007062755757942796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012464452534914017\n",
      "Epoch 550: loss = 0.6771113276481628\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007568837609142065 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012442688457667828\n",
      "Epoch 551: loss = 0.6461676359176636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007703336887061596 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012454249896109104\n",
      "Epoch 552: loss = 0.6368162035942078\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007456174353137612 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01250145211815834\n",
      "Epoch 553: loss = 0.6252694129943848\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007523752865381539 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01247316598892212\n",
      "Epoch 554: loss = 0.6522418856620789\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008182213641703129 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01243335846811533\n",
      "Epoch 555: loss = 0.6969044208526611\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006829857593402267 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012419606558978558\n",
      "Epoch 556: loss = 0.648029625415802\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007302630692720413 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012426238507032394\n",
      "Epoch 557: loss = 0.6414074897766113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007164758280850947 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012432883493602276\n",
      "Epoch 558: loss = 0.6268567442893982\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006966281798668206 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012405174784362316\n",
      "Epoch 559: loss = 0.684120774269104\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006687815766781569 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01246294192969799\n",
      "Epoch 560: loss = 0.688092827796936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007131051970645785 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01258810143917799\n",
      "Epoch 561: loss = 0.7385979294776917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "sites length BEFORE UPSAMPLING:  39848\n",
      "Skipping upsampling, too many sites, sites length:  39848 grid size:  32768\n",
      "eikonal_loss:  0.0006843060255050659 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01260936539620161\n",
      "Epoch 561: loss = 0.7258713245391846\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007390748360194266 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012512024492025375\n",
      "Epoch 562: loss = 0.6915011405944824\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010058165062218904 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012571562081575394\n",
      "Epoch 563: loss = 0.7004172205924988\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007061468786559999 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012526397593319416\n",
      "Epoch 564: loss = 0.7469344735145569\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012792726047337055 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012543228454887867\n",
      "Epoch 565: loss = 0.6951360702514648\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0018289729487150908 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012466850690543652\n",
      "Epoch 566: loss = 0.7054601907730103\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001182108768261969 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012465233914554119\n",
      "Epoch 567: loss = 0.7265142202377319\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012634200975298882 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012411346659064293\n",
      "Epoch 568: loss = 0.762648344039917\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0016712879296392202 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012353652156889439\n",
      "Epoch 569: loss = 0.7564345598220825\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.004248527344316244 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01226341724395752\n",
      "Epoch 570: loss = 0.7617104649543762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002167060738429427 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012258915230631828\n",
      "Epoch 571: loss = 0.7818129658699036\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002402006182819605 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012217896059155464\n",
      "Epoch 572: loss = 0.8049905300140381\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0016578731592744589 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012232935056090355\n",
      "Epoch 573: loss = 0.7880691885948181\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014103034045547247 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012310603633522987\n",
      "Epoch 574: loss = 0.8030955195426941\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010649459436535835 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012265652418136597\n",
      "Epoch 575: loss = 0.8286898136138916\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009813543874770403 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01232109870761633\n",
      "Epoch 576: loss = 0.8539208173751831\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010688016191124916 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012329586781561375\n",
      "Epoch 577: loss = 0.8019589185714722\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011184297036379576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012291543185710907\n",
      "Epoch 578: loss = 0.7789198160171509\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000995721435174346 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012250709347426891\n",
      "Epoch 579: loss = 0.8577402830123901\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001163467881269753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012208404950797558\n",
      "Epoch 580: loss = 0.8315615057945251\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009661696967668831 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012182573787868023\n",
      "Epoch 581: loss = 0.7911984920501709\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008240639581345022 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012263250537216663\n",
      "Epoch 582: loss = 0.8167451024055481\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008806466357782483 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012306293472647667\n",
      "Epoch 583: loss = 0.7907248735427856\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008270030375570059 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01225167978554964\n",
      "Epoch 584: loss = 0.8192354440689087\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008593949023634195 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01222261507064104\n",
      "Epoch 585: loss = 0.7572020888328552\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009213026496581733 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012265649624168873\n",
      "Epoch 586: loss = 0.8132921457290649\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001388479140587151 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012262066826224327\n",
      "Epoch 587: loss = 0.8551350831985474\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012985897483304143 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012272728607058525\n",
      "Epoch 588: loss = 0.783414900302887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014246006030589342 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012206739746034145\n",
      "Epoch 589: loss = 0.8264421224594116\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006273656035773456 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012257132679224014\n",
      "Epoch 590: loss = 0.8329761624336243\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007449939730577171 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012242211028933525\n",
      "Epoch 591: loss = 0.7861509323120117\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005930932238698006 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01222162414342165\n",
      "Epoch 592: loss = 0.7461174130439758\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005839706864207983 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012261435389518738\n",
      "Epoch 593: loss = 0.7652198076248169\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005770514253526926 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01241307146847248\n",
      "Epoch 594: loss = 0.775269091129303\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007533500902354717 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012418380007147789\n",
      "Epoch 595: loss = 0.7233490347862244\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007551645394414663 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012469763867557049\n",
      "Epoch 596: loss = 0.7689899206161499\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007214455981738865 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012562152929604053\n",
      "Epoch 597: loss = 0.7528268098831177\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006342652486637235 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012531988322734833\n",
      "Epoch 598: loss = 0.7599678635597229\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005747986724600196 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01269957609474659\n",
      "Epoch 599: loss = 0.6874958276748657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000559232837986201 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0126720005646348\n",
      "Epoch 600: loss = 0.6695471405982971\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000568850024137646 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0126997921615839\n",
      "Epoch 601: loss = 0.6348211765289307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000504477764479816 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012682752683758736\n",
      "Epoch 602: loss = 0.6478666067123413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005770649877376854 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012667830102145672\n",
      "Epoch 603: loss = 0.6970614194869995\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000947699008975178 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012643350288271904\n",
      "Epoch 604: loss = 0.6856531500816345\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0012305726995691657 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012628449127078056\n",
      "Epoch 605: loss = 0.6619988679885864\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0025387341156601906 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012695151381194592\n",
      "Epoch 606: loss = 0.6477652788162231\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024540210142731667 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012663881294429302\n",
      "Epoch 607: loss = 0.7024248838424683\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0024680490605533123 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012623981572687626\n",
      "Epoch 608: loss = 0.6932876110076904\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0020298033487051725 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012664137408137321\n",
      "Epoch 609: loss = 0.714255690574646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0020896904170513153 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012625813484191895\n",
      "Epoch 610: loss = 0.695649266242981\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0021269398275762796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012548522092401981\n",
      "Epoch 611: loss = 0.6785447597503662\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002272901125252247 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012509265914559364\n",
      "Epoch 612: loss = 0.6824724674224854\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002271887380629778 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01254773698747158\n",
      "Epoch 613: loss = 0.6910573840141296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0019162398530170321 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01259728241711855\n",
      "Epoch 614: loss = 0.673708438873291\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0023590996861457825 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012639657594263554\n",
      "Epoch 615: loss = 0.6625092625617981\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001692354679107666 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012696681544184685\n",
      "Epoch 616: loss = 0.6261079907417297\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001527588814496994 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012759748846292496\n",
      "Epoch 617: loss = 0.6097827553749084\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0015626142267137766 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012786351144313812\n",
      "Epoch 618: loss = 0.6126626133918762\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014219252625480294 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012814355082809925\n",
      "Epoch 619: loss = 0.6256718039512634\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0013367895735427737 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012811008840799332\n",
      "Epoch 620: loss = 0.6425099968910217\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014185530599206686 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012749242596328259\n",
      "Epoch 621: loss = 0.6060716509819031\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014404852408915758 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012708155438303947\n",
      "Epoch 622: loss = 0.6281439661979675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001240844838321209 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012713721953332424\n",
      "Epoch 623: loss = 0.5968817472457886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009731852915138006 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012670956552028656\n",
      "Epoch 624: loss = 0.5959955453872681\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009442406008020043 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012633330188691616\n",
      "Epoch 625: loss = 0.6144368648529053\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008751283749006689 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012632484547793865\n",
      "Epoch 626: loss = 0.6387770771980286\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009213056764565408 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012656604871153831\n",
      "Epoch 627: loss = 0.6450752019882202\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009234333992935717 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012673172168433666\n",
      "Epoch 628: loss = 0.5450154542922974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000872208911459893 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012648354284465313\n",
      "Epoch 629: loss = 0.5679816007614136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008615559199824929 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012623392045497894\n",
      "Epoch 630: loss = 0.5514426827430725\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000899347651284188 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012605976313352585\n",
      "Epoch 631: loss = 0.597754180431366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009623832884244621 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012547740712761879\n",
      "Epoch 632: loss = 0.5913768410682678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008211326785385609 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012535697780549526\n",
      "Epoch 633: loss = 0.6506521105766296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007698990521021187 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012523352168500423\n",
      "Epoch 634: loss = 0.6040326952934265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007400813628919423 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012475988827645779\n",
      "Epoch 635: loss = 0.6200371384620667\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000759393151383847 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012451628223061562\n",
      "Epoch 636: loss = 0.5832636952400208\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007272509392350912 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012460067868232727\n",
      "Epoch 637: loss = 0.554226279258728\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007014331640675664 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012464637868106365\n",
      "Epoch 638: loss = 0.5722368359565735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007044267258606851 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012442905455827713\n",
      "Epoch 639: loss = 0.5758152008056641\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005936400848440826 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012409646064043045\n",
      "Epoch 640: loss = 0.5759555101394653\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005503126885741949 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012414324097335339\n",
      "Epoch 641: loss = 0.6077539324760437\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005966663011349738 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012509244494140148\n",
      "Epoch 642: loss = 0.5713062882423401\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005678643356077373 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012487596832215786\n",
      "Epoch 643: loss = 0.5407230257987976\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005609229556284845 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012517445720732212\n",
      "Epoch 644: loss = 0.596019983291626\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005471730837598443 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012597724795341492\n",
      "Epoch 645: loss = 0.5465406775474548\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005309350090101361 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012625732459127903\n",
      "Epoch 646: loss = 0.5746780633926392\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000512017635628581 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012589875608682632\n",
      "Epoch 647: loss = 0.5287423133850098\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00048082112334668636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012602382339537144\n",
      "Epoch 648: loss = 0.580811619758606\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005285323713906109 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012670906260609627\n",
      "Epoch 649: loss = 0.5144380331039429\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005342788645066321 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012687519192695618\n",
      "Epoch 650: loss = 0.5900696516036987\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000539448345080018 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012790901586413383\n",
      "Epoch 651: loss = 0.5284309983253479\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000563792185857892 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012718656100332737\n",
      "Epoch 652: loss = 0.5457024574279785\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005541024729609489 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012616180814802647\n",
      "Epoch 653: loss = 0.5563454031944275\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005058387760072947 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01267135888338089\n",
      "Epoch 654: loss = 0.5207965970039368\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006887282943353057 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012721607461571693\n",
      "Epoch 655: loss = 0.531639575958252\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011200918816030025 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012740561738610268\n",
      "Epoch 656: loss = 0.5131389498710632\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010925786336883903 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012720901519060135\n",
      "Epoch 657: loss = 0.5681690573692322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010512863518670201 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012835909612476826\n",
      "Epoch 658: loss = 0.5678868889808655\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010649616597220302 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01280530821532011\n",
      "Epoch 659: loss = 0.5694708228111267\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0010518803028389812 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012854795902967453\n",
      "Epoch 660: loss = 0.5954587459564209\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.001016518217511475 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012805541977286339\n",
      "Epoch 661: loss = 0.6169719099998474\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000998260686174035 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012758797034621239\n",
      "Epoch 662: loss = 0.6018744111061096\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0009400038979947567 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012707889080047607\n",
      "Epoch 663: loss = 0.6183991432189941\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000830588978715241 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012777787633240223\n",
      "Epoch 664: loss = 0.633868932723999\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007932719308882952 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012749706394970417\n",
      "Epoch 665: loss = 0.5876869559288025\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007048859260976315 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012705405242741108\n",
      "Epoch 666: loss = 0.576603889465332\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000703164841979742 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012710241600871086\n",
      "Epoch 667: loss = 0.5859033465385437\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007340885931625962 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012635232880711555\n",
      "Epoch 668: loss = 0.5585154294967651\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0008212653920054436 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012671215459704399\n",
      "Epoch 669: loss = 0.5451756119728088\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007579316152259707 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012689168564975262\n",
      "Epoch 670: loss = 0.551615834236145\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007238987018354237 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012744693085551262\n",
      "Epoch 671: loss = 0.5846233367919922\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006601556669920683 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012768330983817577\n",
      "Epoch 672: loss = 0.5754827260971069\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006160198827274144 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01273253746330738\n",
      "Epoch 673: loss = 0.5709178447723389\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004856529994867742 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01270022802054882\n",
      "Epoch 674: loss = 0.5528297424316406\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00048240070464089513 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012659610249102116\n",
      "Epoch 675: loss = 0.5418755412101746\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004771916719619185 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012694055214524269\n",
      "Epoch 676: loss = 0.5345327854156494\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00048805878031998873 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012729602865874767\n",
      "Epoch 677: loss = 0.540697455406189\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004482431977521628 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012766290456056595\n",
      "Epoch 678: loss = 0.5461608171463013\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00045396326459012926 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012746861204504967\n",
      "Epoch 679: loss = 0.5077131390571594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004420786281116307 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012760152108967304\n",
      "Epoch 680: loss = 0.5220670700073242\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004331825184635818 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012787146493792534\n",
      "Epoch 681: loss = 0.5067519545555115\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004256365355104208 motion_loss: \n",
      "smoothed_heaviside_loss:  0.012866595759987831\n",
      "Epoch 682: loss = 0.5187370181083679\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004114002804271877 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013147717341780663\n",
      "Epoch 683: loss = 0.5390536189079285\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004466618411242962 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013193309307098389\n",
      "Epoch 684: loss = 0.49377548694610596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004369762900751084 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013177674263715744\n",
      "Epoch 685: loss = 0.5090251564979553\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004440643242560327 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01319311186671257\n",
      "Epoch 686: loss = 0.5140767693519592\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038722355384379625 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01322393212467432\n",
      "Epoch 687: loss = 0.4917542338371277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036537632695399225 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013227438554167747\n",
      "Epoch 688: loss = 0.48653942346572876\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035422871587798 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01322909165173769\n",
      "Epoch 689: loss = 0.497911274433136\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034286396112293005 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013209962286055088\n",
      "Epoch 690: loss = 0.4959428012371063\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039840955287218094 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013193827122449875\n",
      "Epoch 691: loss = 0.4872899651527405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003692956524901092 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013280965387821198\n",
      "Epoch 692: loss = 0.5256460309028625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003510915848892182 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013373762369155884\n",
      "Epoch 693: loss = 0.49303215742111206\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003895621048286557 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01343020610511303\n",
      "Epoch 694: loss = 0.5079337954521179\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003849428321700543 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01340418215841055\n",
      "Epoch 695: loss = 0.48035502433776855\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003779553226195276 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01348732691258192\n",
      "Epoch 696: loss = 0.4849151074886322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036475202068686485 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013590067625045776\n",
      "Epoch 697: loss = 0.46326765418052673\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038595334626734257 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01356000266969204\n",
      "Epoch 698: loss = 0.4560272991657257\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006287526921369135 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013735760003328323\n",
      "Epoch 699: loss = 0.46553370356559753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006226563709788024 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01378614827990532\n",
      "Epoch 700: loss = 0.4499051868915558\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000412930065067485 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013752241618931293\n",
      "Epoch 701: loss = 0.5037869811058044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005198352155275643 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013734482228755951\n",
      "Epoch 702: loss = 0.4677185118198395\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005147416959516704 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013761135749518871\n",
      "Epoch 703: loss = 0.4624021649360657\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005302723147906363 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01380366925150156\n",
      "Epoch 704: loss = 0.49045464396476746\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004939734353683889 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013755414634943008\n",
      "Epoch 705: loss = 0.5268459916114807\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000395672075683251 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013674268499016762\n",
      "Epoch 706: loss = 0.5126825571060181\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043924746569246054 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013606101274490356\n",
      "Epoch 707: loss = 0.5408787131309509\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036935394746251404 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013615408912301064\n",
      "Epoch 708: loss = 0.5081483125686646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00037573761073872447 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01362014189362526\n",
      "Epoch 709: loss = 0.5263754725456238\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038078363286331296 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013673573732376099\n",
      "Epoch 710: loss = 0.5345996022224426\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004275320970918983 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01368832029402256\n",
      "Epoch 711: loss = 0.498485803604126\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00040020691812969744 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01364001352339983\n",
      "Epoch 712: loss = 0.5061233639717102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00042324274545535445 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013635336421430111\n",
      "Epoch 713: loss = 0.5175540447235107\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003890347434207797 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013557624071836472\n",
      "Epoch 714: loss = 0.5856095552444458\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004161172255408019 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013540149666368961\n",
      "Epoch 715: loss = 0.553906261920929\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004114430630579591 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013565942645072937\n",
      "Epoch 716: loss = 0.5193848609924316\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004291797522455454 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013541923835873604\n",
      "Epoch 717: loss = 0.5195443034172058\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043379078852012753 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013609645888209343\n",
      "Epoch 718: loss = 0.4920039772987366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043181495857425034 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01358900498598814\n",
      "Epoch 719: loss = 0.48704975843429565\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043228239519521594 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013547249138355255\n",
      "Epoch 720: loss = 0.4919550120830536\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003940330061595887 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013497770763933659\n",
      "Epoch 721: loss = 0.49612513184547424\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003911361563950777 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013470024801790714\n",
      "Epoch 722: loss = 0.5504632592201233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038498424692079425 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013425802811980247\n",
      "Epoch 723: loss = 0.5425921082496643\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038227011100389063 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013379559852182865\n",
      "Epoch 724: loss = 0.5322398543357849\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00037993810838088393 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013347761705517769\n",
      "Epoch 725: loss = 0.536786675453186\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003895075060427189 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013378467410802841\n",
      "Epoch 726: loss = 0.5063426494598389\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039982254384085536 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013486796990036964\n",
      "Epoch 727: loss = 0.5039588809013367\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004169940366409719 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013417486101388931\n",
      "Epoch 728: loss = 0.5341864824295044\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004002192581538111 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013441224582493305\n",
      "Epoch 729: loss = 0.50409334897995\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003790554474107921 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013403229415416718\n",
      "Epoch 730: loss = 0.49040719866752625\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00037550891283899546 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013354499824345112\n",
      "Epoch 731: loss = 0.47977280616760254\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003809532499872148 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013384186662733555\n",
      "Epoch 732: loss = 0.47658222913742065\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039409034070558846 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013354001566767693\n",
      "Epoch 733: loss = 0.4792901575565338\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003791263443417847 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01335639227181673\n",
      "Epoch 734: loss = 0.45438897609710693\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000373433023924008 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01341051235795021\n",
      "Epoch 735: loss = 0.4532383382320404\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036297127371653914 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013409510254859924\n",
      "Epoch 736: loss = 0.45685073733329773\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035716505954042077 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013391882181167603\n",
      "Epoch 737: loss = 0.45217734575271606\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003516646393109113 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013456319458782673\n",
      "Epoch 738: loss = 0.5045733451843262\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004900546628050506 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013522516936063766\n",
      "Epoch 739: loss = 0.47034624218940735\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004738346906378865 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013511594384908676\n",
      "Epoch 740: loss = 0.46218448877334595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004964240361005068 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013563228771090508\n",
      "Epoch 741: loss = 0.47737789154052734\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00046475225826725364 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013538996689021587\n",
      "Epoch 742: loss = 0.4502786099910736\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043758758693002164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013519691303372383\n",
      "Epoch 743: loss = 0.5143082141876221\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004356592835392803 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013592205941677094\n",
      "Epoch 744: loss = 0.47027555108070374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00042006338480859995 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013553721830248833\n",
      "Epoch 745: loss = 0.4438983201980591\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00040497188456356525 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013550950214266777\n",
      "Epoch 746: loss = 0.45330479741096497\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003882857272401452 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013571565970778465\n",
      "Epoch 747: loss = 0.47759810090065\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003759577521122992 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013558490201830864\n",
      "Epoch 748: loss = 0.4705735445022583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003731042379513383 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013545768335461617\n",
      "Epoch 749: loss = 0.4742380976676941\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036885004374198616 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013539721257984638\n",
      "Epoch 750: loss = 0.4619694650173187\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000372698821593076 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013424346223473549\n",
      "Epoch 751: loss = 0.45410671830177307\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003625454264692962 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013421524316072464\n",
      "Epoch 752: loss = 0.47869256138801575\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035574816865846515 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013231265358626842\n",
      "Epoch 753: loss = 0.47839242219924927\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035153047065250576 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013282930478453636\n",
      "Epoch 754: loss = 0.5224670171737671\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034619017969816923 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01341351866722107\n",
      "Epoch 755: loss = 0.5381571650505066\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000363076658686623 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01347771193832159\n",
      "Epoch 756: loss = 0.5659064650535583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003430630313232541 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01346531230956316\n",
      "Epoch 757: loss = 0.5554748773574829\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003402087604627013 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013519729487597942\n",
      "Epoch 758: loss = 0.5231990814208984\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032296814606525004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013588083907961845\n",
      "Epoch 759: loss = 0.5160949230194092\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032331107649952173 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013506020419299603\n",
      "Epoch 760: loss = 0.5056526064872742\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003373699728399515 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013559691607952118\n",
      "Epoch 761: loss = 0.4971522390842438\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003364587319083512 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013526380062103271\n",
      "Epoch 762: loss = 0.5160204768180847\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003464585461188108 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013490608893334866\n",
      "Epoch 763: loss = 0.5377333760261536\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034622475504875183 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01346964668482542\n",
      "Epoch 764: loss = 0.5356094241142273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034528839751146734 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01336608361452818\n",
      "Epoch 765: loss = 0.5530509352684021\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003485782945062965 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013344557955861092\n",
      "Epoch 766: loss = 0.48402950167655945\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034773797960951924 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01337187085300684\n",
      "Epoch 767: loss = 0.5121251344680786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035425429814495146 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01345470268279314\n",
      "Epoch 768: loss = 0.46829545497894287\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038496829802170396 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013612248934805393\n",
      "Epoch 769: loss = 0.43056848645210266\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004169597814325243 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013712028972804546\n",
      "Epoch 770: loss = 0.42028892040252686\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004243178409524262 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013748612254858017\n",
      "Epoch 771: loss = 0.41669127345085144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00047224550507962704 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01375637948513031\n",
      "Epoch 772: loss = 0.4241470396518707\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00045656447764486074 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013780423440039158\n",
      "Epoch 773: loss = 0.42505210638046265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006540543981827796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013774055987596512\n",
      "Epoch 774: loss = 0.39966607093811035\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0021373573690652847 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013700629584491253\n",
      "Epoch 775: loss = 0.3975466191768646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00203796592541039 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013878731988370419\n",
      "Epoch 776: loss = 0.3833000361919403\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0022295729722827673 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013848495669662952\n",
      "Epoch 777: loss = 0.3997710049152374\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.002020528307184577 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013888967223465443\n",
      "Epoch 778: loss = 0.4157928228378296\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017066417494788766 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013864687643945217\n",
      "Epoch 779: loss = 0.3947449028491974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0017347860848531127 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013843272812664509\n",
      "Epoch 780: loss = 0.4058118462562561\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0014628428034484386 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013776988722383976\n",
      "Epoch 781: loss = 0.3779946565628052\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0011323484359309077 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013708473183214664\n",
      "Epoch 782: loss = 0.4074075222015381\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007773994002491236 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013705486431717873\n",
      "Epoch 783: loss = 0.4347366988658905\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007187110022641718 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013683968223631382\n",
      "Epoch 784: loss = 0.4219359755516052\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006663301610387862 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013532963581383228\n",
      "Epoch 785: loss = 0.40625348687171936\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006174416048452258 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013449208810925484\n",
      "Epoch 786: loss = 0.4117562472820282\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006384172011166811 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013537552207708359\n",
      "Epoch 787: loss = 0.4379970133304596\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006010535289533436 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01363121997565031\n",
      "Epoch 788: loss = 0.4124905467033386\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007034146692603827 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013676929287612438\n",
      "Epoch 789: loss = 0.39812371134757996\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005950517952442169 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01360328495502472\n",
      "Epoch 790: loss = 0.41729018092155457\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005642547621391714 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013777399435639381\n",
      "Epoch 791: loss = 0.4497179388999939\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004963805549778044 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013858761638402939\n",
      "Epoch 792: loss = 0.43817856907844543\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004951601731590927 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01383817009627819\n",
      "Epoch 793: loss = 0.4376702606678009\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004652411735150963 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013768987730145454\n",
      "Epoch 794: loss = 0.42471763491630554\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004487074329517782 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013824354857206345\n",
      "Epoch 795: loss = 0.41600942611694336\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004390738904476166 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013761861249804497\n",
      "Epoch 796: loss = 0.4289230704307556\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000425345468102023 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013708710670471191\n",
      "Epoch 797: loss = 0.44588539004325867\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00044459354830905795 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013705137185752392\n",
      "Epoch 798: loss = 0.4533027410507202\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004545019764918834 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013663734309375286\n",
      "Epoch 799: loss = 0.43938013911247253\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005660677561536431 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013708940707147121\n",
      "Epoch 800: loss = 0.4668534994125366\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005463536363095045 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013672161847352982\n",
      "Epoch 801: loss = 0.46999919414520264\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005251041147857904 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013778671622276306\n",
      "Epoch 802: loss = 0.43982210755348206\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005099773406982422 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013754838146269321\n",
      "Epoch 803: loss = 0.45392173528671265\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000478506030049175 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01384600531309843\n",
      "Epoch 804: loss = 0.43371060490608215\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004673640360124409 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013841690495610237\n",
      "Epoch 805: loss = 0.429009348154068\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043444393668323755 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013836543075740337\n",
      "Epoch 806: loss = 0.40754541754722595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004212424682918936 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013850907795131207\n",
      "Epoch 807: loss = 0.42139917612075806\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004152340115979314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01392399612814188\n",
      "Epoch 808: loss = 0.4050540030002594\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039888510946184397 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013964899815618992\n",
      "Epoch 809: loss = 0.4606720805168152\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039376303902827203 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013978858478367329\n",
      "Epoch 810: loss = 0.40239977836608887\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00038500558002851903 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0140726612880826\n",
      "Epoch 811: loss = 0.44250911474227905\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003789035836234689 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014075948856770992\n",
      "Epoch 812: loss = 0.40438997745513916\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003893328830599785 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014079423621296883\n",
      "Epoch 813: loss = 0.41362643241882324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003827800101134926 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014101496897637844\n",
      "Epoch 814: loss = 0.4188206195831299\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00037728046299889684 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014053162187337875\n",
      "Epoch 815: loss = 0.416404128074646\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003765133151318878 motion_loss: \n",
      "smoothed_heaviside_loss:  0.014013164676725864\n",
      "Epoch 816: loss = 0.39157798886299133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003703738912008703 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013981934636831284\n",
      "Epoch 817: loss = 0.40463387966156006\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003661267110146582 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013945305719971657\n",
      "Epoch 818: loss = 0.3872518241405487\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003617163165472448 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013933212496340275\n",
      "Epoch 819: loss = 0.40099474787712097\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035815348383039236 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01392319519072771\n",
      "Epoch 820: loss = 0.39446568489074707\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035704561742022634 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013895549811422825\n",
      "Epoch 821: loss = 0.40473833680152893\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003648615092970431 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013847677037119865\n",
      "Epoch 822: loss = 0.38707438111305237\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003646587720140815 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013812554068863392\n",
      "Epoch 823: loss = 0.40028971433639526\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036228736280463636 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013751438818871975\n",
      "Epoch 824: loss = 0.4103495180606842\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036305803223513067 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013807368464767933\n",
      "Epoch 825: loss = 0.3749390244483948\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003480742161627859 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013730376958847046\n",
      "Epoch 826: loss = 0.3583274781703949\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003387677716091275 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013694226741790771\n",
      "Epoch 827: loss = 0.3683083951473236\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000336674100253731 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013614439405500889\n",
      "Epoch 828: loss = 0.3729378581047058\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032973510678857565 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01359808910638094\n",
      "Epoch 829: loss = 0.3808877766132355\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003293511108495295 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013527166098356247\n",
      "Epoch 830: loss = 0.3523956835269928\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003299119707662612 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013531343080103397\n",
      "Epoch 831: loss = 0.3303838074207306\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032695967820473015 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013417134992778301\n",
      "Epoch 832: loss = 0.33088603615760803\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032341587939299643 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013377649709582329\n",
      "Epoch 833: loss = 0.33701789379119873\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003232936724089086 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013391481712460518\n",
      "Epoch 834: loss = 0.37771475315093994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003187860129401088 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013398640789091587\n",
      "Epoch 835: loss = 0.3608088195323944\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003154243458993733 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013386174105107784\n",
      "Epoch 836: loss = 0.3737528622150421\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032046291744336486 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013362113386392593\n",
      "Epoch 837: loss = 0.39586377143859863\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031893287086859345 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01343923807144165\n",
      "Epoch 838: loss = 0.3785824477672577\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031702147680334747 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013431115075945854\n",
      "Epoch 839: loss = 0.3654336929321289\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031557996408082545 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01342692133039236\n",
      "Epoch 840: loss = 0.3815559148788452\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003187512047588825 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013391063548624516\n",
      "Epoch 841: loss = 0.39949196577072144\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003221249207854271 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013412072323262691\n",
      "Epoch 842: loss = 0.43400153517723083\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003180365019943565 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013409163802862167\n",
      "Epoch 843: loss = 0.41012752056121826\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031682936241850257 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01340258214622736\n",
      "Epoch 844: loss = 0.3997754752635956\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003174506709910929 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013351504690945148\n",
      "Epoch 845: loss = 0.3992260694503784\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003179456980433315 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01343577727675438\n",
      "Epoch 846: loss = 0.4290609359741211\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031543668592348695 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013397537171840668\n",
      "Epoch 847: loss = 0.44433465600013733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031764942104928195 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013355942443013191\n",
      "Epoch 848: loss = 0.4405130445957184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004544473486021161 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013386470265686512\n",
      "Epoch 849: loss = 0.41658908128738403\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00039596849819645286 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013446246273815632\n",
      "Epoch 850: loss = 0.4674842953681946\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004044492088723928 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013402505777776241\n",
      "Epoch 851: loss = 0.49093693494796753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00040766780148260295 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013356857001781464\n",
      "Epoch 852: loss = 0.4605642259120941\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006268048309721053 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01335128489881754\n",
      "Epoch 853: loss = 0.40527862310409546\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000397940311813727 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013327685184776783\n",
      "Epoch 854: loss = 0.42884954810142517\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003927460638806224 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01330757886171341\n",
      "Epoch 855: loss = 0.44107821583747864\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035715402918867767 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01333346962928772\n",
      "Epoch 856: loss = 0.43282845616340637\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034880623570643365 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013444839045405388\n",
      "Epoch 857: loss = 0.4594591557979584\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035029451828449965 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013457330875098705\n",
      "Epoch 858: loss = 0.4061554968357086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034619992948137224 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013540051877498627\n",
      "Epoch 859: loss = 0.4207199215888977\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003433746169321239 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01354604959487915\n",
      "Epoch 860: loss = 0.4034022092819214\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003671463346108794 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013522351160645485\n",
      "Epoch 861: loss = 0.39865511655807495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004876422171946615 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013548566959798336\n",
      "Epoch 862: loss = 0.42081671953201294\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004576998180709779 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013536731712520123\n",
      "Epoch 863: loss = 0.4313800632953644\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004336304555181414 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013549530878663063\n",
      "Epoch 864: loss = 0.4040619432926178\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004348013608250767 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013554779812693596\n",
      "Epoch 865: loss = 0.4629582166671753\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00041374092688784003 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013540777377784252\n",
      "Epoch 866: loss = 0.4715082049369812\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004468908009584993 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01349005475640297\n",
      "Epoch 867: loss = 0.4684378504753113\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004204532888252288 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013448559679090977\n",
      "Epoch 868: loss = 0.4791933298110962\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003914968983735889 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013406263664364815\n",
      "Epoch 869: loss = 0.450229287147522\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004039533087052405 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013362995348870754\n",
      "Epoch 870: loss = 0.43603813648223877\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00041345294448547065 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013329696841537952\n",
      "Epoch 871: loss = 0.4204738736152649\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000575924408622086 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013386394828557968\n",
      "Epoch 872: loss = 0.427529513835907\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006090996321290731 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013419976457953453\n",
      "Epoch 873: loss = 0.4077438414096832\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005997791886329651 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013423016294836998\n",
      "Epoch 874: loss = 0.4389234185218811\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006592512945644557 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013478836044669151\n",
      "Epoch 875: loss = 0.4524427652359009\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000648146029561758 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013524781912565231\n",
      "Epoch 876: loss = 0.45378538966178894\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0006188657134771347 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013475279323756695\n",
      "Epoch 877: loss = 0.45896297693252563\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005039905081503093 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013421844691038132\n",
      "Epoch 878: loss = 0.4751105308532715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00048317742766812444 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0133968535810709\n",
      "Epoch 879: loss = 0.44479236006736755\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004948436398990452 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013410838320851326\n",
      "Epoch 880: loss = 0.451309472322464\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0005392667953856289 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013410803861916065\n",
      "Epoch 881: loss = 0.471095472574234\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004842315101996064 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01338186301290989\n",
      "Epoch 882: loss = 0.4693727493286133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004634633951354772 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013338848017156124\n",
      "Epoch 883: loss = 0.44499337673187256\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004771531675942242 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01335437223315239\n",
      "Epoch 884: loss = 0.4501892924308777\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00045419076923280954 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0133409658446908\n",
      "Epoch 885: loss = 0.47035306692123413\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00047372374683618546 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013343943282961845\n",
      "Epoch 886: loss = 0.457080602645874\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004481695359572768 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01333307009190321\n",
      "Epoch 887: loss = 0.4776586592197418\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004583570989780128 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013314024545252323\n",
      "Epoch 888: loss = 0.4796064496040344\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004339718434493989 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01329809706658125\n",
      "Epoch 889: loss = 0.42020294070243835\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00042887928429991007 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0132469916716218\n",
      "Epoch 890: loss = 0.43063145875930786\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00042527151526883245 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0134182870388031\n",
      "Epoch 891: loss = 0.4304691553115845\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003781419654842466 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013465707190334797\n",
      "Epoch 892: loss = 0.4231155514717102\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003725191636476666 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01341648492962122\n",
      "Epoch 893: loss = 0.4030672013759613\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036541823646984994 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013443972915410995\n",
      "Epoch 894: loss = 0.4333796799182892\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036752771120518446 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013430244289338589\n",
      "Epoch 895: loss = 0.3997955918312073\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003796665114350617 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013469069264829159\n",
      "Epoch 896: loss = 0.4027674198150635\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035836989991366863 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013454446569085121\n",
      "Epoch 897: loss = 0.38803452253341675\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036593980621546507 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013443762436509132\n",
      "Epoch 898: loss = 0.417644202709198\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003837613621726632 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01361288595944643\n",
      "Epoch 899: loss = 0.3995998501777649\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003607339458540082 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013566420413553715\n",
      "Epoch 900: loss = 0.3938276469707489\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035591894993558526 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013529233634471893\n",
      "Epoch 901: loss = 0.41955187916755676\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003498340956866741 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013444449752569199\n",
      "Epoch 902: loss = 0.41494065523147583\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003391761565580964 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013452554121613503\n",
      "Epoch 903: loss = 0.3873031735420227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003412460209801793 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013475107029080391\n",
      "Epoch 904: loss = 0.3777309060096741\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032742408802732825 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013432823121547699\n",
      "Epoch 905: loss = 0.4074574112892151\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032249331707134843 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013398746959865093\n",
      "Epoch 906: loss = 0.39464202523231506\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003163717337884009 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013423792086541653\n",
      "Epoch 907: loss = 0.40554866194725037\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003128985408693552 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013470092788338661\n",
      "Epoch 908: loss = 0.4067877531051636\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031080751796253026 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013429203070700169\n",
      "Epoch 909: loss = 0.4190417528152466\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034544066875241697 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013531121425330639\n",
      "Epoch 910: loss = 0.3849596381187439\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034365837927907705 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01355247013270855\n",
      "Epoch 911: loss = 0.3918270170688629\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003129126853309572 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013595146127045155\n",
      "Epoch 912: loss = 0.4009800851345062\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000309264927636832 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013663290068507195\n",
      "Epoch 913: loss = 0.39729711413383484\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003078035661019385 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013658085837960243\n",
      "Epoch 914: loss = 0.3958972692489624\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030859443359076977 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013630791567265987\n",
      "Epoch 915: loss = 0.40852493047714233\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003096883883699775 motion_loss: \n",
      "smoothed_heaviside_loss:  0.0137323047965765\n",
      "Epoch 916: loss = 0.42088061571121216\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030982133466750383 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013725519180297852\n",
      "Epoch 917: loss = 0.3928963541984558\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003301183460280299 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013660973869264126\n",
      "Epoch 918: loss = 0.4069979190826416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003076189022976905 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013682001270353794\n",
      "Epoch 919: loss = 0.42315414547920227\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030658027390018106 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013593711890280247\n",
      "Epoch 920: loss = 0.4151413142681122\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003086055221501738 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013559004291892052\n",
      "Epoch 921: loss = 0.38117343187332153\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031220001983456314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013528394512832165\n",
      "Epoch 922: loss = 0.3670547306537628\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031285398290492594 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013539781793951988\n",
      "Epoch 923: loss = 0.3729923367500305\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003145512891933322 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013510327786207199\n",
      "Epoch 924: loss = 0.3699599504470825\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000370452442439273 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01348838396370411\n",
      "Epoch 925: loss = 0.36886540055274963\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00040788186015561223 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013459108769893646\n",
      "Epoch 926: loss = 0.36142149567604065\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003883680619765073 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01355787180364132\n",
      "Epoch 927: loss = 0.35795116424560547\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00034265726571902633 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01354458462446928\n",
      "Epoch 928: loss = 0.3542545437812805\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0007030082633718848 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013512241654098034\n",
      "Epoch 929: loss = 0.38682985305786133\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004595680511556566 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013625702820718288\n",
      "Epoch 930: loss = 0.3773006498813629\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003677122003864497 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013636372052133083\n",
      "Epoch 931: loss = 0.4206567108631134\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003543751663528383 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013657324016094208\n",
      "Epoch 932: loss = 0.38586747646331787\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000330752634909004 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013664815574884415\n",
      "Epoch 933: loss = 0.37707003951072693\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00033024471485987306 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01365883368998766\n",
      "Epoch 934: loss = 0.3737971782684326\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003249862347729504 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013690726831555367\n",
      "Epoch 935: loss = 0.39298370480537415\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003199412894900888 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013655370101332664\n",
      "Epoch 936: loss = 0.36837077140808105\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003192478325217962 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013703462667763233\n",
      "Epoch 937: loss = 0.3807235658168793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003173376899212599 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013661189936101437\n",
      "Epoch 938: loss = 0.38069847226142883\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030784038244746625 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013657585717737675\n",
      "Epoch 939: loss = 0.40431761741638184\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003210637951269746 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013759895227849483\n",
      "Epoch 940: loss = 0.42152321338653564\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003190407296642661 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013775316067039967\n",
      "Epoch 941: loss = 0.42238378524780273\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00032170169288292527 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013718308880925179\n",
      "Epoch 942: loss = 0.4265790283679962\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003234980977140367 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013690393418073654\n",
      "Epoch 943: loss = 0.39378491044044495\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031913683051243424 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013622282072901726\n",
      "Epoch 944: loss = 0.383086621761322\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003132588171865791 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01374730933457613\n",
      "Epoch 945: loss = 0.42889782786369324\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003079265297856182 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01364822220057249\n",
      "Epoch 946: loss = 0.43693166971206665\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030289945425465703 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013705515302717686\n",
      "Epoch 947: loss = 0.4511551558971405\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030925715691410005 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01367736142128706\n",
      "Epoch 948: loss = 0.41671398282051086\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030423200223594904 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013719284906983376\n",
      "Epoch 949: loss = 0.39653754234313965\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00029243636527098715 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013758686371147633\n",
      "Epoch 950: loss = 0.3756225109100342\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002892998163588345 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013689961284399033\n",
      "Epoch 951: loss = 0.36509257555007935\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002876314101740718 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013734260573983192\n",
      "Epoch 952: loss = 0.40061619877815247\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028658739756792784 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013693426735699177\n",
      "Epoch 953: loss = 0.39771267771720886\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028590441797859967 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01364833116531372\n",
      "Epoch 954: loss = 0.4077979326248169\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002852387842722237 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013586965389549732\n",
      "Epoch 955: loss = 0.40462833642959595\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028500729240477085 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013507360592484474\n",
      "Epoch 956: loss = 0.38790997862815857\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002837242791429162 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013522857800126076\n",
      "Epoch 957: loss = 0.4139769673347473\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00027990882517769933 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013513969257473946\n",
      "Epoch 958: loss = 0.4124159812927246\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002795026812236756 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013543752953410149\n",
      "Epoch 959: loss = 0.40747976303100586\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002803234674502164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013502364978194237\n",
      "Epoch 960: loss = 0.40181872248649597\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028628011932596564 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013481100089848042\n",
      "Epoch 961: loss = 0.3938544690608978\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000296052050543949 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013468192890286446\n",
      "Epoch 962: loss = 0.3822038173675537\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030187025549821556 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013478594832122326\n",
      "Epoch 963: loss = 0.380764365196228\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003003515594173223 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013410920277237892\n",
      "Epoch 964: loss = 0.39806586503982544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00031007773941382766 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01334073580801487\n",
      "Epoch 965: loss = 0.40891534090042114\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003093226405326277 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01336435042321682\n",
      "Epoch 966: loss = 0.39277058839797974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003081766190007329 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013345843181014061\n",
      "Epoch 967: loss = 0.3860599994659424\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00030372885521501303 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013425319455564022\n",
      "Epoch 968: loss = 0.3716542720794678\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003090899554081261 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01355453860014677\n",
      "Epoch 969: loss = 0.38867059350013733\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.000307598733343184 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013503561727702618\n",
      "Epoch 970: loss = 0.3730420172214508\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00029036312480457127 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013381444849073887\n",
      "Epoch 971: loss = 0.3667941093444824\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002802792296279222 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013470284640789032\n",
      "Epoch 972: loss = 0.373050719499588\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002801074006129056 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013448046520352364\n",
      "Epoch 973: loss = 0.36139705777168274\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002805713447742164 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013427532278001308\n",
      "Epoch 974: loss = 0.3620361089706421\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002801115915644914 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013417127542197704\n",
      "Epoch 975: loss = 0.3906172513961792\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028085181838832796 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01337799709290266\n",
      "Epoch 976: loss = 0.40798819065093994\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002814853796735406 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013337415643036366\n",
      "Epoch 977: loss = 0.4011605381965637\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002834138576872647 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013324662111699581\n",
      "Epoch 978: loss = 0.39057180285453796\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00028378632850944996 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013234519399702549\n",
      "Epoch 979: loss = 0.388199120759964\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0002862851833924651 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013245511800050735\n",
      "Epoch 980: loss = 0.38285550475120544\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004203899297863245 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01323311310261488\n",
      "Epoch 981: loss = 0.3853934705257416\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004219332477077842 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013294115662574768\n",
      "Epoch 982: loss = 0.36893925070762634\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00033472280483692884 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013300128281116486\n",
      "Epoch 983: loss = 0.39972859621047974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00041733181569725275 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013307332061231136\n",
      "Epoch 984: loss = 0.3898176848888397\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004267234180588275 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01328076608479023\n",
      "Epoch 985: loss = 0.3690372407436371\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004214238724671304 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013334950432181358\n",
      "Epoch 986: loss = 0.3973982632160187\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004163272387813777 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013372722081840038\n",
      "Epoch 987: loss = 0.38460129499435425\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0004081179213244468 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013313853181898594\n",
      "Epoch 988: loss = 0.3772933781147003\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00040444533806294203 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01333128847181797\n",
      "Epoch 989: loss = 0.4041077494621277\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003993453283328563 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01337092649191618\n",
      "Epoch 990: loss = 0.39296573400497437\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003910722734872252 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013452542945742607\n",
      "Epoch 991: loss = 0.37514784932136536\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003878780407831073 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013452690094709396\n",
      "Epoch 992: loss = 0.3818154036998749\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036997636198066175 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013399898074567318\n",
      "Epoch 993: loss = 0.38442471623420715\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00036492268554866314 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013388772495090961\n",
      "Epoch 994: loss = 0.38655808568000793\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003570095868781209 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013375822454690933\n",
      "Epoch 995: loss = 0.399277925491333\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00035951181780546904 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013374989852309227\n",
      "Epoch 996: loss = 0.4049990177154541\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003764176508411765 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013351240195333958\n",
      "Epoch 997: loss = 0.3827499747276306\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003808256587944925 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013284595683217049\n",
      "Epoch 998: loss = 0.41398218274116516\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.0003866711922455579 motion_loss: \n",
      "smoothed_heaviside_loss:  0.013284699991345406\n",
      "Epoch 999: loss = 0.3933049440383911\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "eikonal_loss:  0.00043014908442273736 motion_loss: \n",
      "smoothed_heaviside_loss:  0.01317628100514412\n",
      "Epoch 1000: loss = 0.3678874373435974\n",
      "-----------------\n",
      "Learning rate:  0.0005\n",
      "Sites length:  39848\n",
      "min sites:  tensor(-1.0959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1.1011, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f\"{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy\"\n",
    "# check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    # import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "    # with torch.profiler.profile(activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=False,\n",
    "    #     with_stack=True  # Captures function calls\n",
    "    # ) as prof:\n",
    "    #     sites, optimized_sites_sdf = train_DCCVT(sites, sdf0, offset=None, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "    # prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # #\n",
    "    sites, optimized_sites_sdf = train_DCCVT(\n",
    "        sites, sdf0, max_iter=max_iter, upsampling=10, lambda_weights=lambda_weights\n",
    "    )\n",
    "\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf torch.Size([39848])\n",
      "sites ./images/autograd/End2End_DCCVT_interpolSDF/gargoyle_unconverged1000_1000_3d_sites_4096_chamfer1000.pth\n",
      "sites_np shape:  (39848, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "\n",
    "# model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "sdf_file_path = f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sdf_{num_centroids}_chamfer{lambda_chamfer}.pth\"\n",
    "\n",
    "\n",
    "sites = torch.load(site_file_path)\n",
    "sdf_v = torch.load(sdf_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "print(\"sdf\", sdf_v.shape)\n",
    "print(\"sites\", site_file_path)\n",
    "\n",
    "ps_cloud_f = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\", sites_np)\n",
    "ps_cloud_f.add_scalar_quantity(\n",
    "    \"vis_grid_pred\",\n",
    "    sdf_v.detach().cpu().numpy(),\n",
    "    enabled=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vminmax=(-0.15, 0.15),\n",
    ")\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "# print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "# remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Delaunay simplices...\n",
      "Computing Delaunay simplices...\n"
     ]
    }
   ],
   "source": [
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "# ps.register_surface_mesh(\"model final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "# ps.register_surface_mesh(\"model final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# if mesh[0] == \"sphere\":\n",
    "#     # generate sphere sdf\n",
    "#     print(\"Generating sphere SDF\")\n",
    "#     sdf_v = sphere_sdf(sites, torch.zeros(3).to(device), 0.50)\n",
    "\n",
    "\n",
    "(\n",
    "    v_vect,\n",
    "    f_vect,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = su.get_clipped_mesh_numba(sites, None, None, False, sdf_v, True)\n",
    "\n",
    "f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "ps.register_surface_mesh(\"sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf_v, True)\n",
    "f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "ps.register_surface_mesh(\"sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "\n",
    "\n",
    "# export obj file\n",
    "output_obj_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_outputmesh.obj\"\n",
    ")\n",
    "output_ply_file = (\n",
    "    f\"{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}_targetpointcloud.ply\"\n",
    ")\n",
    "su.save_obj(output_obj_file, v_vect.detach().cpu().numpy(), f_vect)\n",
    "su.save_target_pc_ply(output_ply_file, mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites, sdf = train_DCCVT(\n",
    "#     sites, sdf_v, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights, voroloss_optim=True\n",
    "# )\n",
    "# (\n",
    "#     v_vect,\n",
    "#     f_vect,\n",
    "#     _,\n",
    "#     _,\n",
    "#     _,\n",
    "# ) = su.get_clipped_mesh_numba(sites, None, None, False, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final unclipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "\n",
    "# v_vect, f_vect, _, _, _ = su.get_clipped_mesh_numba(sites, None, None, True, sdf, True)\n",
    "# ps.register_surface_mesh(\"voromeh sdf final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # f_vect = [[f[0], f[i], f[i + 1]] for f in f_vect for i in range(1, len(f) - 1)]\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5067aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a71d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/autograd/End2End_DCCVT_interpolSDF/gargoyle_unconverged1000_1000_3d_sites_4096_chamfer1000_outputmesh.obj\n",
      "/home/wylliam/dev/Kyushu_experiments/mesh/gargoyle_unconverged.obj\n",
      "Chamfer Accuracy (Ours  GT): 0.002889\n",
      "Chamfer Completeness (GT  Ours): 0.001524\n",
      "Chamfer Distance (symmetric): 0.004413\n"
     ]
    }
   ],
   "source": [
    "# chamfer metric\n",
    "# add sampled points to polyscope and ground truth mesh to polyscope\n",
    "\n",
    "import trimesh\n",
    "\n",
    "\n",
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    # normalize mesh\n",
    "    mesh.apply_translation(-mesh.centroid)\n",
    "    mesh.apply_scale(1.0 / np.max(np.abs(mesh.vertices)))\n",
    "    # export mesh to obj file\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "    print(mesh_path)\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def chamfer_accuracy_completeness(ours_pts, gt_pts):\n",
    "    # Completeness: GT  Ours\n",
    "    dists_gt_to_ours = cKDTree(ours_pts).query(gt_pts, k=1)[0]\n",
    "    completeness = np.mean(dists_gt_to_ours**2)\n",
    "\n",
    "    # Accuracy: Ours  GT\n",
    "    dists_ours_to_gt = cKDTree(gt_pts).query(ours_pts, k=1)[0]\n",
    "    accuracy = np.mean(dists_ours_to_gt**2)\n",
    "\n",
    "    return accuracy, completeness\n",
    "\n",
    "\n",
    "ours_pts, _ = sample_points_on_mesh(output_obj_file, n_points=100000)\n",
    "m = mesh[1].replace(\"data\", \"mesh\")\n",
    "gt_pts, _ = sample_points_on_mesh(m + \".obj\", n_points=100000)\n",
    "\n",
    "acc, comp = chamfer_accuracy_completeness(ours_pts, gt_pts)\n",
    "\n",
    "print(f\"Chamfer Accuracy (Ours  GT): {acc:.6f}\")\n",
    "print(f\"Chamfer Completeness (GT  Ours): {comp:.6f}\")\n",
    "print(f\"Chamfer Distance (symmetric): {acc + comp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_on_mesh(mesh_path, n_points=100000):\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "\n",
    "    # Normalize mesh (centered and scaled uniformly)\n",
    "    bbox = mesh.bounds\n",
    "    center = mesh.centroid\n",
    "    scale = np.linalg.norm(bbox[1] - bbox[0])\n",
    "    mesh.apply_translation(-center)\n",
    "    mesh.apply_scale(1.0 / scale)\n",
    "\n",
    "    # Export normalized mesh\n",
    "    mesh.export(mesh_path.replace(\".obj\", \".obj\"))\n",
    "\n",
    "    points, _ = trimesh.sample.sample_surface(mesh, n_points)\n",
    "    return points, mesh\n",
    "\n",
    "\n",
    "_, _ = sample_points_on_mesh(\n",
    "    \"/home/wylliam/dev/Kyushu_experiments/outputs/gargoyle_unconverged/cdp1000_v0_cvt100_clipTrue_buildFalse_upsampling0_num_centroids32_target_size32_final.obj\",\n",
    "    n_points=100000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
