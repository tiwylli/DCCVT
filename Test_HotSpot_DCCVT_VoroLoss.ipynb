{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed71bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import math\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import trimesh\n",
    "import diffvoronoi\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d, Delaunay\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import sdfpred_utils.Steik_data3d as sd3d\n",
    "import sdfpred_utils.Steik_Loss as sl\n",
    "import sdfpred_utils.Steik_utils as Stu \n",
    "import open3d as o3d\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "#torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "#lr_model = 0.00005*2\n",
    "destination = \"./images/autograd/HotSpot/\"\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "\n",
    "model_trained_it = \"\"\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ddcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Voroloss_opt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Voroloss_opt, self).__init__()\n",
    "        self.knn = 16\n",
    "\n",
    "    def __call__(self, points, spoints):\n",
    "        \"\"\"points, self.points\"\"\"\n",
    "        # WARNING: fecthing for knn\n",
    "        with torch.no_grad():\n",
    "            indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "        points_knn = knn_gather(spoints, indices)\n",
    "        points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "        voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "        voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "        vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "            -1\n",
    "        ) / voronoi_edge_l\n",
    "        sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "        return sq_dist.min(-1)[0]\n",
    "    \n",
    "voroloss = Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95a49d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sites_spread_loss(sites):\n",
    "#     with torch.no_grad():\n",
    "#         indices = knn_points(sites, sites, K=).idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a4b4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([512, 3])\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 8**3\n",
    "\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "#add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n",
    "#ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())\n",
    "\n",
    "# # load pointcloud used for sdf training\n",
    "# pointcloud = o3d.io.read_point_cloud(mesh[1]+\".ply\")\n",
    "# print(\"Pointcloud shape: \", np.asarray(pointcloud.points).shape)\n",
    "# # sample pointcloud to 150*32*32\n",
    "# chamfer_distance_pc_gt = pointcloud.uniform_down_sample(int((128**3)/(150*32*32)))\n",
    "# chamfer_distance_pc_gt = np.asarray(chamfer_distance_pc_gt.points)\n",
    "# print(\"Chamfer distance pointcloud shape: \", chamfer_distance_pc_gt.shape)\n",
    "\n",
    "\n",
    "# ps.register_point_cloud(\"pointcloud_gt\", chamfer_distance_pc_gt)\n",
    "# chamfer_distance_pc_gt = torch.tensor(chamfer_distance_pc_gt, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe96048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=32*32*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5753400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add mnfld points with random noise to sites \n",
    "N = mnfld_points.squeeze(0).shape[0]\n",
    "num_samples = 16**3\n",
    "idx = torch.randint(0, N, (num_samples,))\n",
    "sampled = mnfld_points.squeeze(0)[idx]\n",
    "perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.1\n",
    "sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "\n",
    "sites_pred = model(sites)#[\"nonmanifold_pnts_pred\"]\n",
    "#mnfld_preds = model(mnfld_points)#[\"nonmanifold_pnts_pred\"]\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "#mnf_cloud.add_scalar_quantity(\"mnfld_points_pred\", mnfld_preds.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "initial_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "ps.register_surface_mesh(\"initial Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    # lambda_pc = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    # lambda_laplace = lambda_weights[3]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    lambda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    # lambda_target_points = lambda_weights[7]\n",
    "    lambda_sdf = 5e3\n",
    "    lambda_div = 1e2\n",
    "    lambda_eikonal = 5e1\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute voronoi and delaunay once for each epoch and pass it around\n",
    "        # Compute Voronoi diagram\n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        #vor = Voronoi(sites_np)\n",
    "        #tri = Delaunay(sites_np)\n",
    "      \n",
    "      \n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "        #print(\"Delaunay simplices shape: \", d3dsimplices.shape)\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "                \n",
    "        #vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, tri, None, model)\n",
    "        #vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, vor, tri, None, model)\n",
    "        \n",
    "        vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "        print(\"points\", points.shape) \n",
    "\n",
    "        # Compute losses       \n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_voronoi(sites, vor)\n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, tri)\n",
    "    \n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        \n",
    "        from pytorch3d.loss import chamfer_distance\n",
    "        chamfer_loss, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        print(f\"Chamfer loss PYTORCH3D {chamfer_loss} weighted: {lambda_chamfer*chamfer_loss} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "        # voroloss_loss = voroloss(points.unsqueeze(0), mnfld_points)\n",
    "        # #voroloss_loss = voroloss(sites.unsqueeze(0), mnfld_points)\n",
    "        # voroloss_loss = voroloss_loss.mean()\n",
    "        # print(f\"After Voronoi loss: {voroloss_loss} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        \n",
    "        #SDF loss\n",
    "        sdf_loss = torch.mean(model(points)**2) + torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "        #sdf_loss = torch.maximum((model(sites).abs() - 0.01), torch.tensor(0.0)).mean()\n",
    "        #sdf_loss = torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "        print(\"SDF loss: \", sdf_loss, \"weighted: \", lambda_chamfer*sdf_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sites_loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            #lambda_chamfer * chamfer_loss \n",
    "            #lambda_chamfer * voroloss_loss\n",
    "            + lambda_chamfer * sdf_loss\n",
    "        )\n",
    "            \n",
    "        loss = sites_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            #if upsampled > 0:\n",
    "                #print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "        if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/50) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "    \n",
    "    #Export the sites, their sdf values, the gradients of the sdf values and the hessian\n",
    "    sdf_values = model(sites)\n",
    "\n",
    "    sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "\n",
    "    N, D = sites.shape\n",
    "    hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    for i in range(D):\n",
    "        grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "        hess_sdf[:, i, :] = grad2 # fill row i of each 3Ã—3 Hessian\n",
    "    \n",
    "    np.savez(f'{mesh[0]}voroloss_to_clip{model_trained_it}.npz', sites=sites.detach().cpu().numpy(), sdf_values=sdf_values.detach().cpu().numpy(), sdf_gradients=sdf_gradients.detach().cpu().numpy(), sdf_hessians=hess_sdf.detach().cpu().numpy())\n",
    "    print(f\"Saved to {mesh[0]}voroloss_to_clip{model_trained_it}.npz\")\n",
    "    return best_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7e96773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fb2cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points torch.Size([19478, 3])\n",
      "CVT loss:  tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.6245, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00034346155007369816 weighted: 0.3434615433216095 : Allocated: 159.504384 MB, Reserved: 650.11712 MB\n",
      "SDF loss:  tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(69.6370, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 0: loss = 76.261474609375\n",
      "before loss.backward(): Allocated: 479.81568 MB, Reserved: 650.11712 MB\n",
      "After loss.backward(): Allocated: 152.754688 MB, Reserved: 650.11712 MB\n",
      "-----------------\n",
      "points torch.Size([18874, 3])\n",
      "CVT loss:  tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.8497, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00018509011715650558 weighted: 0.18509012460708618 : Allocated: 159.229952 MB, Reserved: 650.11712 MB\n",
      "SDF loss:  tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(68.4712, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 1: loss = 75.32096862792969\n",
      "before loss.backward(): Allocated: 464.400896 MB, Reserved: 650.11712 MB\n",
      "After loss.backward(): Allocated: 151.960576 MB, Reserved: 650.11712 MB\n",
      "-----------------\n",
      "points torch.Size([18832, 3])\n",
      "CVT loss:  tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(7.4788, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.0001262923760805279 weighted: 0.1262923777103424 : Allocated: 160.065024 MB, Reserved: 650.11712 MB\n",
      "SDF loss:  tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(67.4806, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 2: loss = 74.95940399169922\n",
      "before loss.backward(): Allocated: 464.169472 MB, Reserved: 650.11712 MB\n",
      "After loss.backward(): Allocated: 152.82688 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18968, 3])\n",
      "CVT loss:  tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(7.8325, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00010647004091879353 weighted: 0.10647004097700119 : Allocated: 159.252992 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(66.6167, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 3: loss = 74.44916534423828\n",
      "before loss.backward(): Allocated: 465.439232 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.966208 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19085, 3])\n",
      "CVT loss:  tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(8.5735, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.848849731497467e-05 weighted: 0.09848849475383759 : Allocated: 160.17664 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(65.7901, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 4: loss = 74.36356353759766\n",
      "before loss.backward(): Allocated: 467.011584 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.841216 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19031, 3])\n",
      "CVT loss:  tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(9.0988, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.761532419361174e-05 weighted: 0.0976153239607811 : Allocated: 159.28576 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(64.9948, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 5: loss = 74.09355163574219\n",
      "before loss.backward(): Allocated: 466.1504 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.970304 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19123, 3])\n",
      "CVT loss:  tensor(0.0565, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.6521, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.566823428031057e-05 weighted: 0.0956682339310646 : Allocated: 160.194048 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(64.2046, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 6: loss = 69.8567123413086\n",
      "before loss.backward(): Allocated: 467.43808 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.843264 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19532, 3])\n",
      "CVT loss:  tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(9.2391, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.0002514420775696635 weighted: 0.2514420747756958 : Allocated: 159.49824 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(63.5015, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 7: loss = 72.7406005859375\n",
      "before loss.backward(): Allocated: 480.506368 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.998464 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19498, 3])\n",
      "CVT loss:  tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.5991, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.000263505382463336 weighted: 0.26350536942481995 : Allocated: 160.359424 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(62.7305, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 8: loss = 69.32960510253906\n",
      "before loss.backward(): Allocated: 480.700416 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.864256 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19168, 3])\n",
      "CVT loss:  tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(9.9777, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.000240902227233164 weighted: 0.24090223014354706 : Allocated: 159.350784 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(61.9500, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 9: loss = 71.92768859863281\n",
      "before loss.backward(): Allocated: 467.693568 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.977984 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19100, 3])\n",
      "CVT loss:  tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(8.7187, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00023989900364540517 weighted: 0.23989900946617126 : Allocated: 160.18176 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(61.1954, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 10: loss = 69.91404724121094\n",
      "before loss.backward(): Allocated: 467.177984 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.841728 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19046, 3])\n",
      "CVT loss:  tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(7.0461, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00024018743715714663 weighted: 0.24018743634223938 : Allocated: 159.290368 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(60.4413, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 11: loss = 67.48739624023438\n",
      "before loss.backward(): Allocated: 466.316288 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.970304 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18937, 3])\n",
      "CVT loss:  tensor(0.0552, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.5163, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00022476819867733866 weighted: 0.22476819157600403 : Allocated: 160.106496 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(59.6812, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 12: loss = 65.19750213623047\n",
      "before loss.backward(): Allocated: 465.340928 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.832512 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18874, 3])\n",
      "CVT loss:  tensor(0.0573, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.7347, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00022907741367816925 weighted: 0.22907741367816925 : Allocated: 159.21408 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(58.9218, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 13: loss = 64.65658569335938\n",
      "before loss.backward(): Allocated: 464.385024 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.960576 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18754, 3])\n",
      "CVT loss:  tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.4656, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00023250907543115318 weighted: 0.2325090765953064 : Allocated: 160.040448 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(58.1685, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 14: loss = 64.6341552734375\n",
      "before loss.backward(): Allocated: 463.301632 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.822272 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18874, 3])\n",
      "CVT loss:  tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.4395, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00021641403145622462 weighted: 0.21641403436660767 : Allocated: 159.218688 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(57.4152, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 15: loss = 63.854740142822266\n",
      "before loss.backward(): Allocated: 464.389632 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.960576 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18874, 3])\n",
      "CVT loss:  tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.5062, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 0.00020994422084186226 weighted: 0.2099442183971405 : Allocated: 160.083456 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(56.6700, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 16: loss = 63.176239013671875\n",
      "before loss.backward(): Allocated: 464.64 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.82944 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18754, 3])\n",
      "CVT loss:  tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.8409, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.22377803362906e-05 weighted: 0.09223777800798416 : Allocated: 159.149568 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(55.8962, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 17: loss = 62.73716354370117\n",
      "before loss.backward(): Allocated: 463.025152 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.953408 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18836, 3])\n",
      "CVT loss:  tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.8247, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.329855674877763e-05 weighted: 0.09329855442047119 : Allocated: 160.060928 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(55.1600, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 18: loss = 61.98468780517578\n",
      "before loss.backward(): Allocated: 464.208384 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.827392 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([18990, 3])\n",
      "CVT loss:  tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7335, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.271643648389727e-05 weighted: 0.09271643310785294 : Allocated: 159.254016 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(54.4244, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 19: loss = 58.15789031982422\n",
      "before loss.backward(): Allocated: 465.677312 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.967232 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19036, 3])\n",
      "CVT loss:  tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2593, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 9.075936395674944e-05 weighted: 0.09075936675071716 : Allocated: 160.137728 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(53.7017, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 20: loss = 57.96098327636719\n",
      "before loss.backward(): Allocated: 466.441728 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.837632 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19117, 3])\n",
      "CVT loss:  tensor(0.0317, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1735, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.953121141530573e-05 weighted: 0.08953121304512024 : Allocated: 159.305728 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(52.9826, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 21: loss = 56.15607833862305\n",
      "before loss.backward(): Allocated: 467.099648 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.9744 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19229, 3])\n",
      "CVT loss:  tensor(0.0391, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.9091, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.773074659984559e-05 weighted: 0.08773074299097061 : Allocated: 160.222208 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(52.2599, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 22: loss = 56.16899490356445\n",
      "before loss.backward(): Allocated: 468.61056 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.848896 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19307, 3])\n",
      "CVT loss:  tensor(0.0316, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1630, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.66494738147594e-05 weighted: 0.08664947748184204 : Allocated: 159.387136 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(51.5474, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 23: loss = 54.710384368896484\n",
      "before loss.backward(): Allocated: 469.229568 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.985152 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19395, 3])\n",
      "CVT loss:  tensor(0.0469, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.6911, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.524795703124255e-05 weighted: 0.0852479562163353 : Allocated: 160.29184 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(50.8382, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 24: loss = 55.52933120727539\n",
      "before loss.backward(): Allocated: 470.470656 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.858624 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19530, 3])\n",
      "CVT loss:  tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.6217, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.378094935324043e-05 weighted: 0.08378095179796219 : Allocated: 159.47776 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(50.1258, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 25: loss = 56.74751281738281\n",
      "before loss.backward(): Allocated: 480.482816 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 151.997952 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19570, 3])\n",
      "CVT loss:  tensor(0.0491, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.9081, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.269667159765959e-05 weighted: 0.08269666880369186 : Allocated: 160.367616 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(49.4213, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 26: loss = 54.3293571472168\n",
      "before loss.backward(): Allocated: 480.820224 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.868352 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19655, 3])\n",
      "CVT loss:  tensor(0.0379, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7903, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.169226202880964e-05 weighted: 0.08169226348400116 : Allocated: 159.54176 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(48.7115, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 27: loss = 52.5018310546875\n",
      "before loss.backward(): Allocated: 480.743936 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.00512 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19716, 3])\n",
      "CVT loss:  tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0280, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.059392712311819e-05 weighted: 0.08059392869472504 : Allocated: 160.429568 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(48.0092, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 28: loss = 52.03715133666992\n",
      "before loss.backward(): Allocated: 481.115648 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.877056 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19772, 3])\n",
      "CVT loss:  tensor(0.0201, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.0124, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.968300633365288e-05 weighted: 0.07968300580978394 : Allocated: 159.583744 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(47.3149, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 29: loss = 49.327301025390625\n",
      "before loss.backward(): Allocated: 480.970752 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.011264 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19890, 3])\n",
      "CVT loss:  tensor(0.0335, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.898568583186716e-05 weighted: 0.07898568361997604 : Allocated: 160.50176 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(46.6267, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 30: loss = 49.980796813964844\n",
      "before loss.backward(): Allocated: 481.460736 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.887296 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([19944, 3])\n",
      "CVT loss:  tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.8974, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.861514313844964e-05 weighted: 0.07861514389514923 : Allocated: 159.655936 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(45.9437, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 31: loss = 47.841148376464844\n",
      "before loss.backward(): Allocated: 481.312768 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.021504 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20084, 3])\n",
      "CVT loss:  tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.1411, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.255092689068988e-05 weighted: 0.08255092799663544 : Allocated: 160.580608 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(45.2794, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 32: loss = 47.42050552368164\n",
      "before loss.backward(): Allocated: 482.080768 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.897024 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20096, 3])\n",
      "CVT loss:  tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.8589, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.750427175778896e-05 weighted: 0.07750426977872849 : Allocated: 159.72608 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(44.6020, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 33: loss = 46.46090316772461\n",
      "before loss.backward(): Allocated: 481.883648 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.029696 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20193, 3])\n",
      "CVT loss:  tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.9616, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.638389797648415e-05 weighted: 0.07638389617204666 : Allocated: 160.633856 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(43.9338, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 34: loss = 45.8953971862793\n",
      "before loss.backward(): Allocated: 482.529792 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.904192 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20396, 3])\n",
      "CVT loss:  tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.6967, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.550802547484636e-05 weighted: 0.07550802826881409 : Allocated: 159.845888 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(43.2732, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 35: loss = 44.96996307373047\n",
      "before loss.backward(): Allocated: 483.093504 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.047616 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20469, 3])\n",
      "CVT loss:  tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.2017, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.554953481303528e-05 weighted: 0.07554953545331955 : Allocated: 160.741888 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(42.6215, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 36: loss = 44.8232421875\n",
      "before loss.backward(): Allocated: 483.637248 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.91904 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20468, 3])\n",
      "CVT loss:  tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.6303, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.488485425710678e-05 weighted: 0.07488485425710678 : Allocated: 159.873536 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(41.9715, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 37: loss = 43.60182571411133\n",
      "before loss.backward(): Allocated: 483.379712 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.050688 MB, Reserved: 652.214272 MB\n",
      "-----------------\n",
      "points torch.Size([20658, 3])\n",
      "CVT loss:  tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.5782, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.567217107862234e-05 weighted: 0.07567217200994492 : Allocated: 160.823808 MB, Reserved: 652.214272 MB\n",
      "SDF loss:  tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(41.3363, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 38: loss = 42.91454315185547\n",
      "before loss.backward(): Allocated: 488.91904 MB, Reserved: 652.214272 MB\n",
      "After loss.backward(): Allocated: 152.930304 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([20758, 3])\n",
      "CVT loss:  tensor(0.0256, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.5574, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.581134559586644e-05 weighted: 0.07581134885549545 : Allocated: 159.996928 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(40.7022, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 39: loss = 43.25954055786133\n",
      "before loss.backward(): Allocated: 489.837568 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.068096 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([20848, 3])\n",
      "CVT loss:  tensor(0.0512, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.1157, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.489525887649506e-05 weighted: 0.07489525526762009 : Allocated: 160.903168 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(40.0763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 40: loss = 45.191978454589844\n",
      "before loss.backward(): Allocated: 491.144192 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.941056 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([20927, 3])\n",
      "CVT loss:  tensor(0.0437, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.3746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.455840386683121e-05 weighted: 0.07455840706825256 : Allocated: 160.068096 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(39.4520, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 41: loss = 43.826690673828125\n",
      "before loss.backward(): Allocated: 491.817984 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.077312 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([20981, 3])\n",
      "CVT loss:  tensor(0.0366, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6617, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.434299914166331e-05 weighted: 0.0743429958820343 : Allocated: 160.96 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(38.8329, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 42: loss = 42.494598388671875\n",
      "before loss.backward(): Allocated: 492.704256 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.949248 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21031, 3])\n",
      "CVT loss:  tensor(0.0302, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0208, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.389778329525143e-05 weighted: 0.07389778643846512 : Allocated: 160.111616 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(38.2144, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 43: loss = 41.23517608642578\n",
      "before loss.backward(): Allocated: 495.094784 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.082944 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21131, 3])\n",
      "CVT loss:  tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.2060, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.273723895195872e-05 weighted: 0.07273723930120468 : Allocated: 161.021952 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(37.5983, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 44: loss = 38.80434036254883\n",
      "before loss.backward(): Allocated: 496.41984 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.957952 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21108, 3])\n",
      "CVT loss:  tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.5172, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.232222560560331e-05 weighted: 0.07232222706079483 : Allocated: 160.144896 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(36.9850, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 45: loss = 39.502166748046875\n",
      "before loss.backward(): Allocated: 495.917568 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.087552 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21179, 3])\n",
      "CVT loss:  tensor(0.0528, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.2832, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.172801269916818e-05 weighted: 0.07172801345586777 : Allocated: 161.04448 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(36.3751, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 46: loss = 41.65821838378906\n",
      "before loss.backward(): Allocated: 496.9344 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.960512 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21142, 3])\n",
      "CVT loss:  tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0097, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.155447383411229e-05 weighted: 0.07155447453260422 : Allocated: 160.158208 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(35.7689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 47: loss = 39.77853775024414\n",
      "before loss.backward(): Allocated: 496.283136 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.0896 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21263, 3])\n",
      "CVT loss:  tensor(0.0518, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.1815, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.159334199968725e-05 weighted: 0.07159334421157837 : Allocated: 161.079808 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(35.1615, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 48: loss = 40.342933654785156\n",
      "before loss.backward(): Allocated: 497.834496 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.96512 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21344, 3])\n",
      "CVT loss:  tensor(0.0377, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7711, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.133313192753121e-05 weighted: 0.0713331326842308 : Allocated: 160.240128 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(34.5671, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 49: loss = 38.33822250366211\n",
      "before loss.backward(): Allocated: 498.43968 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.100864 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21437, 3])\n",
      "CVT loss:  tensor(0.0578, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.7776, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.115125481504947e-05 weighted: 0.0711512565612793 : Allocated: 161.144832 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(33.9698, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 50: loss = 39.74747085571289\n",
      "before loss.backward(): Allocated: 499.686912 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.974336 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21477, 3])\n",
      "CVT loss:  tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0377, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.134507177397609e-05 weighted: 0.07134506851434708 : Allocated: 160.29696 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(33.3765, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 51: loss = 37.41421127319336\n",
      "before loss.backward(): Allocated: 499.863552 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.109056 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21486, 3])\n",
      "CVT loss:  tensor(0.0364, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6410, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.556982745882124e-05 weighted: 0.07556983083486557 : Allocated: 161.170432 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(32.7999, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 52: loss = 36.440956115722656\n",
      "before loss.backward(): Allocated: 500.214784 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.97792 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21528, 3])\n",
      "CVT loss:  tensor(0.0332, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3192, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.246161840157583e-05 weighted: 0.07246161997318268 : Allocated: 160.323584 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(32.2105, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 53: loss = 35.529693603515625\n",
      "before loss.backward(): Allocated: 501.440512 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.111616 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21540, 3])\n",
      "CVT loss:  tensor(0.0324, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2420, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.111657760106027e-05 weighted: 0.07111657410860062 : Allocated: 161.197568 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(31.6282, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 54: loss = 34.87018585205078\n",
      "before loss.backward(): Allocated: 501.810688 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.980992 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21595, 3])\n",
      "CVT loss:  tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.1377, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.123587420210242e-05 weighted: 0.07123587280511856 : Allocated: 160.34816 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(31.0538, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 55: loss = 35.191436767578125\n",
      "before loss.backward(): Allocated: 502.083584 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.115712 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21721, 3])\n",
      "CVT loss:  tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.8375, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.123497925931588e-05 weighted: 0.07123497873544693 : Allocated: 161.265664 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(30.4822, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 56: loss = 32.319644927978516\n",
      "before loss.backward(): Allocated: 503.553024 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.991232 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21759, 3])\n",
      "CVT loss:  tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.7626, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.10796593921259e-05 weighted: 0.07107965648174286 : Allocated: 160.414208 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(29.9141, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 57: loss = 31.67670249938965\n",
      "before loss.backward(): Allocated: 503.666176 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.124416 MB, Reserved: 677.380096 MB\n",
      "-----------------\n",
      "points torch.Size([21832, 3])\n",
      "CVT loss:  tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.7189, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.065853424137458e-05 weighted: 0.07065853476524353 : Allocated: 161.311232 MB, Reserved: 677.380096 MB\n",
      "SDF loss:  tensor(0.0293, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(29.3455, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 58: loss = 30.06438446044922\n",
      "before loss.backward(): Allocated: 504.626176 MB, Reserved: 677.380096 MB\n",
      "After loss.backward(): Allocated: 152.996864 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([21851, 3])\n",
      "CVT loss:  tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0692, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.043760706437752e-05 weighted: 0.07043761014938354 : Allocated: 160.45312 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(28.7808, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 59: loss = 32.85001754760742\n",
      "before loss.backward(): Allocated: 504.55808 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.130048 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([21913, 3])\n",
      "CVT loss:  tensor(0.0308, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0780, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.121800445020199e-05 weighted: 0.07121800631284714 : Allocated: 161.344512 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(28.2229, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 60: loss = 31.300888061523438\n",
      "before loss.backward(): Allocated: 505.41056 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.001472 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([21959, 3])\n",
      "CVT loss:  tensor(0.0456, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.5647, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.062921940814704e-05 weighted: 0.07062921673059464 : Allocated: 160.50176 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(27.6688, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 61: loss = 32.23353958129883\n",
      "before loss.backward(): Allocated: 505.606656 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.136704 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([21927, 3])\n",
      "CVT loss:  tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.7207, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.061869837343693e-05 weighted: 0.07061869651079178 : Allocated: 161.35424 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(27.1138, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 62: loss = 30.834524154663086\n",
      "before loss.backward(): Allocated: 505.549312 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.002496 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22039, 3])\n",
      "CVT loss:  tensor(0.0491, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.9098, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.042684592306614e-05 weighted: 0.07042684406042099 : Allocated: 160.53248 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(26.5661, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 63: loss = 31.47597312927246\n",
      "before loss.backward(): Allocated: 506.379264 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.1408 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22032, 3])\n",
      "CVT loss:  tensor(0.0583, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.8278, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.0062349550426e-05 weighted: 0.07006234675645828 : Allocated: 161.400832 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(26.0173, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 64: loss = 31.845069885253906\n",
      "before loss.backward(): Allocated: 506.568704 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.009152 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22000, 3])\n",
      "CVT loss:  tensor(0.0537, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.3700, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.009685214143246e-05 weighted: 0.07009685039520264 : Allocated: 160.519168 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(25.4763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 65: loss = 30.84638786315918\n",
      "before loss.backward(): Allocated: 506.002432 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.138752 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22095, 3])\n",
      "CVT loss:  tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7994, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.014305447228253e-05 weighted: 0.07014305144548416 : Allocated: 161.424384 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(24.9410, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 66: loss = 27.740442276000977\n",
      "before loss.backward(): Allocated: 507.173376 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.012736 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22101, 3])\n",
      "CVT loss:  tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0749, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.066132820909843e-05 weighted: 0.07066132873296738 : Allocated: 160.556032 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(24.4095, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 67: loss = 27.484386444091797\n",
      "before loss.backward(): Allocated: 506.97472 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.144384 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22118, 3])\n",
      "CVT loss:  tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.0611, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.09268351783976e-05 weighted: 0.07092683762311935 : Allocated: 161.434112 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0239, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(23.8843, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 68: loss = 27.94542121887207\n",
      "before loss.backward(): Allocated: 507.395584 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.01376 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22086, 3])\n",
      "CVT loss:  tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.0015, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.090697181411088e-05 weighted: 0.07090697437524796 : Allocated: 160.556032 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(23.3594, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 69: loss = 26.360891342163086\n",
      "before loss.backward(): Allocated: 506.83648 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.144384 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22101, 3])\n",
      "CVT loss:  tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.0919, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.017924508545548e-05 weighted: 0.07017924636602402 : Allocated: 161.428992 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0228, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(22.8370, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 70: loss = 28.92886734008789\n",
      "before loss.backward(): Allocated: 507.23328 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.012736 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22145, 3])\n",
      "CVT loss:  tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(5.3787, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.025567174423486e-05 weighted: 0.07025567442178726 : Allocated: 160.58112 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0223, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(22.3180, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 71: loss = 27.696674346923828\n",
      "before loss.backward(): Allocated: 507.40992 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.147456 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22191, 3])\n",
      "CVT loss:  tensor(0.0424, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(4.2449, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.012241258053109e-05 weighted: 0.07012241333723068 : Allocated: 161.464832 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0218, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(21.8036, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 72: loss = 26.048498153686523\n",
      "before loss.backward(): Allocated: 508.10368 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.017856 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22259, 3])\n",
      "CVT loss:  tensor(0.0313, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.1324, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.011594425421208e-05 weighted: 0.07011594623327255 : Allocated: 160.6272 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0213, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(21.2916, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 73: loss = 24.424020767211914\n",
      "before loss.backward(): Allocated: 508.507648 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.1536 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22324, 3])\n",
      "CVT loss:  tensor(0.0323, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.2262, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.019179611233994e-05 weighted: 0.07019179314374924 : Allocated: 161.519104 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(20.7825, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 74: loss = 24.00871467590332\n",
      "before loss.backward(): Allocated: 509.3888 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.025536 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22351, 3])\n",
      "CVT loss:  tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(6.6885, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 6.990550900809467e-05 weighted: 0.06990551203489304 : Allocated: 160.665088 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0203, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(20.2815, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 75: loss = 26.969974517822266\n",
      "before loss.backward(): Allocated: 509.398016 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.158208 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22418, 3])\n",
      "CVT loss:  tensor(0.0260, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.6049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.000339974183589e-05 weighted: 0.07000339776277542 : Allocated: 161.559552 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0198, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(19.7876, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 76: loss = 22.392547607421875\n",
      "before loss.backward(): Allocated: 510.30016 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.030656 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22401, 3])\n",
      "CVT loss:  tensor(0.0334, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.3418, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 6.977781595196575e-05 weighted: 0.06977781653404236 : Allocated: 160.6912 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0193, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(19.2982, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 77: loss = 22.639949798583984\n",
      "before loss.backward(): Allocated: 509.889536 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.162304 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22490, 3])\n",
      "CVT loss:  tensor(0.0270, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7035, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.029106200207025e-05 weighted: 0.07029106467962265 : Allocated: 161.590784 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(18.8174, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 78: loss = 21.520906448364258\n",
      "before loss.backward(): Allocated: 510.995968 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.035264 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22398, 3])\n",
      "CVT loss:  tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.8522, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 6.99346128385514e-05 weighted: 0.06993461400270462 : Allocated: 160.690176 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(18.3412, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 79: loss = 19.19344711303711\n",
      "before loss.backward(): Allocated: 509.856768 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.16128 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22527, 3])\n",
      "CVT loss:  tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.8621, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.002973870839924e-05 weighted: 0.07002973556518555 : Allocated: 161.60512 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(17.8714, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 80: loss = 18.733489990234375\n",
      "before loss.backward(): Allocated: 511.351296 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.036288 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22506, 3])\n",
      "CVT loss:  tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.6835, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.0436108217109e-05 weighted: 0.07043610513210297 : Allocated: 160.732672 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(17.4040, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 81: loss = 18.087493896484375\n",
      "before loss.backward(): Allocated: 510.899712 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 152.167936 MB, Reserved: 679.477248 MB\n",
      "-----------------\n",
      "points torch.Size([22531, 3])\n",
      "CVT loss:  tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.6372, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 8.535003144061193e-05 weighted: 0.08535002917051315 : Allocated: 161.613312 MB, Reserved: 679.477248 MB\n",
      "SDF loss:  tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(16.9645, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 82: loss = 17.60171127319336\n",
      "before loss.backward(): Allocated: 517.690368 MB, Reserved: 679.477248 MB\n",
      "After loss.backward(): Allocated: 153.037824 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22615, 3])\n",
      "CVT loss:  tensor(0.0168, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.6776, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.017957977950573e-05 weighted: 0.07017958164215088 : Allocated: 160.7808 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0165, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(16.4927, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 83: loss = 18.170289993286133\n",
      "before loss.backward(): Allocated: 519.208448 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.174592 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22671, 3])\n",
      "CVT loss:  tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.7531, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.085644756443799e-05 weighted: 0.07085644453763962 : Allocated: 161.667584 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(16.0474, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 84: loss = 17.80052375793457\n",
      "before loss.backward(): Allocated: 519.94368 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.045504 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22672, 3])\n",
      "CVT loss:  tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(3.6982, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.106435077730566e-05 weighted: 0.07106435298919678 : Allocated: 160.804352 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(15.6108, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 85: loss = 19.30902671813965\n",
      "before loss.backward(): Allocated: 519.70304 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.177664 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22613, 3])\n",
      "CVT loss:  tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.2979, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.110716251190752e-05 weighted: 0.0711071640253067 : Allocated: 161.652736 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(15.1851, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 86: loss = 17.483076095581055\n",
      "before loss.backward(): Allocated: 519.449088 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.04192 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22674, 3])\n",
      "CVT loss:  tensor(0.0276, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.7649, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.061756332404912e-05 weighted: 0.07061756402254105 : Allocated: 160.807936 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(14.7666, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 87: loss = 17.531526565551758\n",
      "before loss.backward(): Allocated: 519.723008 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.177152 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22722, 3])\n",
      "CVT loss:  tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.9191, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.106914563337341e-05 weighted: 0.0710691437125206 : Allocated: 161.69984 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0143, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(14.3477, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 88: loss = 16.266813278198242\n",
      "before loss.backward(): Allocated: 520.39424 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.049088 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22714, 3])\n",
      "CVT loss:  tensor(0.0107, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.0717, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.024766819085926e-05 weighted: 0.07024766504764557 : Allocated: 160.82688 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(13.9379, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 89: loss = 15.009593963623047\n",
      "before loss.backward(): Allocated: 520.070144 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.180224 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22864, 3])\n",
      "CVT loss:  tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.6829, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.070916763041168e-05 weighted: 0.07070916891098022 : Allocated: 161.75616 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0135, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(13.5329, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 90: loss = 14.21580982208252\n",
      "before loss.backward(): Allocated: 521.618944 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.056768 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22918, 3])\n",
      "CVT loss:  tensor(0.0223, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.2307, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.03616751707159e-05 weighted: 0.0703616738319397 : Allocated: 160.904192 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(13.1265, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 91: loss = 15.357172012329102\n",
      "before loss.backward(): Allocated: 521.828352 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.191488 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22876, 3])\n",
      "CVT loss:  tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.3877, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.02174729667604e-05 weighted: 0.07021747529506683 : Allocated: 161.761792 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(12.7269, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 92: loss = 15.114638328552246\n",
      "before loss.backward(): Allocated: 521.723392 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.057792 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22907, 3])\n",
      "CVT loss:  tensor(0.0298, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(2.9769, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.030143751762807e-05 weighted: 0.07030143588781357 : Allocated: 160.899072 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(12.3404, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 93: loss = 15.31731128692627\n",
      "before loss.backward(): Allocated: 521.729024 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.190464 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22966, 3])\n",
      "CVT loss:  tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.1719, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.046800601528957e-05 weighted: 0.070468008518219 : Allocated: 161.790976 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0120, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(11.9598, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 94: loss = 13.131744384765625\n",
      "before loss.backward(): Allocated: 522.494464 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.0624 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22874, 3])\n",
      "CVT loss:  tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.4721, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.027661195024848e-05 weighted: 0.07027661055326462 : Allocated: 160.88832 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(11.5874, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 95: loss = 12.059479713439941\n",
      "before loss.backward(): Allocated: 521.447936 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.188928 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22945, 3])\n",
      "CVT loss:  tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.6902, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.043833466013893e-05 weighted: 0.07043833285570145 : Allocated: 161.78432 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(11.2227, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 96: loss = 11.91285514831543\n",
      "before loss.backward(): Allocated: 522.315264 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.060352 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22903, 3])\n",
      "CVT loss:  tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(1.0449, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.020737393759191e-05 weighted: 0.0702073723077774 : Allocated: 160.902656 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0109, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(10.8604, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 97: loss = 11.90533447265625\n",
      "before loss.backward(): Allocated: 521.69984 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.189952 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22997, 3])\n",
      "CVT loss:  tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.5246, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.023556827334687e-05 weighted: 0.07023556530475616 : Allocated: 161.807872 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0105, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(10.5025, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 98: loss = 11.027036666870117\n",
      "before loss.backward(): Allocated: 522.765312 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.063936 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22952, 3])\n",
      "CVT loss:  tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.3708, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.027659739833325e-05 weighted: 0.07027659565210342 : Allocated: 160.925696 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(10.1471, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 99: loss = 10.517889022827148\n",
      "before loss.backward(): Allocated: 522.128384 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 152.193024 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "points torch.Size([22994, 3])\n",
      "CVT loss:  tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>) weighted:  tensor(0.6107, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Chamfer loss PYTORCH3D 7.012142305029556e-05 weighted: 0.07012142241001129 : Allocated: 161.807872 MB, Reserved: 692.06016 MB\n",
      "SDF loss:  tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>) weighted:  tensor(9.7944, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Epoch 100: loss = 10.405030250549316\n",
      "before loss.backward(): Allocated: 522.740736 MB, Reserved: 692.06016 MB\n",
      "After loss.backward(): Allocated: 153.063936 MB, Reserved: 692.06016 MB\n",
      "-----------------\n",
      "Saved to bunnyvoroloss_to_clip.npz\n",
      "Sites length:  4608\n",
      "min sites:  tensor(-0.8866, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(0.7490, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "#ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "#plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e4fcfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/HotSpot/bunny100_100_3d_model_512_chamfer1000.pth\n",
      "sites ./images/autograd/HotSpot/bunny100_100_3d_sites_512_chamfer1000.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clipped_mesh(sites, model):\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "    d3dsimplices = np.array(d3dsimplices)\n",
    "    \n",
    "    # sdf_values = model(sites)\n",
    "    # sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "    # N, D = sites.shape\n",
    "    # hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    # for i in range(D):\n",
    "    #     grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "    #     hess_sdf[:, i, :] = grad2 # fill row i of each 3Ã—3 Hessian\n",
    "\n",
    "    d3dsimplices = torch.tensor(d3dsimplices, device=device)\n",
    "    voronoi_vertices = su.compute_vertices_3d_vectorized(sites, d3dsimplices)    \n",
    "    tetra_edges = torch.cat([\n",
    "        d3dsimplices[:, [0, 1]],\n",
    "        d3dsimplices[:, [1, 2]],\n",
    "        d3dsimplices[:, [2, 3]],\n",
    "        d3dsimplices[:, [3, 0]],\n",
    "        d3dsimplices[:, [0, 2]],\n",
    "        d3dsimplices[:, [1, 3]]\n",
    "                                ], dim=0).to(device)\n",
    "    # Sort each edge to ensure uniqueness (because (a, b) and (b, a) are the same)\n",
    "    tetra_edges, _ = torch.sort(tetra_edges, dim=1)\n",
    "    # Get unique edges\n",
    "    voronoi_ridges = torch.unique(tetra_edges, dim=0)\n",
    "\n",
    "    # create a dictionnary to store the vertices_inidces composing the faces\n",
    "    # there is a face for every voronoi_ridges\n",
    "    # every vertices of the face is a simplex containing the two sites of the ridge\n",
    "    \n",
    "    #TODO:\n",
    "    \n",
    "    faces = {}\n",
    "    for i in range(voronoi_ridges.shape[0]):\n",
    "        # get the two sites composing the ridge\n",
    "        site1 = voronoi_ridges[i, 0]\n",
    "        site2 = voronoi_ridges[i, 1]\n",
    "        # get the simplices containing the two sites\n",
    "        simplices = d3dsimplices[(d3dsimplices == site1) & (d3dsimplices == site2)]\n",
    "        # get the vertices of the face\n",
    "        face = torch.unique(simplices)\n",
    "        faces[i] = face\n",
    "\n",
    "    print(face)\n",
    "\n",
    "\n",
    "    return V, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72e588cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], device='cuda:0', dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m faces \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaces\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# back to a list of lists\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ps\u001b[38;5;241m.\u001b[39mregister_surface_mesh(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero-Crossing faces final\u001b[39m\u001b[38;5;124m\"\u001b[39m, verts, faces)\n\u001b[0;32m---> 15\u001b[0m v_clip, f_clip \u001b[38;5;241m=\u001b[39m \u001b[43mget_clipped_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m ps\u001b[38;5;241m.\u001b[39mregister_surface_mesh(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClipped Voronoi faces\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_clip, f_clip)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspatial\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 46\u001b[0m, in \u001b[0;36mget_clipped_mesh\u001b[0;34m(sites, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m     faces[i] \u001b[38;5;241m=\u001b[39m face\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(face)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mV\u001b[49m, F\n",
      "\u001b[0;31mNameError\u001b[0m: name 'V' is not defined"
     ]
    }
   ],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"Zero-Crossing faces direct\", final_mesh[0], final_mesh[1])\n",
    "\n",
    "#save to file\n",
    "final_mesh_file = f'{mesh[0]}voroloss_sdf_trained{model_trained_it}.npz'\n",
    "faces = np.array(final_mesh[1], dtype=object)\n",
    "np.savez(final_mesh_file, vertices=final_mesh[0], faces=faces)\n",
    "\n",
    "data = np.load(final_mesh_file, allow_pickle=True)\n",
    "verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces final\", verts, faces)\n",
    "\n",
    "v_clip, f_clip = get_clipped_mesh(sites, model)\n",
    "ps.register_surface_mesh(\"Clipped Voronoi faces\", v_clip, f_clip)\n",
    "\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "tri = Delaunay(sites_np)\n",
    "delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "sdf_values = model(sites)\n",
    "\n",
    "\n",
    "# Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "print(marching_tetrehedra_mesh)\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "vertices = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "ps.register_surface_mesh(\"Marching Tetrahedra Mesh final\", vertices_np, faces_np)\n",
    "\n",
    "# clipped_cvt = \"clipped_CVT.obj\"\n",
    "# if os.path.exists(clipped_cvt):\n",
    "#     clipped_cvt_mesh = trimesh.load(clipped_cvt)\n",
    "#     ps.register_surface_mesh(\"Clipped CVT\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5deeda7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "End of script",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of script\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: End of script"
     ]
    }
   ],
   "source": [
    "assert False, \"End of script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d940a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the meshed of different sdf trained total epochs and the clipped version \n",
    "import polyscope as ps\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import os \n",
    "\n",
    "ps.init()\n",
    "nb_it = [\"\",\"_1000\",\"_3000\",\"_5000\",\"_7000\"]\n",
    "for it in nb_it:\n",
    "    final_mesh_file = f'gargoyle_sdf_trained{it}.npz'\n",
    "    data = np.load(final_mesh_file, allow_pickle=True)\n",
    "    verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "    faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "    ps.register_surface_mesh(f\"Zero-Crossing faces final {it}\", verts, faces) \n",
    "\n",
    "for it in nb_it:\n",
    "    clipped_mesh_file = f'gargoyle_to_clip{it}.npz_clipped.obj'\n",
    "    clipped_cvt_mesh = trimesh.load(clipped_mesh_file)\n",
    "    ps.register_surface_mesh(f\"Clipped CVT {it}\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "    \n",
    "for it in nb_it:\n",
    "    final_mesh_file = f'bunnyvoroloss_sdf_trained{it}.npz'\n",
    "    if os.path.exists(final_mesh_file):\n",
    "        data = np.load(final_mesh_file, allow_pickle=True)\n",
    "        verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "        faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "        ps.register_surface_mesh(f\"Voroloss Zero-Crossing faces final {it}\", verts, faces) \n",
    "\n",
    "for it in nb_it:\n",
    "    clipped_mesh_file = f'bunnyvoroloss_to_clip{it}.npz_clipped.obj'\n",
    "    if os.path.exists(clipped_mesh_file):\n",
    "        clipped_cvt_mesh = trimesh.load(clipped_mesh_file)\n",
    "        ps.register_surface_mesh(f\"Voroloss Clipped CVT {it}\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "    \n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"End of script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72240a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Voromesh points and values to try and plot the voronoi diagram\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "import polyscope as ps\n",
    "def get_zero_crossing_mesh_3d(sites, values):\n",
    "    sites_np = sites\n",
    "    vor = Voronoi(sites_np)  # Compute 3D Voronoi diagram\n",
    "\n",
    "    sdf_values = values\n",
    "\n",
    "    valid_faces = []  # List of polygonal faces\n",
    "    used_vertices = set()  # Set of indices for valid vertices\n",
    "\n",
    "    for (point1, point2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:\n",
    "            continue  # Skip infinite ridges\n",
    "\n",
    "        # Check if SDF changes sign across this ridge\n",
    "        if np.sign(sdf_values[point1]) != np.sign(sdf_values[point2]):\n",
    "            valid_faces.append(ridge_vertices)\n",
    "            used_vertices.update(ridge_vertices)\n",
    "\n",
    "    # **Filter Voronoi vertices**\n",
    "    used_vertices = sorted(used_vertices)  # Keep unique, sorted indices\n",
    "    vertex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(used_vertices)}\n",
    "    filtered_vertices = vor.vertices[used_vertices]\n",
    "\n",
    "    # **Re-index faces to match the new filtered vertex list**\n",
    "    filtered_faces = [[vertex_map[v] for v in face] for face in valid_faces]\n",
    "\n",
    "    return filtered_vertices, filtered_faces\n",
    "\n",
    "n_sample = [1, 16, 32, 150, 2400]\n",
    "grid_size = [32,128]\n",
    "voromesh_points = []\n",
    "voromesh_values = []\n",
    "ps.init()\n",
    "# groud plane none\n",
    "ps.set_ground_plane_mode(\"none\") \n",
    "for g in grid_size:\n",
    "    for i in n_sample:\n",
    "        try:\n",
    "            voromesh_points = np.load(f\"/home/wylliam/dev/VoroMesh/points_{i}_{g}.npy\")\n",
    "            voromesh_values = np.load(f\"/home/wylliam/dev/VoroMesh/values_{i}_{g}.npy\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for points_{i}_{g}.npy or values_{i}_{g}.npy\")\n",
    "            continue\n",
    "        mesh = get_zero_crossing_mesh_3d(voromesh_points, voromesh_values)\n",
    "        ps.register_surface_mesh(f\"mesh_{g}_{i}\", mesh[0], mesh[1])\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
