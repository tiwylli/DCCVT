{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed71bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import math\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import trimesh\n",
    "import diffvoronoi\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d, Delaunay\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import sdfpred_utils.Steik_data3d as sd3d\n",
    "import sdfpred_utils.Steik_Loss as sl\n",
    "import sdfpred_utils.Steik_utils as Stu \n",
    "import open3d as o3d\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "#torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "#lr_model = 0.00005*2\n",
    "destination = \"./images/autograd/HotSpot/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "# mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ddcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Voroloss_opt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Voroloss_opt, self).__init__()\n",
    "        self.knn = 16\n",
    "\n",
    "    def __call__(self, points, spoints):\n",
    "        \"\"\"points, self.points\"\"\"\n",
    "        # WARNING: fecthing for knn\n",
    "        with torch.no_grad():\n",
    "            indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "        points_knn = knn_gather(spoints, indices)\n",
    "        points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "        voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "        voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "        vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "            -1\n",
    "        ) / voronoi_edge_l\n",
    "        sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "        return sq_dist.min(-1)[0]\n",
    "    \n",
    "voroloss = Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a49d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sites_spread_loss(sites):\n",
    "#     with torch.no_grad():\n",
    "#         indices = knn_points(sites, sites, K=).idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4b4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([512, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 570.144\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 8**3\n",
    "\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "#add noise to meshgrid\n",
    "meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n",
    "#ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())\n",
    "\n",
    "# # load pointcloud used for sdf training\n",
    "# pointcloud = o3d.io.read_point_cloud(mesh[1]+\".ply\")\n",
    "# print(\"Pointcloud shape: \", np.asarray(pointcloud.points).shape)\n",
    "# # sample pointcloud to 150*32*32\n",
    "# chamfer_distance_pc_gt = pointcloud.uniform_down_sample(int((128**3)/(150*32*32)))\n",
    "# chamfer_distance_pc_gt = np.asarray(chamfer_distance_pc_gt.points)\n",
    "# print(\"Chamfer distance pointcloud shape: \", chamfer_distance_pc_gt.shape)\n",
    "\n",
    "\n",
    "# ps.register_point_cloud(\"pointcloud_gt\", chamfer_distance_pc_gt)\n",
    "# chamfer_distance_pc_gt = torch.tensor(chamfer_distance_pc_gt, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe96048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=32*32*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5753400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add mnfld points with random noise to sites \n",
    "N = mnfld_points.squeeze(0).shape[0]\n",
    "num_samples = 32**3\n",
    "idx = torch.randint(0, N, (num_samples,))\n",
    "sampled = mnfld_points.squeeze(0)[idx]\n",
    "perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.1\n",
    "sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "\n",
    "sites_pred = model(sites)#[\"nonmanifold_pnts_pred\"]\n",
    "#mnfld_preds = model(mnfld_points)#[\"nonmanifold_pnts_pred\"]\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "#mnf_cloud.add_scalar_quantity(\"mnfld_points_pred\", mnfld_preds.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "initial_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "ps.register_surface_mesh(\"initial Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    # lambda_pc = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    # lambda_laplace = lambda_weights[3]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    lambda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    # lambda_target_points = lambda_weights[7]\n",
    "    lambda_sdf = 5e3\n",
    "    lambda_div = 1e2\n",
    "    lambda_eikonal = 5e1\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute voronoi and delaunay once for each epoch and pass it around\n",
    "        # Compute Voronoi diagram\n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        #vor = Voronoi(sites_np)\n",
    "        #tri = Delaunay(sites_np)\n",
    "      \n",
    "      \n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "        #print(\"Delaunay simplices shape: \", d3dsimplices.shape)\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "                \n",
    "        #vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, tri, None, model)\n",
    "        #vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, vor, tri, None, model)\n",
    "        \n",
    "        vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "        print(\"points\", points.shape) \n",
    "\n",
    "        # Compute losses       \n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_voronoi(sites, vor)\n",
    "        #cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, tri)\n",
    "    \n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        \n",
    "        from pytorch3d.loss import chamfer_distance\n",
    "        chamfer_loss, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        print(f\"Chamfer loss PYTORCH3D {chamfer_loss} weighted: {lambda_chamfer*chamfer_loss} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "        # voroloss_loss = voroloss(points.unsqueeze(0), mnfld_points)\n",
    "        # #voroloss_loss = voroloss(sites.unsqueeze(0), mnfld_points)\n",
    "        # voroloss_loss = voroloss_loss.mean()\n",
    "        # print(f\"After Voronoi loss: {voroloss_loss} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        \n",
    "        #SDF loss\n",
    "        sdf_loss = torch.mean(model(points)**2) + torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "        #sdf_loss = torch.maximum((model(sites).abs() - 0.01), torch.tensor(0.0)).mean()\n",
    "        #sdf_loss = torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "        print(\"SDF loss: \", sdf_loss, \"weighted: \", lambda_chamfer*sdf_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sites_loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            #lambda_chamfer * chamfer_loss \n",
    "            #lambda_chamfer * voroloss_loss\n",
    "            + lambda_chamfer * sdf_loss\n",
    "        )\n",
    "            \n",
    "        loss = sites_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            #if upsampled > 0:\n",
    "                #print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "        if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/50) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "    \n",
    "    #Export the sites, their sdf values, the gradients of the sdf values and the hessian\n",
    "    sdf_values = model(sites)\n",
    "\n",
    "    sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "\n",
    "    N, D = sites.shape\n",
    "    hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    for i in range(D):\n",
    "        grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "        hess_sdf[:, i, :] = grad2 # fill row i of each 3×3 Hessian\n",
    "    \n",
    "    np.savez(f'{mesh[0]}voroloss_to_clip{model_trained_it}.npz', sites=sites.detach().cpu().numpy(), sdf_values=sdf_values.detach().cpu().numpy(), sdf_gradients=sdf_gradients.detach().cpu().numpy(), sdf_hessians=hess_sdf.detach().cpu().numpy())\n",
    "    print(f\"Saved to {mesh[0]}voroloss_to_clip{model_trained_it}.npz\")\n",
    "    return best_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e96773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "#ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "#plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4fcfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/HotSpot/bunny100_100_3d_model_512_chamfer1000.pth\n",
      "sites ./images/autograd/HotSpot/bunny100_100_3d_sites_512_chamfer1000.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e588cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"Zero-Crossing faces direct\", final_mesh[0], final_mesh[1])\n",
    "\n",
    "#save to file\n",
    "final_mesh_file = f'{mesh[0]}voroloss_sdf_trained{model_trained_it}.npz'\n",
    "faces = np.array(final_mesh[1], dtype=object)\n",
    "np.savez(final_mesh_file, vertices=final_mesh[0], faces=faces)\n",
    "\n",
    "data = np.load(final_mesh_file, allow_pickle=True)\n",
    "verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "print(\"Zero-Crossing faces final shape: \", verts.shape)\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces final\", verts, faces)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=512)\n",
    "ps.register_surface_mesh(\"torch polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "s_p = su.sample_mesh_points(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "ps.register_point_cloud(\"sampled clipped mesh\", s_p.detach().cpu().numpy())\n",
    "\n",
    "hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "\n",
    "# ##register original mesh\n",
    "# mesh_file = mesh[1]+\".stl\"\n",
    "# #load mesh \n",
    "# m = trimesh.load(mesh_file)\n",
    "# #convert to numpy\n",
    "# mesh_np = np.array(m.vertices)\n",
    "# #normalize mesh\n",
    "# mesh_np = mesh_np - np.mean(mesh_np, axis=0)\n",
    "# mesh_np = mesh_np / np.max(np.abs(mesh_np))\n",
    "# mesh_faces = np.array(m.faces)\n",
    "# ps.register_surface_mesh(\"Original Mesh\", mesh_np, mesh_faces)\n",
    "\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "tri = Delaunay(sites_np)\n",
    "delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "sdf_values = model(sites)\n",
    "\n",
    "# Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "vertices = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "ps.register_surface_mesh(\"Marching Tetrahedra Mesh final\", vertices_np, faces_np)\n",
    "\n",
    "# clipped_cvt = \"clipped_CVT.obj\"\n",
    "# if os.path.exists(clipped_cvt):\n",
    "#     clipped_cvt_mesh = trimesh.load(clipped_cvt)\n",
    "#     ps.register_surface_mesh(\"Clipped CVT\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5deeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"End of script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d940a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the meshed of different sdf trained total epochs and the clipped version \n",
    "import polyscope as ps\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import os \n",
    "\n",
    "ps.init()\n",
    "nb_it = [\"\",\"_1000\",\"_3000\",\"_5000\",\"_7000\"]\n",
    "for it in nb_it:\n",
    "    final_mesh_file = f'gargoyle_sdf_trained{it}.npz'\n",
    "    data = np.load(final_mesh_file, allow_pickle=True)\n",
    "    verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "    faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "    ps.register_surface_mesh(f\"Zero-Crossing faces final {it}\", verts, faces) \n",
    "\n",
    "for it in nb_it:\n",
    "    clipped_mesh_file = f'gargoyle_to_clip{it}.npz_clipped.obj'\n",
    "    clipped_cvt_mesh = trimesh.load(clipped_mesh_file)\n",
    "    ps.register_surface_mesh(f\"Clipped CVT {it}\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "    \n",
    "for it in nb_it:\n",
    "    final_mesh_file = f'bunnyvoroloss_sdf_trained{it}.npz'\n",
    "    if os.path.exists(final_mesh_file):\n",
    "        data = np.load(final_mesh_file, allow_pickle=True)\n",
    "        verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "        faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "        ps.register_surface_mesh(f\"Voroloss Zero-Crossing faces final {it}\", verts, faces) \n",
    "\n",
    "for it in nb_it:\n",
    "    clipped_mesh_file = f'bunnyvoroloss_to_clip{it}.npz_clipped.obj'\n",
    "    if os.path.exists(clipped_mesh_file):\n",
    "        clipped_cvt_mesh = trimesh.load(clipped_mesh_file)\n",
    "        ps.register_surface_mesh(f\"Voroloss Clipped CVT {it}\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "    \n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"End of script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72240a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Voromesh points and values to try and plot the voronoi diagram\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "import polyscope as ps\n",
    "def get_zero_crossing_mesh_3d(sites, values):\n",
    "    sites_np = sites\n",
    "    vor = Voronoi(sites_np)  # Compute 3D Voronoi diagram\n",
    "\n",
    "    sdf_values = values\n",
    "\n",
    "    valid_faces = []  # List of polygonal faces\n",
    "    used_vertices = set()  # Set of indices for valid vertices\n",
    "\n",
    "    for (point1, point2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:\n",
    "            continue  # Skip infinite ridges\n",
    "\n",
    "        # Check if SDF changes sign across this ridge\n",
    "        if np.sign(sdf_values[point1]) != np.sign(sdf_values[point2]):\n",
    "            valid_faces.append(ridge_vertices)\n",
    "            used_vertices.update(ridge_vertices)\n",
    "\n",
    "    # **Filter Voronoi vertices**\n",
    "    used_vertices = sorted(used_vertices)  # Keep unique, sorted indices\n",
    "    vertex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(used_vertices)}\n",
    "    filtered_vertices = vor.vertices[used_vertices]\n",
    "\n",
    "    # **Re-index faces to match the new filtered vertex list**\n",
    "    filtered_faces = [[vertex_map[v] for v in face] for face in valid_faces]\n",
    "\n",
    "    return filtered_vertices, filtered_faces\n",
    "\n",
    "n_sample = [1, 16, 32, 150, 2400]\n",
    "grid_size = [32,128]\n",
    "voromesh_points = []\n",
    "voromesh_values = []\n",
    "ps.init()\n",
    "# groud plane none\n",
    "ps.set_ground_plane_mode(\"none\") \n",
    "for g in grid_size:\n",
    "    for i in n_sample:\n",
    "        try:\n",
    "            voromesh_points = np.load(f\"/home/wylliam/dev/VoroMesh/points_{i}_{g}.npy\")\n",
    "            voromesh_values = np.load(f\"/home/wylliam/dev/VoroMesh/values_{i}_{g}.npy\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for points_{i}_{g}.npy or values_{i}_{g}.npy\")\n",
    "            continue\n",
    "        mesh = get_zero_crossing_mesh_3d(voromesh_points, voromesh_values)\n",
    "        ps.register_surface_mesh(f\"mesh_{g}_{i}\", mesh[0], mesh[1])\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"End of script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#working version of differentiable meshing not vectorized\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def sort_face_loop(vertices, face):\n",
    "    pts = vertices[face]                       # shape (N,3)\n",
    "    ctr = pts.mean(axis=0)                     # centroid\n",
    "    # compute a normal for the polygon plane (via PCA/SVD)\n",
    "    _, _, vt = np.linalg.svd(pts - ctr)\n",
    "    normal = vt[2]                             # last singular vector\n",
    "\n",
    "    # pick a reference axis in the plane\n",
    "    ref = pts[0] - ctr\n",
    "    ref -= normal * (normal @ ref)             # project off normal\n",
    "\n",
    "    # compute angles of each point around the centroid\n",
    "    def angle(p):\n",
    "        v = p - ctr\n",
    "        v -= normal * (normal @ v)             # project into plane\n",
    "        a = np.arctan2(np.linalg.norm(np.cross(ref, v)),\n",
    "                    ref @ v)\n",
    "        # sign:\n",
    "        if np.dot(normal, np.cross(ref, v)) < 0:\n",
    "            a = 2*np.pi - a\n",
    "        return a\n",
    "\n",
    "    face_sorted = sorted(face, key=lambda idx: angle(vertices[idx]))\n",
    "    return face_sorted\n",
    "\n",
    "def get_clipped_mesh(sites, model):\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "    d3dsimplices_np = np.array(d3dsimplices)\n",
    "    \n",
    "    sdf_values = model(sites)\n",
    "    # sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "    # N, D = sites.shape\n",
    "    # hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    # for i in range(D):\n",
    "    #     grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "    #     hess_sdf[:, i, :] = grad2 # fill row i of each 3×3 Hessian\n",
    "\n",
    "    d3dsimplices_tensor = torch.tensor(d3dsimplices_np, device=device)\n",
    "    voronoi_vertices = su.compute_vertices_3d_vectorized(sites, d3dsimplices_tensor)    \n",
    "    tetra_edges = torch.cat([\n",
    "        d3dsimplices_tensor[:, [0, 1]],\n",
    "        d3dsimplices_tensor[:, [1, 2]],\n",
    "        d3dsimplices_tensor[:, [2, 3]],\n",
    "        d3dsimplices_tensor[:, [3, 0]],\n",
    "        d3dsimplices_tensor[:, [0, 2]],\n",
    "        d3dsimplices_tensor[:, [1, 3]]\n",
    "                                ], dim=0).to(device)\n",
    "    # Sort each edge to ensure uniqueness (because (a, b) and (b, a) are the same)\n",
    "    tetra_edges, _ = torch.sort(tetra_edges, dim=1)\n",
    "    # Get unique edges\n",
    "    voronoi_ridges = torch.unique(tetra_edges, dim=0)\n",
    "    \n",
    "    # create a dictionnary to store the d3dsimplices composing the faces\n",
    "    # there is a face for every voronoi_ridges\n",
    "    # every vertices of the face is a simplex containing the two sites of the ridge\n",
    "    face_dict = defaultdict(list)\n",
    "    for simplex_idx, simplex in enumerate(d3dsimplices_np):\n",
    "        for a,b in itertools.combinations(simplex, 2):\n",
    "            key = (a, b) if a < b else (b, a)\n",
    "            face_dict[key].append(simplex_idx)\n",
    "\n",
    "\n",
    "    # Extract the SDF values for each site in the pair\n",
    "    sdf_i = sdf_values[voronoi_ridges[:, 0]]  # First site in each pair\n",
    "    sdf_j = sdf_values[voronoi_ridges[:, 1]]  # Second site in each pair\n",
    "    # Find the indices where SDF values have opposing signs or one is zero\n",
    "    mask_zero_crossing_sites = (sdf_i * sdf_j <= 0).squeeze()\n",
    "    # filter the faces based on the mask\n",
    "    filtered_faces = voronoi_ridges[mask_zero_crossing_sites].detach().cpu().numpy()\n",
    "    faces = []\n",
    "    for a, b in filtered_faces:\n",
    "        key = (a, b) if a < b else (b, a)\n",
    "        match = face_dict.get(key, [])\n",
    "        #match = sort_face_loop(voronoi_vertices.detach().cpu().numpy(), match)\n",
    "        faces.append(match)\n",
    "\n",
    "\n",
    "    # Not sure if i should do this at all.\n",
    "    # 1) find every vertex index that’s actually used\n",
    "    used = set(idx for face in faces for idx in face)\n",
    "    # 2) create old->new mapping\n",
    "    old2new = {old: new for new, old in enumerate(sorted(used))}\n",
    "    # 3) build new, compact vertex array\n",
    "    vertices = voronoi_vertices[sorted(used), :]\n",
    "    # 4) remap faces\n",
    "    faces_new = [[old2new[idx] for idx in face] for face in faces]\n",
    "\n",
    "    print(\"vertices shape: \", vertices)\n",
    "    print(\"faces : \", faces_new)\n",
    "\n",
    "    return vertices, faces_new\n",
    "    return voronoi_vertices, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ad529c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v_vect, f_vect \u001b[38;5;241m=\u001b[39m \u001b[43mget_clipped_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ps\u001b[38;5;241m.\u001b[39mregister_surface_mesh(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolygon clipped mesh\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_vect\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), f_vect)\n\u001b[1;32m      3\u001b[0m ps\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[18], line 83\u001b[0m, in \u001b[0;36mget_clipped_mesh\u001b[0;34m(sites, model)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m#match = sort_face_loop(voronoi_vertices.detach().cpu().numpy(), match)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     faces\u001b[38;5;241m.\u001b[39mappend(match)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Not sure if i should do this at all.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 1) find every vertex index that’s actually used\u001b[39;00m\n\u001b[1;32m     88\u001b[0m used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(idx \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m face)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "v_vect, f_vect = get_clipped_mesh(sites, model)\n",
    "ps.register_surface_mesh(\"polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "from numba import njit, int64, types\n",
    "from numba.typed import List\n",
    "\n",
    "@njit\n",
    "def _compute_normal(a, b, c):\n",
    "    # cross( b−a, c−a )  \n",
    "    ab = b - a\n",
    "    ac = c - a\n",
    "    # cross product\n",
    "    return np.array((\n",
    "        ab[1]*ac[2] - ab[2]*ac[1],\n",
    "        ab[2]*ac[0] - ab[0]*ac[2],\n",
    "        ab[0]*ac[1] - ab[1]*ac[0],\n",
    "    ), dtype=np.float64)\n",
    "\n",
    "@njit\n",
    "def _normalize(v):\n",
    "    norm = np.sqrt(v[0]*v[0] + v[1]*v[1] + v[2]*v[2])\n",
    "    return v / (norm + 1e-12)\n",
    "\n",
    "@njit\n",
    "def _angle(idx, vertices, ctr, normal, ref):\n",
    "    p = vertices[idx]\n",
    "    v = p - ctr\n",
    "    # project into plane\n",
    "    dot_nv = normal[0]*v[0] + normal[1]*v[1] + normal[2]*v[2]\n",
    "    v = v - normal * dot_nv\n",
    "    # compute angle = atan2(||ref×v||, ref·v)\n",
    "    cr = np.empty(3, dtype=np.float64)\n",
    "    cr[0] = ref[1]*v[2] - ref[2]*v[1]\n",
    "    cr[1] = ref[2]*v[0] - ref[0]*v[2]\n",
    "    cr[2] = ref[0]*v[1] - ref[1]*v[0]\n",
    "    num = np.sqrt(cr[0]*cr[0] + cr[1]*cr[1] + cr[2]*cr[2])\n",
    "    den = ref[0]*v[0] + ref[1]*v[1] + ref[2]*v[2]\n",
    "    ang = np.arctan2(num, den)\n",
    "    # sign correction\n",
    "    sign = (normal[0]*cr[0] + normal[1]*cr[1] + normal[2]*cr[2]) < 0\n",
    "    return 2*np.pi - ang if sign else ang\n",
    "\n",
    "@njit\n",
    "def sort_face_loop_numba(vertices, face):\n",
    "    # face: 1D np.array of ints\n",
    "    n = face.shape[0]\n",
    "    # gather points and centroid\n",
    "    ctr = np.zeros(3, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        ctr += vertices[face[i]]\n",
    "    ctr /= n\n",
    "\n",
    "    # make a normal from the first 3 points\n",
    "    a = vertices[face[0]]\n",
    "    b = vertices[face[1]]\n",
    "    c = vertices[face[2]]\n",
    "    normal = _normalize(_compute_normal(a, b, c))\n",
    "\n",
    "    # reference axis\n",
    "    ref = vertices[face[0]] - ctr\n",
    "    dot_nr = normal[0]*ref[0] + normal[1]*ref[1] + normal[2]*ref[2]\n",
    "    ref = ref - normal * dot_nr\n",
    "    ref = _normalize(ref)\n",
    "\n",
    "    # compute all angles\n",
    "    angs = np.empty(n, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        angs[i] = _angle(face[i], vertices, ctr, normal, ref)\n",
    "\n",
    "    # now do an insertion‐sort by angle, carrying indices\n",
    "    sorted_idxs = np.empty(n, dtype=face.dtype)\n",
    "    sorted_angs = np.empty(n, dtype=np.float64)\n",
    "    length = 0\n",
    "    for i in range(n):\n",
    "        a_i = angs[i]\n",
    "        idx_i = face[i]\n",
    "        # find insert position\n",
    "        j = length\n",
    "        while j > 0 and sorted_angs[j-1] > a_i:\n",
    "            sorted_angs[j] = sorted_angs[j-1]\n",
    "            sorted_idxs[j] = sorted_idxs[j-1]\n",
    "            j -= 1\n",
    "        sorted_angs[j]   = a_i\n",
    "        sorted_idxs[j]   = idx_i\n",
    "        length += 1\n",
    "\n",
    "    return sorted_idxs\n",
    "\n",
    "@njit(parallel=True)\n",
    "def batch_sort_numba(vertices, faces_list, counts, output):\n",
    "    R, Kmax = faces_list.shape\n",
    "    for i in prange(R):\n",
    "        length = counts[i]\n",
    "        sorted_i = sort_face_loop_numba(vertices, faces_list[i, :length])\n",
    "        for j in range(length):\n",
    "            output[i, j] = sorted_i[j]\n",
    "\n",
    "# def batch_face_loops_from_mask(face_mask):\n",
    "#     \"\"\"\n",
    "#     face_mask: (C, M) bool tensor where\n",
    "#                face_mask[r, s] == True if simplex s contributes to face r\n",
    "#     returns: Python list of C lists, each containing the simplex-indices\n",
    "#     \"\"\"\n",
    "#     C, M = face_mask.shape\n",
    "#     device = face_mask.device\n",
    "\n",
    "#     # how many verts per face, and pad to the max\n",
    "#     counts = face_mask.sum(dim=1)            # (C,)\n",
    "#     Kmax   = int(counts.max().item())\n",
    "\n",
    "#     # flatten mask → (ridge_idx, simplex_idx)\n",
    "#     ridge_idx, simp_idx = face_mask.nonzero(as_tuple=True)  # both (S,)\n",
    "\n",
    "#     # offsets to compute position within each face\n",
    "#     offsets = torch.cat((counts.new_zeros(1),\n",
    "#                          counts.cumsum(0)[:-1]))            # (C,)\n",
    "#     offs_exp = torch.repeat_interleave(offsets, counts)     # (S,)\n",
    "#     pos     = torch.arange(ridge_idx.size(0), device=device) - offs_exp  # (S,)\n",
    "\n",
    "#     # build a padded index tensor and fill it\n",
    "#     idxs = torch.full((C, Kmax), -1, dtype=torch.long, device=device)\n",
    "#     idxs[ridge_idx, pos] = simp_idx\n",
    "\n",
    "#     # unpack into Python lists\n",
    "#     faces = []\n",
    "#     for r in range(C):\n",
    "#         k = counts[r].item()\n",
    "#         if k > 0:\n",
    "#             faces.append(idxs[r, :k].tolist())\n",
    "#         else:\n",
    "#             faces.append([])\n",
    "#     return faces\n",
    "\n",
    "def faces_via_dict(d3dsimplices, ridges):\n",
    "    # 1) build dict of (a,b) → list of simplex-indices\n",
    "    face_dict = defaultdict(list)\n",
    "    for si, simplex in enumerate(d3dsimplices):\n",
    "        # all 6 edges of a 4-vertex simplex\n",
    "        a,b,c,d = simplex\n",
    "        for u,v in ((a,b),(a,c),(a,d),(b,c),(b,d),(c,d)):\n",
    "            key = (u,v) if u < v else (v,u)\n",
    "            face_dict[key].append(si)\n",
    "\n",
    "    # 2) now for each ridge (a,b) grab its list\n",
    "    out = []\n",
    "    for (a,b) in ridges:\n",
    "        key = (a,b) if a < b else (b,a)\n",
    "        lst = face_dict.get(key, [])\n",
    "        out.append(np.array(lst, dtype=np.int32))\n",
    "    return np.array(out, dtype=object)\n",
    "\n",
    "def get_clipped_mesh_numba(sites, model, d3dsimplices, batch_size=512):\n",
    "    \"\"\"\n",
    "    sites:           (N,3) torch tensor (requires_grad)\n",
    "    model:           SDF model: sites -> (N,1) tensor of signed distances\n",
    "    d3dsimplices:    torch.LongTensor of shape (M,4) from Delaunay\n",
    "    \"\"\"\n",
    "    device = sites.device\n",
    "    if d3dsimplices is None:\n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(sites_np.shape[1]*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "    d3d = torch.tensor(d3dsimplices).to(device)              # (M,4)\n",
    "\n",
    "    # Compute per‐simplex circumcenters (Voronoi vertices)\n",
    "    vor_vertices = su.compute_vertices_3d_vectorized(sites, d3d)  # (M,3)\n",
    "\n",
    "    # Generate all edges of each simplex\n",
    "    #    torch.combinations gives the 6 index‐pairs within a 4‐long row\n",
    "    comb = torch.combinations(torch.arange(d3d.shape[1], device=device), r=2)  # (6,2)\n",
    "    #print(\"comb\", comb.shape)\n",
    "    edges = d3d[:, comb]                    # (M,6,2)\n",
    "    edges = edges.reshape(-1,2)             # (M*6,2)\n",
    "    edges, _ = torch.sort(edges, dim=1)     # sort each row so (a,b) == (b,a)\n",
    "\n",
    "    # Unique ridges across all simplices\n",
    "    ridges, inverse = torch.unique(edges, dim=0, return_inverse=True)  # (R,2)\n",
    "\n",
    "    # Evaluate SDF at each site\n",
    "    sdf = model(sites).view(-1)             # (N,)\n",
    "    sdf_i = sdf[ridges[:,0]]\n",
    "    sdf_j = sdf[ridges[:,1]]\n",
    "    zero_cross = (sdf_i * sdf_j <= 0)       # (R,)\n",
    "    # Keep only the zero-crossing ridges\n",
    "    ridges = ridges[zero_cross]             # (R0,2)\n",
    "    filtered_ridges = ridges.detach().cpu().numpy()\n",
    "    \n",
    "    faces = faces_via_dict(d3dsimplices, filtered_ridges)  # (R0, List of simplices)\n",
    "    # faces_np, counts = collect_faces_padded(d3dsimplices, filtered_ridges)  # (R0, List of simplices)\n",
    "    # # faces = [list(fl) for fl in faces_typed]\n",
    "    # faces = [ faces_np[i, :counts[i]].tolist() for i in range(faces_np.shape[0]) ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # faces = []\n",
    "    \n",
    "    # R0 = ridges.shape[0]\n",
    "    # for start in range(0, R0, batch_size):\n",
    "    #     end = min(start + batch_size, R0)\n",
    "    #     ridges_chunk = ridges[start:end]\n",
    "    #     a0 = ridges_chunk[:,0].view(-1,1,1)\n",
    "    #     b0 = ridges_chunk[:,1].view(-1,1,1)\n",
    "    #     mask_a = (d3d.unsqueeze(0) == a0).any(dim=2)\n",
    "    #     mask_b = (d3d.unsqueeze(0) == b0).any(dim=2)\n",
    "    #     face_mask = mask_a & mask_b             # (R0, M)\n",
    "    #     #print(\"face_mask\", face_mask.shape)\n",
    "    #     faces_chunk = batch_face_loops_from_mask(face_mask)\n",
    "    #     faces.extend(faces_chunk)\n",
    "    \n",
    "\n",
    "    R = len(faces)\n",
    "    counts = np.array([len(face) for face in faces], dtype=np.int64)\n",
    "    Kmax = counts.max()\n",
    "    faces_np = np.full((R, Kmax), -1, dtype=np.int64)\n",
    "    for i, face in enumerate(faces):\n",
    "        faces_np[i, :len(face)] = face\n",
    "\n",
    "    sorted_faces_np = np.full((R, Kmax), -1, dtype=np.int64)\n",
    "\n",
    "    batch_sort_numba(vor_vertices.detach().cpu().numpy(), faces_np, counts, sorted_faces_np)\n",
    "    faces_sorted = [sorted_faces_np[i, :counts[i]].tolist() for i in range(R)]\n",
    "    faces = faces_sorted\n",
    "\n",
    "    # Compact the vertex list\n",
    "    used = {idx for face in faces for idx in face}\n",
    "    old2new = {old: new for new, old in enumerate(sorted(used))}\n",
    "    new_vertices = vor_vertices[sorted(used)]\n",
    "    new_faces = [[old2new[i] for i in face] for face in faces]\n",
    "\n",
    "\n",
    "    # clip the vertices of the faces to the zero-crossing of the sdf\n",
    "    sdf_verts = model(new_vertices).view(-1)           # (M,)\n",
    "\n",
    "    # compute gradients ∇f(v)  — note create_graph=True if you\n",
    "    #    want second-order gradients to flow back into the model\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=sdf_verts,\n",
    "        inputs=new_vertices,\n",
    "        grad_outputs=torch.ones_like(sdf_verts),\n",
    "        create_graph=True,\n",
    "    )[0]                                               # (M,3)\n",
    "\n",
    "    # one Newton step https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n",
    "    epsilon = 1e-6\n",
    "    grad_norm2 = torch.sqrt(((grads + epsilon)**2).sum(dim=1, keepdim=True))    # (M,1)\n",
    "    step = sdf_verts.unsqueeze(1) * grads / (grad_norm2 + epsilon)\n",
    "    proj_vertices = new_vertices - step    \n",
    "\n",
    "    #print(\"-> vertices:\", new_vertices.shape)\n",
    "    #print(\"-> projected vertices:\", proj_vertices.shape)\n",
    "    #print(\"-> #faces:\", len(new_faces))\n",
    "    return proj_vertices, new_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "118b8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vect, f_vect = get_clipped_mesh_numba(sites, model, None, batch_size=6000)\n",
    "ps.register_surface_mesh(\"polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
