{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "#mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "#trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83b787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class Voroloss_opt(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Voroloss_opt, self).__init__()\n",
    "#         self.knn = 16\n",
    "\n",
    "#     def __call__(self, points, spoints):\n",
    "#         \"\"\"points, self.points\"\"\"\n",
    "#         # WARNING: fecthing for knn\n",
    "#         with torch.no_grad():\n",
    "#             indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "#         points_knn = knn_gather(spoints, indices)\n",
    "#         points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "#         voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "#         voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "#         vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "#             -1\n",
    "#         ) / voronoi_edge_l\n",
    "#         sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "#         return sq_dist.min(-1)[0]\n",
    "    \n",
    "voroloss = lf.Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([512, 3])\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 8**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "\n",
    "#add noise to meshgrid\n",
    "#meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "Allocated: 605.983232 MB, Reserved: 2208.301056 MB\n"
     ]
    }
   ],
   "source": [
    "# #add mnfld points with random noise to sites \n",
    "N = mnfld_points.squeeze(0).shape[0]\n",
    "num_samples = grid**3 - num_centroids\n",
    "idx = torch.randint(0, N, (num_samples,))\n",
    "sampled = mnfld_points.squeeze(0)[idx]\n",
    "perturbed = sampled + (torch.rand_like(sampled)-0.5)*0.05\n",
    "sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 605.983232 MB, Reserved: 2208.301056 MB\n"
     ]
    }
   ],
   "source": [
    "sites_pred = model(sites).detach()#[\"nonmanifold_pnts_pred\"]\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "#mnfld_preds = model(mnfld_points)#[\"nonmanifold_pnts_pred\"]\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "#mnf_cloud.add_scalar_quantity(\"mnfld_points_pred\", mnfld_preds.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "#initial_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"initial Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=4096)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites},\n",
    "    #{'params': model.parameters(), 'lr': lr_model}\n",
    "])\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # sites_np = sites.detach().cpu().numpy()\n",
    "        # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        # d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "        # print(\"points\", points.shape) \n",
    "    \n",
    "        # cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        # print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "        # #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        \n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "        # print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # s1 = torch.mean(model(points)**2)\n",
    "        # s2 = torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "        # sdf_loss = 0*s1+s2\n",
    "        # sdf_loss.backward(retain_graph=True)\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = True\n",
    "            \n",
    "        # #print(\"SDF loss: \", sdf_loss, \"weighted: \", lambda_chamfer/10*sdf_loss)\n",
    "        \n",
    "        # #v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, d3dsimplices, batch_size=4096)\n",
    "        # v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, d3dsimplices)\n",
    "        \n",
    "        \n",
    "        # #ps.register_surface_mesh(\"polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "\n",
    "        # # fanning to transform polygon faces to triangle faces\n",
    "        # triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "        # #ps.register_surface_mesh(\"triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "        # triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "        # hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "        # print(\"hs_p shape: \", hs_p.shape)\n",
    "        # #ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "        \n",
    "        # from pytorch3d.loss import chamfer_distance\n",
    "        # chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "        # print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "\n",
    "        voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        # #voroloss_loss = voroloss(sites.unsqueeze(0), mnfld_points)\n",
    "        \n",
    "        \n",
    "        # # triangle area loss\n",
    "        # # 1) Gather triangle vertices\n",
    "        # v0 = v_vect[triangle_faces[:, 0]]  # (F,3)\n",
    "        # v1 = v_vect[triangle_faces[:, 1]]  # (F,3)\n",
    "        # v2 = v_vect[triangle_faces[:, 2]]  # (F,3)\n",
    "\n",
    "        # # 2) Compute triangle areas for weighting\n",
    "        # e0 = v1 - v0               # (F,3)\n",
    "        # e1 = v2 - v0               # (F,3)\n",
    "        # cross = torch.cross(e0, e1, dim=1)  # (F,3)\n",
    "        # areas = 0.5 * cross.norm(dim=1)     # (F,)\n",
    "        # mean_area = areas.mean()  # (1,)\n",
    "        # triangle_area_loss = torch.mean(areas-mean_area)**2\n",
    "        # print(\"triangle loss: \", triangle_area_loss, \"weighted: \", lambda_chamfer*0.01*triangle_area_loss)\n",
    "\n",
    "\n",
    "        sites_loss = (\n",
    "            #lambda_cvt * cvt_loss +\n",
    "            #lambda_chamfer * chamfer_loss_mesh \n",
    "            #+ lambda_chamfer*0.01 * triangle_area_loss\n",
    "            #+ lambda_chamfer * chamfer_loss_points\n",
    "            lambda_chamfer * voroloss_loss\n",
    "            #+ lambda_chamfer/10 * sdf_loss\n",
    "        )\n",
    "            \n",
    "        loss = sites_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            #if upsampled > 0:\n",
    "                #print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "        if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "                                          #{'params': model.parameters(), 'lr': lr_model}\n",
    "                                          ])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "            \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "    \n",
    "    #Export the sites, their sdf values, the gradients of the sdf values and the hessian\n",
    "    sdf_values = model(sites)\n",
    "\n",
    "    sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "\n",
    "    N, D = sites.shape\n",
    "    hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    for i in range(D):\n",
    "        grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "        hess_sdf[:, i, :] = grad2 # fill row i of each 3Ã—3 Hessian\n",
    "    \n",
    "    np.savez(f'{mesh[0]}voroloss_to_clip{model_trained_it}.npz', sites=sites.detach().cpu().numpy(), sdf_values=sdf_values.detach().cpu().numpy(), sdf_gradients=sdf_gradients.detach().cpu().numpy(), sdf_hessians=hess_sdf.detach().cpu().numpy())\n",
    "    print(f\"Saved to {mesh[0]}voroloss_to_clip{model_trained_it}.npz\")\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.007484010420739651\n",
      "before loss.backward(): Allocated: 686.322176 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 608.549376 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 1: loss = 0.006939945742487907\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 2: loss = 0.00553774693980813\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 3: loss = 0.004699479788541794\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 4: loss = 0.004056088160723448\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 5: loss = 0.0035226044710725546\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 6: loss = 0.0030613781418651342\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 7: loss = 0.0026615799870342016\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 8: loss = 0.0023495275527238846\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 9: loss = 0.00207084184512496\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 10: loss = 0.0018299403600394726\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 11: loss = 0.0016164148692041636\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 12: loss = 0.0014318953035399318\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 13: loss = 0.0012800825061276555\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 14: loss = 0.0011400383664295077\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 15: loss = 0.0010337908752262592\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 16: loss = 0.0009257867932319641\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 17: loss = 0.000839059823192656\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 18: loss = 0.0007590799359604716\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 19: loss = 0.0006935287383385003\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 20: loss = 0.0006329641328193247\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 21: loss = 0.000582442618906498\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 22: loss = 0.0005383759271353483\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 23: loss = 0.0004924401291646063\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 24: loss = 0.0004521666851360351\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 25: loss = 0.0004235633823554963\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 26: loss = 0.00039197655860334635\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 27: loss = 0.00036320125218480825\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 28: loss = 0.00034220246016047895\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 29: loss = 0.00031950397533364594\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 30: loss = 0.00029658805578947067\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 31: loss = 0.0002717799216043204\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 32: loss = 0.0002611838572192937\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 33: loss = 0.0002461550466250628\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 34: loss = 0.00023441137454938143\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 35: loss = 0.00021889418712817132\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 36: loss = 0.000210743019124493\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 37: loss = 0.0001995881466427818\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 38: loss = 0.00018988465308211744\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 39: loss = 0.00018259434727951884\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 40: loss = 0.00017154919623862952\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 41: loss = 0.00016360011068172753\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 42: loss = 0.00015501576126553118\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 43: loss = 0.00014960496628191322\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 44: loss = 0.00014514592476189137\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 45: loss = 0.00013557249621953815\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 46: loss = 0.0001300420262850821\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 47: loss = 0.0001292050292249769\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 47 with loss 0.0001292050292249769\n",
      "Epoch 48: loss = 0.00012280012015253305\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 49: loss = 0.00011691058170981705\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 50: loss = 0.00011206995259271935\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 51: loss = 0.00011015854397555813\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 52: loss = 0.00010406216460978612\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 53: loss = 0.000101630634162575\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 54: loss = 9.962115291273221e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 55: loss = 9.453532402403653e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 56: loss = 9.372068598167971e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 56 with loss 9.372068598167971e-05\n",
      "Epoch 57: loss = 8.991832146421075e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 58: loss = 8.803422679193318e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 59: loss = 8.62872984725982e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 60: loss = 8.316246385220438e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 61: loss = 8.303693175548688e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 61 with loss 8.303693175548688e-05\n",
      "Epoch 62: loss = 8.110646740533412e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 63: loss = 7.769888179609552e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 64: loss = 7.723212911514565e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 64 with loss 7.723212911514565e-05\n",
      "Epoch 65: loss = 7.549356087110937e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 66: loss = 7.387281220871955e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 67: loss = 7.363298209384084e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 67 with loss 7.363298209384084e-05\n",
      "Epoch 68: loss = 7.10929452907294e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 69: loss = 6.966432556509972e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 70: loss = 7.019101030891761e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 70 with loss 7.019101030891761e-05\n",
      "Epoch 71: loss = 6.666834087809548e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 72: loss = 6.748645682819188e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 72 with loss 6.748645682819188e-05\n",
      "Epoch 73: loss = 6.70203153276816e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 73 with loss 6.70203153276816e-05\n",
      "Epoch 74: loss = 6.539596506627277e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 75: loss = 6.562132330145687e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 75 with loss 6.562132330145687e-05\n",
      "Epoch 76: loss = 6.56963384244591e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 76 with loss 6.56963384244591e-05\n",
      "Epoch 77: loss = 6.3861116359476e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 78: loss = 6.491243402706459e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 79: loss = 6.564144132426009e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 79 with loss 6.564144132426009e-05\n",
      "Epoch 80: loss = 6.484673212980852e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 80 with loss 6.484673212980852e-05\n",
      "Epoch 81: loss = 6.332775228656828e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 82: loss = 6.362024578265846e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 82 with loss 6.362024578265846e-05\n",
      "Epoch 83: loss = 6.283312541199848e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 83 with loss 6.283312541199848e-05\n",
      "Epoch 84: loss = 6.226203549886122e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 84 with loss 6.226203549886122e-05\n",
      "Epoch 85: loss = 6.1008078773738816e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 86: loss = 6.144201324786991e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 86 with loss 6.144201324786991e-05\n",
      "Epoch 87: loss = 6.026240225764923e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 88: loss = 6.09166563663166e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 88 with loss 6.09166563663166e-05\n",
      "Epoch 89: loss = 6.090895112720318e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 89 with loss 6.090895112720318e-05\n",
      "Epoch 90: loss = 6.117497832747176e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 90 with loss 6.117497832747176e-05\n",
      "Epoch 91: loss = 6.302320980466902e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 92: loss = 6.117398152127862e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 93: loss = 6.321404362097383e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 94: loss = 6.186396785778925e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 95: loss = 6.0067166486987844e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 96: loss = 6.068565198802389e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 96 with loss 6.068565198802389e-05\n",
      "Epoch 97: loss = 5.938407048233785e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 98: loss = 5.950759077677503e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 98 with loss 5.950759077677503e-05\n",
      "Epoch 99: loss = 5.822006642119959e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 100: loss = 5.981740832794458e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 101: loss = 5.736930324928835e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 102: loss = 5.861428871867247e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 103: loss = 5.872781912330538e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 103 with loss 5.872781912330538e-05\n",
      "Epoch 104: loss = 6.129375833552331e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 105: loss = 6.186123209772632e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 105 with loss 6.186123209772632e-05\n",
      "Epoch 106: loss = 5.8808807807508856e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 107: loss = 6.112733535701409e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 108: loss = 5.9659498219843954e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 109: loss = 5.9567981224972755e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 109 with loss 5.9567981224972755e-05\n",
      "Epoch 110: loss = 5.915204019402154e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 110 with loss 5.915204019402154e-05\n",
      "Epoch 111: loss = 5.6926564866444096e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 112: loss = 5.9307581977918744e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 113: loss = 6.282562389969826e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 114: loss = 6.15082390140742e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 115: loss = 6.126883818069473e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 115 with loss 6.126883818069473e-05\n",
      "Epoch 116: loss = 6.065071283956058e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 116 with loss 6.065071283956058e-05\n",
      "Epoch 117: loss = 6.0533962823683396e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 117 with loss 6.0533962823683396e-05\n",
      "Epoch 118: loss = 6.228656275197864e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 119: loss = 6.129684334155172e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 119 with loss 6.129684334155172e-05\n",
      "Epoch 120: loss = 6.0999838751740754e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 120 with loss 6.0999838751740754e-05\n",
      "Epoch 121: loss = 6.26577457296662e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 122: loss = 6.247314013307914e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 122 with loss 6.247314013307914e-05\n",
      "Epoch 123: loss = 6.27632689429447e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 123 with loss 6.27632689429447e-05\n",
      "Epoch 124: loss = 6.756465882062912e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 125: loss = 6.608753028558567e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 126: loss = 6.630708230659366e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 126 with loss 6.630708230659366e-05\n",
      "Epoch 127: loss = 6.46809785393998e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 128: loss = 6.379747355822474e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 128 with loss 6.379747355822474e-05\n",
      "Epoch 129: loss = 6.283734546741471e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 129 with loss 6.283734546741471e-05\n",
      "Epoch 130: loss = 6.3972438510973e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 131: loss = 6.515588756883517e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 132: loss = 6.468799983849749e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 132 with loss 6.468799983849749e-05\n",
      "Epoch 133: loss = 6.661118823103607e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 134: loss = 7.175556675065309e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 135: loss = 6.909831427037716e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 136: loss = 6.80647135595791e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 137: loss = 6.748750456608832e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 137 with loss 6.748750456608832e-05\n",
      "Epoch 138: loss = 6.732690235367045e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 138 with loss 6.732690235367045e-05\n",
      "Epoch 139: loss = 6.911309901624918e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 140: loss = 6.754291098332033e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 141: loss = 7.17368457117118e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 142: loss = 7.212428317870945e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 142 with loss 7.212428317870945e-05\n",
      "Epoch 143: loss = 7.040314812911674e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 144: loss = 7.059496419969946e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 144 with loss 7.059496419969946e-05\n",
      "Epoch 145: loss = 7.484360685339198e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 146: loss = 7.660545816179365e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 147: loss = 7.610700413351879e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 147 with loss 7.610700413351879e-05\n",
      "Epoch 148: loss = 7.54026768845506e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 148 with loss 7.54026768845506e-05\n",
      "Epoch 149: loss = 7.265596650540829e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 150: loss = 7.386782817775384e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 151: loss = 7.428716344293207e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 151 with loss 7.428716344293207e-05\n",
      "Epoch 152: loss = 7.663690485060215e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 153: loss = 7.83493451308459e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 154: loss = 8.127417095238343e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 155: loss = 7.993612962309271e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 156: loss = 8.154694660333917e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 157: loss = 8.589456410845742e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 158: loss = 8.274508581962436e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 159: loss = 8.505764708388597e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 160: loss = 8.76870471984148e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 161: loss = 8.760386117501184e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 161 with loss 8.760386117501184e-05\n",
      "Epoch 162: loss = 8.517213427694514e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 163: loss = 9.113532723858953e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 164: loss = 9.124833013629541e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 164 with loss 9.124833013629541e-05\n",
      "Epoch 165: loss = 9.216160833602771e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 165 with loss 9.216160833602771e-05\n",
      "Epoch 166: loss = 9.021878213388845e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 167: loss = 9.027018677443266e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 167 with loss 9.027018677443266e-05\n",
      "Epoch 168: loss = 8.907017763704062e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 169: loss = 9.37059594434686e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 170: loss = 9.572960698278621e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 171: loss = 9.780217806110159e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 172: loss = 9.753675840329379e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 172 with loss 9.753675840329379e-05\n",
      "Epoch 173: loss = 9.69720131251961e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 173 with loss 9.69720131251961e-05\n",
      "Epoch 174: loss = 9.907031198963523e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 175: loss = 0.0001004103833111003\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 176: loss = 0.00010023660433944315\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 176 with loss 0.00010023660433944315\n",
      "Epoch 177: loss = 0.00010158211080124602\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 178: loss = 9.801459236769006e-05\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 179: loss = 0.00010417596058687195\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 180: loss = 0.00011322053615003824\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 181: loss = 0.00011395522597013041\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 181 with loss 0.00011395522597013041\n",
      "Epoch 182: loss = 0.00011285753862466663\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 183: loss = 0.00011668435763567686\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 184: loss = 0.00011583726154640317\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 184 with loss 0.00011583726154640317\n",
      "Epoch 185: loss = 0.00011418975191190839\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 186: loss = 0.0001204144282382913\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 187: loss = 0.0001240048441104591\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 188: loss = 0.00012141110346419737\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 189: loss = 0.00012535178393591195\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 190: loss = 0.00012518577568698674\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 190 with loss 0.00012518577568698674\n",
      "Epoch 191: loss = 0.00013255924568511546\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 192: loss = 0.00013478539767675102\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 193: loss = 0.00012528816296253353\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 194: loss = 0.0001401458284817636\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 195: loss = 0.00014287001977209002\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 196: loss = 0.00014794772141613066\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 197: loss = 0.00015224286471493542\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 198: loss = 0.00014713587006554008\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 199: loss = 0.00015492299280595034\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 200: loss = 0.00015180477930698544\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 201: loss = 0.00016002637858036906\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 202: loss = 0.00015662483929190785\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 203: loss = 0.00015281418745871633\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 204: loss = 0.00016376195708289742\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 205: loss = 0.0001633422652957961\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 205 with loss 0.0001633422652957961\n",
      "Epoch 206: loss = 0.00016119732754305005\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 207: loss = 0.00017036116332747042\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 208: loss = 0.00017138202383648604\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 209: loss = 0.00017789403500501066\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 210: loss = 0.00018279021605849266\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 211: loss = 0.00017767884128261358\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 212: loss = 0.0001726979826344177\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 213: loss = 0.00017429505533073097\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 214: loss = 0.00019420665921643376\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 215: loss = 0.00019695611263159662\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 216: loss = 0.0001913840533234179\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 217: loss = 0.00019006291404366493\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 218: loss = 0.00019789001089520752\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 219: loss = 0.00020340691844467074\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 220: loss = 0.00021135702263563871\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 221: loss = 0.00020623474847525358\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 222: loss = 0.00021846093295607716\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 223: loss = 0.00022580380027648062\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 224: loss = 0.0002250633406220004\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 224 with loss 0.0002250633406220004\n",
      "Epoch 225: loss = 0.00023234594846144319\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 226: loss = 0.0002461107505951077\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 227: loss = 0.00024106861383188516\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 228: loss = 0.0002466829610057175\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 229: loss = 0.00023947455338202417\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 230: loss = 0.00024338123330380768\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 231: loss = 0.00024892366491258144\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 232: loss = 0.00024989922530949116\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 232 with loss 0.00024989922530949116\n",
      "Epoch 233: loss = 0.0002556072431616485\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 234: loss = 0.00024576939176768064\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 235: loss = 0.0002584006288088858\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 236: loss = 0.00025638705119490623\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 237: loss = 0.0002610427909530699\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 238: loss = 0.00026147105381824076\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 238 with loss 0.00026147105381824076\n",
      "Epoch 239: loss = 0.0002616674464661628\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 239 with loss 0.0002616674464661628\n",
      "Epoch 240: loss = 0.0002706988307181746\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 241: loss = 0.00028456715517677367\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 242: loss = 0.0002848520816769451\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 242 with loss 0.0002848520816769451\n",
      "Epoch 243: loss = 0.0002915575751103461\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 244: loss = 0.0002957840042654425\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 245: loss = 0.0003126458905171603\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 246: loss = 0.000322652020258829\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 247: loss = 0.0003078399458900094\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 248: loss = 0.0003225890395697206\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 249: loss = 0.00031644993578083813\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 250: loss = 0.00033256050664931536\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 251: loss = 0.0003362965362612158\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 252: loss = 0.0003168041293974966\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 253: loss = 0.00032321320031769574\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 254: loss = 0.0003237560740672052\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 254 with loss 0.0003237560740672052\n",
      "Epoch 255: loss = 0.0003320935065858066\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 256: loss = 0.0003463602624833584\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 257: loss = 0.0003427250776439905\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 258: loss = 0.0003449140058364719\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 259: loss = 0.0003656051412690431\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 260: loss = 0.00037101321504451334\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 261: loss = 0.0003684010007418692\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 262: loss = 0.0003728761221282184\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 263: loss = 0.00036615869612433016\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 264: loss = 0.00038523756666108966\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 265: loss = 0.0003826479660347104\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 266: loss = 0.00038061541272327304\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 267: loss = 0.0003961053735110909\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 268: loss = 0.00039964806637726724\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 269: loss = 0.0003902600146830082\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 270: loss = 0.000396995572373271\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 271: loss = 0.0004071388393640518\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 272: loss = 0.0004039510677102953\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 273: loss = 0.00040281403926201165\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 274: loss = 0.00042386766290292144\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 275: loss = 0.0004315580299589783\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 276: loss = 0.00042675097938627005\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 277: loss = 0.00042805622797459364\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 278: loss = 0.00042764522368088365\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 278 with loss 0.00042764522368088365\n",
      "Epoch 279: loss = 0.0004417926538735628\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 280: loss = 0.000435634923633188\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 281: loss = 0.0004278511623851955\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 282: loss = 0.00042110562208108604\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 283: loss = 0.0004288863274268806\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 284: loss = 0.0004384844214655459\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 285: loss = 0.0004616334626916796\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 286: loss = 0.0004506966797634959\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 287: loss = 0.00046057015424594283\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 288: loss = 0.00044610060285776854\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 289: loss = 0.00044402043567970395\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 290: loss = 0.00046701577957719564\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 291: loss = 0.00046190861030481756\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 292: loss = 0.00043713406194001436\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 293: loss = 0.0004547165590338409\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 294: loss = 0.00046240942901931703\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 295: loss = 0.0004830369434785098\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 296: loss = 0.00046226702397689223\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 297: loss = 0.0004697804106399417\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 298: loss = 0.00046388249029405415\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 299: loss = 0.00044939553481526673\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 300: loss = 0.0004571464378386736\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 301: loss = 0.0004634104552678764\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 302: loss = 0.00046676324564032257\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 303: loss = 0.0004613110504578799\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 304: loss = 0.0004834288847632706\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 305: loss = 0.0004716083931270987\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 306: loss = 0.0004689553170464933\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 307: loss = 0.000482593517517671\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 308: loss = 0.00047183004789985716\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 309: loss = 0.0004741361190099269\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 310: loss = 0.0004883091314695776\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 311: loss = 0.0004968399880453944\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 312: loss = 0.00048725580563768744\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 313: loss = 0.0005032289191149175\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 314: loss = 0.0004976607742719352\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 315: loss = 0.0004989407607354224\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 316: loss = 0.0005098609835840762\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 317: loss = 0.0005144935566931963\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 318: loss = 0.0005173581885173917\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 319: loss = 0.0005000216187909245\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 320: loss = 0.0005301573546603322\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 321: loss = 0.0005151143413968384\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 322: loss = 0.000514355895575136\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 322 with loss 0.000514355895575136\n",
      "Epoch 323: loss = 0.0005285445949994028\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 324: loss = 0.0005160250002518296\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 325: loss = 0.000502029259223491\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 326: loss = 0.0004994588089175522\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 327: loss = 0.0004922805237583816\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 328: loss = 0.0005030893953517079\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 329: loss = 0.0004818981688003987\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 330: loss = 0.0005237367586232722\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 331: loss = 0.0004886545357294381\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 332: loss = 0.0005228575901128352\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 333: loss = 0.0005182517925277352\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 334: loss = 0.0005023840349167585\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 335: loss = 0.0005085758166387677\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 336: loss = 0.0004979386576451361\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 337: loss = 0.0005029587773606181\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 338: loss = 0.0005027296137996018\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 338 with loss 0.0005027296137996018\n",
      "Epoch 339: loss = 0.0004990187590010464\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 340: loss = 0.0005148918717168272\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 341: loss = 0.0005097740795463324\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 342: loss = 0.0005247148801572621\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 343: loss = 0.0005058163078501821\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 344: loss = 0.0005160892033018172\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 345: loss = 0.0005148054333403707\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 346: loss = 0.0005220797029323876\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 347: loss = 0.0005293592112138867\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 348: loss = 0.0005349739221855998\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 349: loss = 0.0005297652096487582\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 350: loss = 0.0005121987196616828\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 351: loss = 0.0005093093495815992\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 352: loss = 0.0005419142544269562\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 353: loss = 0.0005206678179092705\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 354: loss = 0.000521371141076088\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 354 with loss 0.000521371141076088\n",
      "Epoch 355: loss = 0.0005192894604988396\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 356: loss = 0.0005121144349686801\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 357: loss = 0.0005142560694366693\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 358: loss = 0.0005026280414313078\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 359: loss = 0.000502444279845804\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 359 with loss 0.000502444279845804\n",
      "Epoch 360: loss = 0.0005085435695946217\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 361: loss = 0.0005179466097615659\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 362: loss = 0.0005081660347059369\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 363: loss = 0.0005022023688070476\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 364: loss = 0.0005056263762526214\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 365: loss = 0.0005408232682384551\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 366: loss = 0.0005138990236446261\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 367: loss = 0.0005140347639098763\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 367 with loss 0.0005140347639098763\n",
      "Epoch 368: loss = 0.0004991957684978843\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 369: loss = 0.0004981597303412855\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 370: loss = 0.0004970378940925002\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 371: loss = 0.00048140008584596217\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 372: loss = 0.0004791112442035228\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 373: loss = 0.0005126537871547043\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 374: loss = 0.0004931476432830095\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 375: loss = 0.0005108985933475196\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 376: loss = 0.0005330042913556099\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 377: loss = 0.0005028081359341741\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 378: loss = 0.000496487133204937\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 379: loss = 0.0005099860718473792\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 380: loss = 0.0004837894521187991\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 381: loss = 0.0004670949710998684\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 382: loss = 0.0004659984551835805\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 383: loss = 0.0004664027947001159\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 383 with loss 0.0004664027947001159\n",
      "Epoch 384: loss = 0.0004904082743450999\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 385: loss = 0.00045045415754429996\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 386: loss = 0.0004643178835976869\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 387: loss = 0.00045758686610497534\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 388: loss = 0.0004468098341021687\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 389: loss = 0.00045390689047053456\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 390: loss = 0.0004257721593603492\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 391: loss = 0.0004464744997676462\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 392: loss = 0.0004443477955646813\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 393: loss = 0.00043738383101299405\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 394: loss = 0.00043963154894299805\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 395: loss = 0.000444955745479092\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 396: loss = 0.0004476916219573468\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 397: loss = 0.0004219034453853965\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Epoch 398: loss = 0.0004223153227940202\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 398 with loss 0.0004223153227940202\n",
      "Epoch 399: loss = 0.00042250778642483056\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Converged at epoch 399 with loss 0.00042250778642483056\n",
      "Epoch 400: loss = 0.00046138069592416286\n",
      "before loss.backward(): Allocated: 688.552448 MB, Reserved: 3783.262208 MB\n",
      "After loss.backward(): Allocated: 609.335808 MB, Reserved: 3783.262208 MB\n",
      "-----------------\n",
      "Saved to gargoylevoroloss_to_clip.npz\n",
      "Sites length:  32768\n",
      "min sites:  tensor(-1., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=1, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/End2End_DCCVT/gargoyle400_400_3d_model_512_chamfer1000.pth\n",
      "sites ./images/autograd/End2End_DCCVT/gargoyle400_400_3d_sites_512_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 400\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"Zero-Crossing faces direct\", final_mesh[0], final_mesh[1])\n",
    "\n",
    "#save to file\n",
    "final_mesh_file = f'{mesh[0]}voroloss_sdf_trained{model_trained_it}.npz'\n",
    "faces = np.array(final_mesh[1], dtype=object)\n",
    "np.savez(final_mesh_file, vertices=final_mesh[0], faces=faces)\n",
    "\n",
    "data = np.load(final_mesh_file, allow_pickle=True)\n",
    "verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "# print(\"Zero-Crossing faces final shape: \", verts.shape)\n",
    "# ps.register_surface_mesh(\"Zero-Crossing faces final\", verts, faces, back_face_policy=\"identical\")\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=3072)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final clipped triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "# triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "# s_p = su.sample_mesh_points(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"sampled clipped mesh\", s_p.detach().cpu().numpy())\n",
    "\n",
    "# hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "\n",
    "# ##register original mesh\n",
    "# mesh_file = mesh[1]+\".stl\"\n",
    "# #load mesh \n",
    "# m = trimesh.load(mesh_file)\n",
    "# #convert to numpy\n",
    "# mesh_np = np.array(m.vertices)\n",
    "# #normalize mesh\n",
    "# mesh_np = mesh_np - np.mean(mesh_np, axis=0)\n",
    "# mesh_np = mesh_np / np.max(np.abs(mesh_np))\n",
    "# mesh_faces = np.array(m.faces)\n",
    "# ps.register_surface_mesh(\"Original Mesh\", mesh_np, mesh_faces)\n",
    "\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "tri = Delaunay(sites_np)\n",
    "delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "sdf_values = model(sites)\n",
    "\n",
    "# Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "vertices = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "ps.register_surface_mesh(\"Marching Tetrahedra Mesh final\", vertices_np, faces_np)\n",
    "\n",
    "# clipped_cvt = \"clipped_CVT.obj\"\n",
    "# if os.path.exists(clipped_cvt):\n",
    "#     clipped_cvt_mesh = trimesh.load(clipped_cvt)\n",
    "#     ps.register_surface_mesh(\"Clipped CVT\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
