{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9796c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import diffvoronoi\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "input_dims = 3\n",
    "lr_sites = 0.005\n",
    "lr_model = 0.00001\n",
    "destination = \"./images/autograd/End2End_DCCVT/\"\n",
    "model_trained_it = \"\"\n",
    "\n",
    "mesh = [\"gargoyle\",\"/home/wylliam/dev/Kyushu_experiments/data/gargoyle\"]\n",
    "trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-24-18-16-03/gargoyle/gargoyle/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "# mesh = [\"chair\",\"/home/wylliam/dev/Kyushu_experiments/data/chair\"]\n",
    "# trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-05-02-17-56-25/chair/chair/trained_models/model{model_trained_it}.pth\"\n",
    "\n",
    "#mesh = [\"bunny\",\"/home/wylliam/dev/Kyushu_experiments/data/bunny\"]\n",
    "#trained_model_path = f\"/home/wylliam/dev/HotSpot/log/3D/pc/HotSpot-all-2025-04-25-17-32-49/bunny/bunny/trained_models/model{model_trained_it}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import knn_points, knn_gather\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class Voroloss_opt(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Voroloss_opt, self).__init__()\n",
    "#         self.knn = 16\n",
    "\n",
    "#     def __call__(self, points, spoints):\n",
    "#         \"\"\"points, self.points\"\"\"\n",
    "#         # WARNING: fecthing for knn\n",
    "#         with torch.no_grad():\n",
    "#             indices = knn_points(points, spoints, K=self.knn).idx\n",
    "\n",
    "#         points_knn = knn_gather(spoints, indices)\n",
    "#         points_to_voronoi_center = points - points_knn[:, :, 0]\n",
    "\n",
    "#         voronoi_edge = points_knn[:, :, 1:] - points_knn[:, :, 0].unsqueeze(2)\n",
    "#         voronoi_edge_l = torch.sqrt(((voronoi_edge**2).sum(-1)))\n",
    "#         vector_length = (points_to_voronoi_center.unsqueeze(2) * voronoi_edge).sum(\n",
    "#             -1\n",
    "#         ) / voronoi_edge_l\n",
    "#         sq_dist = (vector_length - voronoi_edge_l / 2) ** 2\n",
    "#         return sq_dist.min(-1)[0]\n",
    "    \n",
    "voroloss = lf.Voroloss_opt().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new sites\n",
      "Sites shape:  torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "num_centroids = 32**3\n",
    "grid = 32\n",
    "print(\"Creating new sites\")\n",
    "noise_scale = 0.1\n",
    "domain_limit = 1\n",
    "x = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "y = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "z = torch.linspace(-domain_limit, domain_limit, int(round(num_centroids**(1/3))))\n",
    "meshgrid = torch.meshgrid(x, y, z)\n",
    "meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "\n",
    "\n",
    "#add noise to meshgrid\n",
    "#meshgrid += torch.randn_like(meshgrid) * noise_scale\n",
    "\n",
    "\n",
    "sites = meshgrid.to(device, dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "print(\"Sites shape: \", sites.shape)\n",
    "ps.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2df77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnfld_points shape:  torch.Size([1, 153600, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL WITH HOTSPOT\n",
    "import sys\n",
    "sys.path.append(\"3rdparty/HotSpot\")\n",
    "from dataset import shape_3d\n",
    "import models.Net as Net\n",
    "\n",
    "loss_type = \"igr_w_heat\"\n",
    "loss_weights = [350, 0, 0, 1, 0, 0, 20]\n",
    "\n",
    "train_set = shape_3d.ReconDataset(\n",
    "    file_path = mesh[1]+\".ply\",\n",
    "    n_points=grid*grid*150,#15000, #args.n_points,\n",
    "    n_samples=10001, #args.n_iterations,\n",
    "    grid_res=256, #args.grid_res,\n",
    "    grid_range=1.1, #args.grid_range,\n",
    "    sample_type=\"uniform_central_gaussian\", #args.nonmnfld_sample_type,\n",
    "    sampling_std=0.5, #args.nonmnfld_sample_std,\n",
    "    n_random_samples=7500, #args.n_random_samples,\n",
    "    resample=True,\n",
    "    compute_sal_dist_gt=(\n",
    "        True if \"sal\" in loss_type and loss_weights[5] > 0 else False\n",
    "    ),\n",
    "    scale_method=\"mean\"#\"mean\" #args.pcd_scale_method,\n",
    ")\n",
    "\n",
    "model = Net.Network(\n",
    "    latent_size=0,#args.latent_size,\n",
    "    in_dim=3,\n",
    "    decoder_hidden_dim=128,#args.decoder_hidden_dim,\n",
    "    nl=\"sine\",#args.nl,\n",
    "    encoder_type=\"none\",#args.encoder_type,\n",
    "    decoder_n_hidden_layers=5,#args.decoder_n_hidden_layers,\n",
    "    neuron_type=\"quadratic\",#args.neuron_type,\n",
    "    init_type=\"mfgi\",#args.init_type,\n",
    "    sphere_init_params=[1.6, 0.1],#args.sphere_init_params,\n",
    "    n_repeat_period=30#args.n_repeat_period,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "######       \n",
    "test_dataloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)   \n",
    "test_data = next(iter(test_dataloader))\n",
    "mnfld_points = test_data[\"mnfld_points\"].to(device)\n",
    "mnfld_points.requires_grad_()\n",
    "print(\"mnfld_points shape: \", mnfld_points.shape)\n",
    "if torch.cuda.is_available():\n",
    "    map_location = torch.device(\"cuda\")\n",
    "else:\n",
    "    map_location = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True, map_location=map_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7570e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "Allocated: 3.038208 MB, Reserved: 23.068672 MB\n"
     ]
    }
   ],
   "source": [
    "# # #add mnfld points with random noise to sites \n",
    "# N = mnfld_points.squeeze(0).shape[0]\n",
    "# num_samples = grid**3 - num_centroids\n",
    "# idx = torch.randint(0, N, (num_samples,))\n",
    "# sampled = mnfld_points.squeeze(0)[idx]\n",
    "# perturbed = sampled + (torch.rand_like(sampled)-0.5)\n",
    "# sites = torch.cat((sites, perturbed), dim=0)\n",
    "\n",
    "# make sites a leaf tensor\n",
    "sites = sites.detach().requires_grad_()\n",
    "print(sites.dtype)\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff63634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 11.68896 MB, Reserved: 448.790528 MB\n"
     ]
    }
   ],
   "source": [
    "sites_pred = model(sites).detach()#[\"nonmanifold_pnts_pred\"]\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "#mnfld_preds = model(mnfld_points)#[\"nonmanifold_pnts_pred\"]\n",
    "\n",
    "ps_cloud = ps.register_point_cloud(\"initial_cvt_grid+pc_gt\",sites.detach().cpu().numpy())\n",
    "mnf_cloud = ps.register_point_cloud(\"mnfld_points_pred\",mnfld_points.squeeze(0).detach().cpu().numpy())\n",
    "#mnf_cloud.add_scalar_quantity(\"mnfld_points_pred\", mnfld_preds.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "ps_cloud.add_scalar_quantity(\"vis_grid_pred\", sites_pred.reshape(-1).detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "#initial_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"initial Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=4096)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"initial triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITES OPTIMISATION LOOP\n",
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "sdf_loss_values = []\n",
    "div_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "def train_DCCVT(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites},\n",
    "    #{'params': model.parameters(), 'lr': lr_model}\n",
    "])\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 150, 200, 250], gamma=0.5)\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0.0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        # if epoch > 100:\n",
    "        #     lambda_cvt = 0\n",
    "        #     lambda_chamfer = 0\n",
    "        #     lambda_voroloss = 1000\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # sites_np = sites.detach().cpu().numpy()\n",
    "        # d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        # d3dsimplices = np.array(d3dsimplices)\n",
    "\n",
    "        # vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "        # vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        # bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        # points = torch.cat((vertices, bisectors), 0)\n",
    "        # print(\"points\", points.shape) \n",
    "    \n",
    "        # cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        # print(\"CVT loss: \", cvt_loss, \"weighted: \", lambda_cvt*cvt_loss)\n",
    "        #min_distance_loss = lf.sdf_weighted_min_distance_loss(model, sites)\n",
    "        # \n",
    "#         from pytorch3d.loss import chamfer_distance\n",
    "#         chamfer_loss_points, _ = chamfer_distance(mnfld_points.detach(), points.unsqueeze(0))\n",
    "#         print(f\"Points Chamfer loss PYTORCH3D {chamfer_loss_points} weighted: {lambda_chamfer*chamfer_loss_points} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "# # \n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         s1 = torch.mean(model(points)**2)\n",
    "#         s2 = torch.maximum((model(sites).abs() - 0.05), torch.tensor(0.0)).mean()\n",
    "#         sdf_loss = 0*s1+s2\n",
    "#         sdf_loss.backward(retain_graph=True)\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = True\n",
    "#             # \n",
    "#         #print(\"SDF loss: \", sdf_loss, \"weighted: \", lambda_chamfer/10*sdf_loss)\n",
    "#         # \n",
    "#         #v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, d3dsimplices, batch_size=4096)\n",
    "#         v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, d3dsimplices)\n",
    "#         # \n",
    "#         # \n",
    "#         #ps.register_surface_mesh(\"polygon clipped mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# # \n",
    "#         # fanning to transform polygon faces to triangle faces\n",
    "#         triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "#         #ps.register_surface_mesh(\"triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces)\n",
    "# # \n",
    "#         triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "#         hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=32*32*150)\n",
    "#         print(\"hs_p shape: \", hs_p.shape)\n",
    "#         #ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "#         # \n",
    "#         from pytorch3d.loss import chamfer_distance\n",
    "#         chamfer_loss_mesh, _ = chamfer_distance(mnfld_points.detach(), hs_p.unsqueeze(0))\n",
    "#         print(f\"Mesh Chamfer loss PYTORCH3D {chamfer_loss_mesh} weighted: {lambda_chamfer*chamfer_loss_mesh} : Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "# # \n",
    "\n",
    "        voroloss_loss = voroloss(mnfld_points.squeeze(0), sites).mean()\n",
    "        # #voroloss_loss = voroloss(sites.unsqueeze(0), mnfld_points)\n",
    "        \n",
    "        \n",
    "        # # triangle area loss\n",
    "        # # 1) Gather triangle vertices\n",
    "        # v0 = v_vect[triangle_faces[:, 0]]  # (F,3)\n",
    "        # v1 = v_vect[triangle_faces[:, 1]]  # (F,3)\n",
    "        # v2 = v_vect[triangle_faces[:, 2]]  # (F,3)\n",
    "\n",
    "        # # 2) Compute triangle areas for weighting\n",
    "        # e0 = v1 - v0               # (F,3)\n",
    "        # e1 = v2 - v0               # (F,3)\n",
    "        # cross = torch.cross(e0, e1, dim=1)  # (F,3)\n",
    "        # areas = 0.5 * cross.norm(dim=1)     # (F,)\n",
    "        # mean_area = areas.mean()  # (1,)\n",
    "        # triangle_area_loss = torch.mean(areas-mean_area)**2\n",
    "        # print(\"triangle loss: \", triangle_area_loss, \"weighted: \", lambda_chamfer*0.01*triangle_area_loss)\n",
    "\n",
    "\n",
    "        sites_loss = (\n",
    "            #lambda_cvt * cvt_loss +\n",
    "            #lambda_chamfer * chamfer_loss_mesh \n",
    "            #+ lambda_chamfer*0.01 * triangle_area_loss\n",
    "            #+ lambda_chamfer * chamfer_loss_points\n",
    "            lambda_chamfer * voroloss_loss\n",
    "            #+ lambda_chamfer/10 * sdf_loss\n",
    "        )\n",
    "            \n",
    "        loss = sites_loss\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "        print(f\"before loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "        loss.backward()\n",
    "        print(f\"After loss.backward(): Allocated: {torch.cuda.memory_allocated() / 1e6} MB, Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            #if upsampled > 0:\n",
    "                #print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        # if epoch>100 and (epoch // 100) == upsampled+1 and loss.item() < 0.5 and upsampled < upsampling:\n",
    "        if epoch/max_iter > (upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            optimizer = torch.optim.Adam([{'params': [sites], 'lr': lr_sites}, \n",
    "                                          #{'params': model.parameters(), 'lr': lr_model}\n",
    "                                          ])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            #print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            #print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            #ps.register_surface_mesh(f\"{epoch} triangle clipped mesh\", v_vect.detach().cpu().numpy(), triangle_faces.detach().cpu().numpy())\n",
    "            \n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            torch.save(sites, site_file_path)\n",
    "            \n",
    "        epoch += 1           \n",
    "    \n",
    "    #Export the sites, their sdf values, the gradients of the sdf values and the hessian\n",
    "    sdf_values = model(sites)\n",
    "\n",
    "    sdf_gradients = torch.autograd.grad(outputs=sdf_values, inputs=sites, grad_outputs=torch.ones_like(sdf_values), create_graph=True, retain_graph=True,)[0] # (N, 3)\n",
    "\n",
    "    N, D = sites.shape\n",
    "    hess_sdf = torch.zeros(N, D, D, device=sites.device)\n",
    "    for i in range(D):\n",
    "        grad2 = torch.autograd.grad(outputs=sdf_gradients[:, i], inputs=sites, grad_outputs=torch.ones_like(sdf_gradients[:, i]), create_graph=False, retain_graph=True,)[0] # (N, 3)\n",
    "        hess_sdf[:, i, :] = grad2 # fill row i of each 3Ã—3 Hessian\n",
    "    \n",
    "    np.savez(f'{mesh[0]}voroloss_to_clip{model_trained_it}.npz', sites=sites.detach().cpu().numpy(), sdf_values=sdf_values.detach().cpu().numpy(), sdf_gradients=sdf_gradients.detach().cpu().numpy(), sdf_hessians=hess_sdf.detach().cpu().numpy())\n",
    "    print(f\"Saved to {mesh[0]}voroloss_to_clip{model_trained_it}.npz\")\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_weights = [252,0,0,0,10.211111,0,100,0]\n",
    "#lambda_weights = [500,0,0,0,1000,0,100,0]\n",
    "lambda_weights = [100,0,0,0,1000,0,100,0]\n",
    "\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lambda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_true_points = lambda_weights[7]\n",
    "\n",
    "max_iter = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb5e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.10585826635360718\n",
      "before loss.backward(): Allocated: 220.517888 MB, Reserved: 513.80224 MB\n",
      "After loss.backward(): Allocated: 143.052288 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 1: loss = 0.07943440228700638\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 2: loss = 0.05880922079086304\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 3: loss = 0.04519299417734146\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 4: loss = 0.037114258855581284\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 5: loss = 0.03232475370168686\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 6: loss = 0.02934201993048191\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 7: loss = 0.026887478306889534\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 8: loss = 0.02429809607565403\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 9: loss = 0.02191605418920517\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 10: loss = 0.019415074959397316\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 11: loss = 0.017369979992508888\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 12: loss = 0.0156331118196249\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 13: loss = 0.0143930958583951\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 14: loss = 0.013390878215432167\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 15: loss = 0.012417705729603767\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 16: loss = 0.011522062122821808\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 17: loss = 0.01068032905459404\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 18: loss = 0.009881415404379368\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 19: loss = 0.00912501197308302\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 20: loss = 0.00848850142210722\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 21: loss = 0.008027823641896248\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 22: loss = 0.007479142863303423\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 23: loss = 0.0070274402387440205\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 24: loss = 0.0066409604623913765\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 25: loss = 0.006302749738097191\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 26: loss = 0.005985861178487539\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 27: loss = 0.005743090063333511\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 28: loss = 0.005395675078034401\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 29: loss = 0.00520752277225256\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 30: loss = 0.004946407396346331\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 31: loss = 0.004745763260871172\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 32: loss = 0.004555701278150082\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 33: loss = 0.004356449004262686\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 34: loss = 0.004182074684649706\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 35: loss = 0.004077301360666752\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 36: loss = 0.003906485624611378\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 37: loss = 0.003779312130063772\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 38: loss = 0.0036428850144147873\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 39: loss = 0.0035846272949129343\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 40: loss = 0.003473784774541855\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 41: loss = 0.0033944526221603155\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 42: loss = 0.0032789658289402723\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 43: loss = 0.0032373196445405483\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 44: loss = 0.003123142756521702\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 45: loss = 0.0030431565828621387\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 46: loss = 0.002968561602756381\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 47: loss = 0.0029200767166912556\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 48: loss = 0.0028492696583271027\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 49: loss = 0.0027974434196949005\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 50: loss = 0.002753780223429203\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 51: loss = 0.0026987455785274506\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 52: loss = 0.002652990398928523\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 53: loss = 0.002596244914457202\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 54: loss = 0.0025746370665729046\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 55: loss = 0.0025135932955890894\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 56: loss = 0.0024839441757649183\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 57: loss = 0.002450498752295971\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 58: loss = 0.002420996781438589\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 59: loss = 0.002397590782493353\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 60: loss = 0.0023726115468889475\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 61: loss = 0.002334462944418192\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 62: loss = 0.002313771052286029\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 63: loss = 0.002281653229147196\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 64: loss = 0.0022715916857123375\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 65: loss = 0.002245768439024687\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 66: loss = 0.002233280334621668\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 67: loss = 0.0022306866012513638\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 68: loss = 0.0021982858888804913\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 69: loss = 0.0021950199734419584\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 70: loss = 0.002160628791898489\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 71: loss = 0.0021593060810118914\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 72: loss = 0.0021244073286652565\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 73: loss = 0.0021146456710994244\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 74: loss = 0.0020990101620554924\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 75: loss = 0.0020909016020596027\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 76: loss = 0.002075291471555829\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 77: loss = 0.0020675852429121733\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 78: loss = 0.0020591551437973976\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 79: loss = 0.0020358662586659193\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 80: loss = 0.0020221963059157133\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 81: loss = 0.002000978449359536\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 82: loss = 0.0019976445473730564\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 83: loss = 0.001984493574127555\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 84: loss = 0.001977275824174285\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 85: loss = 0.001970114419236779\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 86: loss = 0.0019647805020213127\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 87: loss = 0.00195542280562222\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 88: loss = 0.0019496137974783778\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 89: loss = 0.001939126173965633\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 90: loss = 0.001935033593326807\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 91: loss = 0.0019259130349382758\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 92: loss = 0.001922056544572115\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 93: loss = 0.0019145687110722065\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 94: loss = 0.0019126132829114795\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 95: loss = 0.0019088074332103133\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 96: loss = 0.0018964966293424368\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 97: loss = 0.0018950060475617647\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 98: loss = 0.0018892143853008747\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 99: loss = 0.001884778612293303\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 100: loss = 0.0018802452832460403\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 101: loss = 0.001875941175967455\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 102: loss = 0.001871959655545652\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 103: loss = 0.001866448437795043\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 104: loss = 0.0018652257276698947\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 105: loss = 0.0018653736915439367\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 105 with loss 0.0018653736915439367\n",
      "Epoch 106: loss = 0.00187268340960145\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 107: loss = 0.001872712979093194\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 107 with loss 0.001872712979093194\n",
      "Epoch 108: loss = 0.0018504749750718474\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 109: loss = 0.0018540102755650878\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 110: loss = 0.0018483620369806886\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 111: loss = 0.0018416766542941332\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 112: loss = 0.0018406640738248825\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 113: loss = 0.0018328218720853329\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 114: loss = 0.001832550042308867\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 114 with loss 0.001832550042308867\n",
      "Epoch 115: loss = 0.0018275395268574357\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 116: loss = 0.001827611937187612\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 116 with loss 0.001827611937187612\n",
      "Epoch 117: loss = 0.0018268342828378081\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 117 with loss 0.0018268342828378081\n",
      "Epoch 118: loss = 0.0018153871642425656\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 119: loss = 0.001813422655686736\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 120: loss = 0.0018090145895257592\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 121: loss = 0.0018028037156909704\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 122: loss = 0.0017989047337323427\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 123: loss = 0.0017944417195394635\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 124: loss = 0.0018049851059913635\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 125: loss = 0.0017897801008075476\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 126: loss = 0.0017954608192667365\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 127: loss = 0.0017956242663785815\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 127 with loss 0.0017956242663785815\n",
      "Epoch 128: loss = 0.0017785067902877927\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 129: loss = 0.00178347690962255\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 130: loss = 0.0017790236743167043\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 131: loss = 0.0017742158379405737\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 132: loss = 0.001770856324583292\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 133: loss = 0.001764032756909728\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 134: loss = 0.0017581998836249113\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 135: loss = 0.0017521119443699718\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 136: loss = 0.0017564526060596108\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 137: loss = 0.0017521611880511045\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 138: loss = 0.001749793067574501\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 139: loss = 0.0017417239723727107\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 140: loss = 0.0017387783154845238\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 141: loss = 0.0017387651605531573\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 141 with loss 0.0017387651605531573\n",
      "Epoch 142: loss = 0.0017286743968725204\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 143: loss = 0.0017281033797189593\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 143 with loss 0.0017281033797189593\n",
      "Epoch 144: loss = 0.0017244953196495771\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 145: loss = 0.0017205200856551528\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 146: loss = 0.0017180395079776645\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 147: loss = 0.0017135868547484279\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 148: loss = 0.0017115036025643349\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 149: loss = 0.0017092492198571563\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 150: loss = 0.0017120245611295104\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 151: loss = 0.0016981008229777217\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 152: loss = 0.0017015995690599084\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 153: loss = 0.0016965243266895413\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 154: loss = 0.0016950611025094986\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 155: loss = 0.0016929124249145389\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 156: loss = 0.0016933341976255178\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 156 with loss 0.0016933341976255178\n",
      "Epoch 157: loss = 0.0016894083237275481\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 158: loss = 0.0016888609388843179\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 158 with loss 0.0016888609388843179\n",
      "Epoch 159: loss = 0.0016861304175108671\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 160: loss = 0.0016854816349223256\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 160 with loss 0.0016854816349223256\n",
      "Epoch 161: loss = 0.001682489411905408\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 162: loss = 0.0016814592527225614\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 163: loss = 0.0016785106854513288\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 164: loss = 0.001677683088928461\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 164 with loss 0.001677683088928461\n",
      "Epoch 165: loss = 0.0016755382530391216\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 166: loss = 0.0016739298589527607\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 167: loss = 0.001671883394010365\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 168: loss = 0.001670930185355246\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 168 with loss 0.001670930185355246\n",
      "Epoch 169: loss = 0.001668390235863626\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 170: loss = 0.0016676810337230563\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 170 with loss 0.0016676810337230563\n",
      "Epoch 171: loss = 0.0016655580839142203\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 172: loss = 0.0016649564495310187\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 172 with loss 0.0016649564495310187\n",
      "Epoch 173: loss = 0.0016619088128209114\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 174: loss = 0.0016627158038318157\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 174 with loss 0.0016627158038318157\n",
      "Epoch 175: loss = 0.0016680159606039524\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 176: loss = 0.0016582991229370236\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 177: loss = 0.0016600049566477537\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 178: loss = 0.0016564229736104608\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 179: loss = 0.001656856620684266\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 179 with loss 0.001656856620684266\n",
      "Epoch 180: loss = 0.0016562910750508308\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 180 with loss 0.0016562910750508308\n",
      "Epoch 181: loss = 0.0016553215682506561\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 181 with loss 0.0016553215682506561\n",
      "Epoch 182: loss = 0.0016576319467276335\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 183: loss = 0.0016594028566032648\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 184: loss = 0.001654048915952444\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 185: loss = 0.0016484666848555207\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 186: loss = 0.0016506474930793047\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 187: loss = 0.0016478771576657891\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 188: loss = 0.0016454848228022456\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 189: loss = 0.0016460411716252565\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 189 with loss 0.0016460411716252565\n",
      "Epoch 190: loss = 0.0016427041264250875\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 191: loss = 0.0016428628005087376\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 191 with loss 0.0016428628005087376\n",
      "Epoch 192: loss = 0.0016396880382671952\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 193: loss = 0.0016432655975222588\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 194: loss = 0.0016470476984977722\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 195: loss = 0.0016573137836530805\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 196: loss = 0.0016418134327977896\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 197: loss = 0.0016721730353310704\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 198: loss = 0.001641630893573165\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 199: loss = 0.0016426527872681618\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 200: loss = 0.0016471438575536013\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 201: loss = 0.001634720480069518\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 202: loss = 0.0016313610831275582\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 203: loss = 0.001636257627978921\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 204: loss = 0.0016353307291865349\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 204 with loss 0.0016353307291865349\n",
      "Epoch 205: loss = 0.0016325617907568812\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 206: loss = 0.0016304058954119682\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 207: loss = 0.001629559206776321\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 207 with loss 0.001629559206776321\n",
      "Epoch 208: loss = 0.0016297922702506185\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 208 with loss 0.0016297922702506185\n",
      "Epoch 209: loss = 0.0016286576865240932\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 210: loss = 0.0016260453267022967\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 211: loss = 0.0016252431087195873\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 211 with loss 0.0016252431087195873\n",
      "Epoch 212: loss = 0.0016243167920038104\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 212 with loss 0.0016243167920038104\n",
      "Epoch 213: loss = 0.001624067546799779\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 213 with loss 0.001624067546799779\n",
      "Epoch 214: loss = 0.0016229856992140412\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 215: loss = 0.0016218128148466349\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 216: loss = 0.0016197102377191186\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 217: loss = 0.0016192812472581863\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 217 with loss 0.0016192812472581863\n",
      "Epoch 218: loss = 0.0016186360735446215\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 218 with loss 0.0016186360735446215\n",
      "Epoch 219: loss = 0.0016183065017685294\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 219 with loss 0.0016183065017685294\n",
      "Epoch 220: loss = 0.0016171502647921443\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 221: loss = 0.0016158580547198653\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 222: loss = 0.0016148511786013842\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 223: loss = 0.0016140718944370747\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 223 with loss 0.0016140718944370747\n",
      "Epoch 224: loss = 0.0016139718936756253\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 224 with loss 0.0016139718936756253\n",
      "Epoch 225: loss = 0.0016128614079207182\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 226: loss = 0.0016116430051624775\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 227: loss = 0.0016112267039716244\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 227 with loss 0.0016112267039716244\n",
      "Epoch 228: loss = 0.001610607374459505\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 228 with loss 0.001610607374459505\n",
      "Epoch 229: loss = 0.0016101673245429993\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 229 with loss 0.0016101673245429993\n",
      "Epoch 230: loss = 0.0016089760465547442\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 231: loss = 0.0016083897789940238\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 231 with loss 0.0016083897789940238\n",
      "Epoch 232: loss = 0.0016084248200058937\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 232 with loss 0.0016084248200058937\n",
      "Epoch 233: loss = 0.001609878265298903\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 234: loss = 0.0016139931976795197\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 235: loss = 0.0016060177003964782\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 236: loss = 0.001608208054676652\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 237: loss = 0.0016063634539023042\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 238: loss = 0.0016046050004661083\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 239: loss = 0.0016054005827754736\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 239 with loss 0.0016054005827754736\n",
      "Epoch 240: loss = 0.0016055222367867827\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 240 with loss 0.0016055222367867827\n",
      "Epoch 241: loss = 0.0016040322370827198\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 242: loss = 0.0016037982422858477\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 242 with loss 0.0016037982422858477\n",
      "Epoch 243: loss = 0.0016038328176364303\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 243 with loss 0.0016038328176364303\n",
      "Epoch 244: loss = 0.0016041075577959418\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 244 with loss 0.0016041075577959418\n",
      "Epoch 245: loss = 0.0016041515627875924\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 245 with loss 0.0016041515627875924\n",
      "Epoch 246: loss = 0.0016035886947065592\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 246 with loss 0.0016035886947065592\n",
      "Epoch 247: loss = 0.001602140604518354\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 248: loss = 0.0016012565465644002\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 248 with loss 0.0016012565465644002\n",
      "Epoch 249: loss = 0.0016018294263631105\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 249 with loss 0.0016018294263631105\n",
      "Epoch 250: loss = 0.0016006875084713101\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 251: loss = 0.0015999042661860585\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 251 with loss 0.0015999042661860585\n",
      "Epoch 252: loss = 0.0015997664304450154\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 252 with loss 0.0015997664304450154\n",
      "Epoch 253: loss = 0.0015993110137060285\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 253 with loss 0.0015993110137060285\n",
      "Epoch 254: loss = 0.001599054434336722\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 254 with loss 0.001599054434336722\n",
      "Epoch 255: loss = 0.0015987238148227334\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 255 with loss 0.0015987238148227334\n",
      "Epoch 256: loss = 0.001598239759914577\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 256 with loss 0.001598239759914577\n",
      "Epoch 257: loss = 0.0015981988981366158\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 257 with loss 0.0015981988981366158\n",
      "Epoch 258: loss = 0.0015976298600435257\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 258 with loss 0.0015976298600435257\n",
      "Epoch 259: loss = 0.0015975086716935039\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 259 with loss 0.0015975086716935039\n",
      "Epoch 260: loss = 0.0015969793312251568\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 260 with loss 0.0015969793312251568\n",
      "Epoch 261: loss = 0.0015968444058671594\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 261 with loss 0.0015968444058671594\n",
      "Epoch 262: loss = 0.0015963045880198479\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 262 with loss 0.0015963045880198479\n",
      "Epoch 263: loss = 0.0015960746677592397\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 263 with loss 0.0015960746677592397\n",
      "Epoch 264: loss = 0.0015956941060721874\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 264 with loss 0.0015956941060721874\n",
      "Epoch 265: loss = 0.0015954052796587348\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 265 with loss 0.0015954052796587348\n",
      "Epoch 266: loss = 0.0015950185479596257\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 266 with loss 0.0015950185479596257\n",
      "Epoch 267: loss = 0.001594580477103591\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 267 with loss 0.001594580477103591\n",
      "Epoch 268: loss = 0.0015942632453516126\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 268 with loss 0.0015942632453516126\n",
      "Epoch 269: loss = 0.0015939042204990983\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 269 with loss 0.0015939042204990983\n",
      "Epoch 270: loss = 0.0015937372809275985\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 270 with loss 0.0015937372809275985\n",
      "Epoch 271: loss = 0.0015932641690596938\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 271 with loss 0.0015932641690596938\n",
      "Epoch 272: loss = 0.0015931224916130304\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 272 with loss 0.0015931224916130304\n",
      "Epoch 273: loss = 0.0015916831325739622\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 274: loss = 0.001591825857758522\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 274 with loss 0.001591825857758522\n",
      "Epoch 275: loss = 0.0015917436685413122\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 275 with loss 0.0015917436685413122\n",
      "Epoch 276: loss = 0.001590902335010469\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 276 with loss 0.001590902335010469\n",
      "Epoch 277: loss = 0.001591002568602562\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 277 with loss 0.001591002568602562\n",
      "Epoch 278: loss = 0.0015905547188594937\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 278 with loss 0.0015905547188594937\n",
      "Epoch 279: loss = 0.001590730738826096\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 279 with loss 0.001590730738826096\n",
      "Epoch 280: loss = 0.0015907754423096776\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 280 with loss 0.0015907754423096776\n",
      "Epoch 281: loss = 0.001591381966136396\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 281 with loss 0.001591381966136396\n",
      "Epoch 282: loss = 0.0015924021136015654\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 283: loss = 0.0015926025807857513\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 283 with loss 0.0015926025807857513\n",
      "Epoch 284: loss = 0.0015900421421974897\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 285: loss = 0.0015906182816252112\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 285 with loss 0.0015906182816252112\n",
      "Epoch 286: loss = 0.0015896328259259462\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 286 with loss 0.0015896328259259462\n",
      "Epoch 287: loss = 0.0015894208336248994\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 287 with loss 0.0015894208336248994\n",
      "Epoch 288: loss = 0.0015897073317319155\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 288 with loss 0.0015897073317319155\n",
      "Epoch 289: loss = 0.0015888088382780552\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 289 with loss 0.0015888088382780552\n",
      "Epoch 290: loss = 0.0015892086084932089\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 290 with loss 0.0015892086084932089\n",
      "Epoch 291: loss = 0.0015888524940237403\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 291 with loss 0.0015888524940237403\n",
      "Epoch 292: loss = 0.0015882401494309306\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 292 with loss 0.0015882401494309306\n",
      "Epoch 293: loss = 0.0015882476000115275\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 293 with loss 0.0015882476000115275\n",
      "Epoch 294: loss = 0.0015879564452916384\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 294 with loss 0.0015879564452916384\n",
      "Epoch 295: loss = 0.001587082864716649\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 295 with loss 0.001587082864716649\n",
      "Epoch 296: loss = 0.0015875115059316158\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 296 with loss 0.0015875115059316158\n",
      "Epoch 297: loss = 0.001587269944138825\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 297 with loss 0.001587269944138825\n",
      "Epoch 298: loss = 0.0015860905405133963\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 299: loss = 0.0015860197599977255\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 299 with loss 0.0015860197599977255\n",
      "Epoch 300: loss = 0.0015862868167459965\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 300 with loss 0.0015862868167459965\n",
      "Epoch 301: loss = 0.0015855637611821294\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 301 with loss 0.0015855637611821294\n",
      "Epoch 302: loss = 0.0015852210344746709\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 302 with loss 0.0015852210344746709\n",
      "Epoch 303: loss = 0.001585233025252819\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 303 with loss 0.001585233025252819\n",
      "Epoch 304: loss = 0.001583823119290173\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 305: loss = 0.001584459445439279\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 305 with loss 0.001584459445439279\n",
      "Epoch 306: loss = 0.0015845211455598474\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 306 with loss 0.0015845211455598474\n",
      "Epoch 307: loss = 0.0015843939036130905\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 307 with loss 0.0015843939036130905\n",
      "Epoch 308: loss = 0.0015838418621569872\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 308 with loss 0.0015838418621569872\n",
      "Epoch 309: loss = 0.0015835982048884034\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 309 with loss 0.0015835982048884034\n",
      "Epoch 310: loss = 0.0015832502394914627\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 310 with loss 0.0015832502394914627\n",
      "Epoch 311: loss = 0.0015829665353521705\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 311 with loss 0.0015829665353521705\n",
      "Epoch 312: loss = 0.001582828932441771\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 312 with loss 0.001582828932441771\n",
      "Epoch 313: loss = 0.0015829296316951513\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 313 with loss 0.0015829296316951513\n",
      "Epoch 314: loss = 0.001584355952218175\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 315: loss = 0.0015866366447880864\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 316: loss = 0.0015822779387235641\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 317: loss = 0.0015829243930056691\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 317 with loss 0.0015829243930056691\n",
      "Epoch 318: loss = 0.0015817297389730811\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 319: loss = 0.0015821837587282062\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 319 with loss 0.0015821837587282062\n",
      "Epoch 320: loss = 0.001583330798894167\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 321: loss = 0.00158489600289613\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 322: loss = 0.0015837064711377025\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 323: loss = 0.00158231845125556\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 324: loss = 0.0015827483730390668\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 324 with loss 0.0015827483730390668\n",
      "Epoch 325: loss = 0.001580662908963859\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 326: loss = 0.0015813378849998116\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 326 with loss 0.0015813378849998116\n",
      "Epoch 327: loss = 0.0015804932918399572\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 327 with loss 0.0015804932918399572\n",
      "Epoch 328: loss = 0.0015803549904376268\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 328 with loss 0.0015803549904376268\n",
      "Epoch 329: loss = 0.001580188050866127\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 329 with loss 0.001580188050866127\n",
      "Epoch 330: loss = 0.0015795431099832058\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 330 with loss 0.0015795431099832058\n",
      "Epoch 331: loss = 0.0015792648773640394\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 331 with loss 0.0015792648773640394\n",
      "Epoch 332: loss = 0.0015790869947522879\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 332 with loss 0.0015790869947522879\n",
      "Epoch 333: loss = 0.0015785658033564687\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 333 with loss 0.0015785658033564687\n",
      "Epoch 334: loss = 0.001578823896124959\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 334 with loss 0.001578823896124959\n",
      "Epoch 335: loss = 0.0015778378583490849\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 335 with loss 0.0015778378583490849\n",
      "Epoch 336: loss = 0.0015781241236254573\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 336 with loss 0.0015781241236254573\n",
      "Epoch 337: loss = 0.00157747115008533\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 337 with loss 0.00157747115008533\n",
      "Epoch 338: loss = 0.001577268005348742\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 338 with loss 0.001577268005348742\n",
      "Epoch 339: loss = 0.0015766012948006392\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 339 with loss 0.0015766012948006392\n",
      "Epoch 340: loss = 0.0015765524003654718\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 340 with loss 0.0015765524003654718\n",
      "Epoch 341: loss = 0.0015751032624393702\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 342: loss = 0.0015756594948470592\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 342 with loss 0.0015756594948470592\n",
      "Epoch 343: loss = 0.0015743685653433204\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 344: loss = 0.0015755875501781702\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 345: loss = 0.0015749582089483738\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 345 with loss 0.0015749582089483738\n",
      "Epoch 346: loss = 0.001574915717355907\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 346 with loss 0.001574915717355907\n",
      "Epoch 347: loss = 0.0015747423749417067\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 347 with loss 0.0015747423749417067\n",
      "Epoch 348: loss = 0.0015730081358924508\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 349: loss = 0.0015728470170870423\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 349 with loss 0.0015728470170870423\n",
      "Epoch 350: loss = 0.0015742771793156862\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 351: loss = 0.0015828104224056005\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 352: loss = 0.0015750303864479065\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 353: loss = 0.0015749878948554397\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 353 with loss 0.0015749878948554397\n",
      "Epoch 354: loss = 0.0015724854310974479\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 355: loss = 0.0015725400298833847\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 355 with loss 0.0015725400298833847\n",
      "Epoch 356: loss = 0.0015730008017271757\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 356 with loss 0.0015730008017271757\n",
      "Epoch 357: loss = 0.0015728649450466037\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 357 with loss 0.0015728649450466037\n",
      "Epoch 358: loss = 0.0015730131417512894\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 358 with loss 0.0015730131417512894\n",
      "Epoch 359: loss = 0.0015722588868811727\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 359 with loss 0.0015722588868811727\n",
      "Epoch 360: loss = 0.001572183333337307\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 360 with loss 0.001572183333337307\n",
      "Epoch 361: loss = 0.0015720580704510212\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 361 with loss 0.0015720580704510212\n",
      "Epoch 362: loss = 0.0015717813512310386\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 362 with loss 0.0015717813512310386\n",
      "Epoch 363: loss = 0.001571883331052959\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 363 with loss 0.001571883331052959\n",
      "Epoch 364: loss = 0.001571645145304501\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 364 with loss 0.001571645145304501\n",
      "Epoch 365: loss = 0.0015715373447164893\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 365 with loss 0.0015715373447164893\n",
      "Epoch 366: loss = 0.0015712459571659565\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 366 with loss 0.0015712459571659565\n",
      "Epoch 367: loss = 0.0015708630671724677\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 367 with loss 0.0015708630671724677\n",
      "Epoch 368: loss = 0.0015704832039773464\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 368 with loss 0.0015704832039773464\n",
      "Epoch 369: loss = 0.0015703154494985938\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 369 with loss 0.0015703154494985938\n",
      "Epoch 370: loss = 0.0015699492068961263\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 370 with loss 0.0015699492068961263\n",
      "Epoch 371: loss = 0.001569737563841045\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 371 with loss 0.001569737563841045\n",
      "Epoch 372: loss = 0.0015694594476372004\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 372 with loss 0.0015694594476372004\n",
      "Epoch 373: loss = 0.001569087035022676\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 373 with loss 0.001569087035022676\n",
      "Epoch 374: loss = 0.0015688104322180152\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 374 with loss 0.0015688104322180152\n",
      "Epoch 375: loss = 0.0015683581586927176\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 375 with loss 0.0015683581586927176\n",
      "Epoch 376: loss = 0.0015680190408602357\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 376 with loss 0.0015680190408602357\n",
      "Epoch 377: loss = 0.0015676419716328382\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 377 with loss 0.0015676419716328382\n",
      "Epoch 378: loss = 0.0015673767775297165\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 378 with loss 0.0015673767775297165\n",
      "Epoch 379: loss = 0.0015670499997213483\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 379 with loss 0.0015670499997213483\n",
      "Epoch 380: loss = 0.0015667642001062632\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 380 with loss 0.0015667642001062632\n",
      "Epoch 381: loss = 0.001566366059705615\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 381 with loss 0.001566366059705615\n",
      "Epoch 382: loss = 0.0015660611679777503\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 382 with loss 0.0015660611679777503\n",
      "Epoch 383: loss = 0.0015656535979360342\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 383 with loss 0.0015656535979360342\n",
      "Epoch 384: loss = 0.001565367914736271\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 384 with loss 0.001565367914736271\n",
      "Epoch 385: loss = 0.00156505242921412\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 385 with loss 0.00156505242921412\n",
      "Epoch 386: loss = 0.0015647836262360215\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 386 with loss 0.0015647836262360215\n",
      "Epoch 387: loss = 0.001564362901262939\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 387 with loss 0.001564362901262939\n",
      "Epoch 388: loss = 0.0015640846686437726\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 388 with loss 0.0015640846686437726\n",
      "Epoch 389: loss = 0.0015638497425243258\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 389 with loss 0.0015638497425243258\n",
      "Epoch 390: loss = 0.001563744037412107\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 390 with loss 0.001563744037412107\n",
      "Epoch 391: loss = 0.0015638386830687523\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 391 with loss 0.0015638386830687523\n",
      "Epoch 392: loss = 0.0015646807150915265\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 392 with loss 0.0015646807150915265\n",
      "Epoch 393: loss = 0.0015656471950933337\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 393 with loss 0.0015656471950933337\n",
      "Epoch 394: loss = 0.001564714009873569\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 394 with loss 0.001564714009873569\n",
      "Epoch 395: loss = 0.0015621376223862171\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 396: loss = 0.001562011893838644\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 396 with loss 0.001562011893838644\n",
      "Epoch 397: loss = 0.001562523772008717\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 397 with loss 0.001562523772008717\n",
      "Epoch 398: loss = 0.0015610242262482643\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Epoch 399: loss = 0.0015613347059115767\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 399 with loss 0.0015613347059115767\n",
      "Epoch 400: loss = 0.0015609711408615112\n",
      "before loss.backward(): Allocated: 222.207488 MB, Reserved: 639.63136 MB\n",
      "After loss.backward(): Allocated: 143.83872 MB, Reserved: 639.63136 MB\n",
      "-----------------\n",
      "Converged at epoch 400 with loss 0.0015609711408615112\n",
      "Saved to gargoylevoroloss_to_clip.npz\n",
      "Sites length:  32768\n",
      "min sites:  tensor(-1., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "max sites:  tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "site_file_path = f'{destination}{max_iter}_cvt_{lambda_cvt}_chamfer_{lambda_chamfer}_eikonal_{lambda_eikonal}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)\n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "\n",
    "#     with torch.profiler.profile(activities=[\n",
    "#             torch.profiler.ProfilerActivity.CPU,\n",
    "#             torch.profiler.ProfilerActivity.CUDA,\n",
    "#         ],\n",
    "#         record_shapes=False,\n",
    "#         with_stack=True  # Captures function calls\n",
    "#     ) as prof:\n",
    "#         sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=1, lambda_weights=lambda_weights)\n",
    "#         torch.cuda.synchronize()\n",
    "# # \n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
    "#     prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    # \n",
    "    sites = train_DCCVT(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}_chamfer{lambda_chamfer}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ./images/autograd/End2End_DCCVT/gargoyle400_400_3d_model_32768_chamfer1000.pth\n",
      "sites ./images/autograd/End2End_DCCVT/gargoyle400_400_3d_sites_32768_chamfer1000.pth\n",
      "sites_np shape:  (32768, 3)\n"
     ]
    }
   ],
   "source": [
    "epoch = 400\n",
    "\n",
    "model_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_model_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}_chamfer{lambda_chamfer}.pth'\n",
    " \n",
    "sites = torch.load(site_file_path)\n",
    "\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "#\n",
    "#polyscope_sdf(model)\n",
    "#\n",
    "print(\"model\", model_file_path)\n",
    "print(\"sites\", site_file_path)\n",
    "ps_cloud = ps.register_point_cloud(f\"{epoch} epoch_cvt_grid\",sites_np)\n",
    "\n",
    "print(\"sites_np shape: \", sites_np.shape)\n",
    "\n",
    "#print sites if Nan\n",
    "if np.isnan(sites_np).any():\n",
    "    print(\"sites_np contains NaN values\")\n",
    "    print(\"sites_np NaN values: \", np.isnan(sites_np).sum())\n",
    "#remove nan values from sites tensor\n",
    "sites_np = sites_np[~np.isnan(sites_np).any(axis=1)]\n",
    "sites = torch.from_numpy(sites_np).to(device).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9772bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = su.get_zero_crossing_mesh_3d(sites, model)\n",
    "#ps.register_surface_mesh(\"Zero-Crossing faces direct\", final_mesh[0], final_mesh[1])\n",
    "\n",
    "#save to file\n",
    "final_mesh_file = f'{mesh[0]}voroloss_sdf_trained{model_trained_it}.npz'\n",
    "faces = np.array(final_mesh[1], dtype=object)\n",
    "np.savez(final_mesh_file, vertices=final_mesh[0], faces=faces)\n",
    "\n",
    "data = np.load(final_mesh_file, allow_pickle=True)\n",
    "verts = data[\"vertices\"]       # (N_vertices, 3)\n",
    "faces = data[\"faces\"].tolist() # back to a list of lists\n",
    "\n",
    "# print(\"Zero-Crossing faces final shape: \", verts.shape)\n",
    "# ps.register_surface_mesh(\"Zero-Crossing faces final\", verts, faces, back_face_policy=\"identical\")\n",
    "\n",
    "#v_vect, f_vect = su.get_clipped_mesh_torch(sites, model, None, batch_size=3072)\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, True)\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final clipped polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final clipped triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "\n",
    "v_vect, f_vect = su.get_clipped_mesh_numba(sites, model, None, False)\n",
    "\n",
    "#only for voroloss case\n",
    "ps.register_surface_mesh(\"final polygon mesh\", v_vect.detach().cpu().numpy(), f_vect)\n",
    "# fanning to transform polygon faces to triangle faces\n",
    "triangle_faces = [[f[0], f[i], f[i+1]] for f in f_vect for i in range(1, len(f)-1)]\n",
    "ps.register_surface_mesh(\"final triangle mesh\", v_vect.detach().cpu().numpy(), triangle_faces, back_face_policy=\"identical\")\n",
    "\n",
    "# triangle_faces = torch.tensor(triangle_faces, device=device)\n",
    "# s_p = su.sample_mesh_points(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"sampled clipped mesh\", s_p.detach().cpu().numpy())\n",
    "\n",
    "# hs_p = su.sample_mesh_points_heitz(v_vect, triangle_faces, num_samples=150*32**2)\n",
    "# ps.register_point_cloud(\"heitz clipped mesh\", hs_p.detach().cpu().numpy())\n",
    "\n",
    "# ##register original mesh\n",
    "# mesh_file = mesh[1]+\".stl\"\n",
    "# #load mesh \n",
    "# m = trimesh.load(mesh_file)\n",
    "# #convert to numpy\n",
    "# mesh_np = np.array(m.vertices)\n",
    "# #normalize mesh\n",
    "# mesh_np = mesh_np - np.mean(mesh_np, axis=0)\n",
    "# mesh_np = mesh_np / np.max(np.abs(mesh_np))\n",
    "# mesh_faces = np.array(m.faces)\n",
    "# ps.register_surface_mesh(\"Original Mesh\", mesh_np, mesh_faces)\n",
    "\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "tri = Delaunay(sites_np)\n",
    "delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "sdf_values = model(sites)\n",
    "\n",
    "# Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "vertices = vertices_list[0]\n",
    "faces = faces_list[0]\n",
    "vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "ps.register_surface_mesh(\"Marching Tetrahedra Mesh final\", vertices_np, faces_np)\n",
    "\n",
    "# clipped_cvt = \"clipped_CVT.obj\"\n",
    "# if os.path.exists(clipped_cvt):\n",
    "#     clipped_cvt_mesh = trimesh.load(clipped_cvt)\n",
    "#     ps.register_surface_mesh(\"Clipped CVT\", clipped_cvt_mesh.vertices, clipped_cvt_mesh.faces)\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
