{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Using device:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import interactive_polyscope\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d, Delaunay\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import trimesh\n",
    "import diffvoronoi\n",
    "\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.03\n",
    "lr_model = 0.0003\n",
    "iterations = 5000\n",
    "save_every = 100\n",
    "max_iter = 100\n",
    "#learning_rate = 0.03\n",
    "destination = \"./images/autograd/3D/TrueSDF/\"\n",
    "mesh = [\"chair\", \"./Resources/chair_low.obj\"]\n",
    "mesh = [\"chair\", \"./Resources/chair_low.obj\"]\n",
    "#mesh = [\"bunny\", \"./Resources/stanford-bunny.obj\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshgrid shape: torch.Size([3375, 3])\n",
      "Meshgrid 1st 5: tensor([[-2.0000, -2.0000, -2.0000],\n",
      "        [-2.0000, -2.0000, -1.7143],\n",
      "        [-2.0000, -2.0000, -1.4286],\n",
      "        [-2.0000, -2.0000, -1.1429],\n",
      "        [-2.0000, -2.0000, -0.8571]])\n",
      "Meshgrid 1st 5: tensor([[-1.9937, -1.9908, -1.9943],\n",
      "        [-1.9971, -1.9894, -1.7230],\n",
      "        [-2.0170, -2.0182, -1.4322],\n",
      "        [-1.9855, -2.0185, -1.1252],\n",
      "        [-1.9916, -1.9875, -0.8528]])\n",
      "Max dim: tensor([2.0381, 2.0246, 2.0244], grad_fn=<MaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#currently sites are between -5 and 5 in all 3 dimensions\n",
    "# check if sites exists\n",
    "#num_centroids = 16*16*16\n",
    "#num_centroids = 24**3\n",
    "num_centroids = 16**3\n",
    "site_fp = f'sites_{num_centroids}_{input_dims}.pt'\n",
    "\n",
    "if os.path.exists(site_fp):\n",
    "    sites = torch.load(site_fp)\n",
    "    print(\"Sites loaded:\", sites.shape)\n",
    "else:\n",
    "    num_centroids = int(num_centroids**(1/3))\n",
    "    domain = 2\n",
    "    x = torch.linspace(-domain, domain, num_centroids)\n",
    "    y = torch.linspace(-domain, domain, num_centroids)\n",
    "    z = torch.linspace(-domain, domain, num_centroids)\n",
    "    meshgrid = torch.meshgrid(x, y, z)\n",
    "    meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "    print(\"Meshgrid shape:\", meshgrid.shape)\n",
    "    print(\"Meshgrid 1st 5:\", meshgrid[:5])\n",
    "    #add noise to meshgrid\n",
    "    meshgrid += torch.randn_like(meshgrid) * 0.01\n",
    "    print(\"Meshgrid 1st 5:\", meshgrid[:5])\n",
    "\n",
    "    sites = meshgrid.to(device, dtype=torch.double).requires_grad_(True)\n",
    "    \n",
    "    max_dim = torch.max(sites, dim=0)[0]\n",
    "    print(\"Max dim:\", max_dim)\n",
    "# else:\n",
    "#     print(\"Creating new sites\")\n",
    "#     sites = su.createCVTgrid(num_centroids=num_centroids, dimensionality=input_dims)\n",
    "#     #save the initial sites torch tensor\n",
    "#     torch.save(sites, site_fp)\n",
    "\n",
    "\n",
    "def plot_voronoi_3d(sites, xlim=5, ylim=5, zlim=5):\n",
    "    import numpy as np\n",
    "    import pyvoro\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "    # initialize random number generator\n",
    "    rng = np.random.default_rng(11)\n",
    "    # create a set of points in 3D\n",
    "    points = sites.detach().cpu().numpy()\n",
    "\n",
    "    # use pyvoro to compute the Voronoi tessellation\n",
    "    # the second argument gives the the axis limits in x,y and z direction\n",
    "    # in this case all between 0 and 1.\n",
    "    # the third argument gives \"dispersion = max distance between two points\n",
    "    # that might be adjacent\" (not sure how exactly this works)\n",
    "    voronoi = pyvoro.compute_voronoi(points,[[-xlim,xlim],[-ylim,ylim],[-zlim,zlim]],1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # for each Voronoi cell, plot all the faces of the corresponding polygon\n",
    "    for vnoicell in voronoi:\n",
    "        faces = []\n",
    "        # the vertices are the corner points of the Voronoi cell\n",
    "        vertices = np.array(vnoicell['vertices'])\n",
    "        # cycle through all faces of the polygon\n",
    "        for face in vnoicell['faces']:\n",
    "            faces.append(vertices[np.array(face['vertices'])])\n",
    "            \n",
    "        # join the faces into a 3D polygon\n",
    "        polygon = Poly3DCollection(faces, alpha=0.5, \n",
    "                                facecolors=rng.uniform(0,1,3),\n",
    "                                linewidths=0.5,edgecolors='black')\n",
    "        ax.add_collection3d(polygon)\n",
    "    \n",
    "    ax.set_xlim([-xlim,xlim])\n",
    "    ax.set_ylim([-ylim,ylim])\n",
    "    ax.set_zlim([-zlim,zlim])\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "#plot_voronoi_3d(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 570.144\n"
     ]
    }
   ],
   "source": [
    "ps.init()\n",
    "#ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Resources/chair_low.obj with shape (128, 128, 128)\n",
      "torch.Size([3375])\n"
     ]
    }
   ],
   "source": [
    "class SDFGrid:\n",
    "    def __init__(self, filename=\"./Resources/dolphin.obj\"):\n",
    "        self.filename = filename\n",
    "        self.sdf_grid = np.load(filename[:-4] + '.npy')\n",
    "        self.grid = torch.tensor(self.sdf_grid, device=device)\n",
    "        self.pc = self.pointcloud()\n",
    "\n",
    "    def pointcloud(self):\n",
    "        # Load the mesh\n",
    "        mesh = trimesh.load(self.filename)\n",
    "        # Sample points from the mesh\n",
    "        points = mesh.sample(32*32*32)\n",
    "        # Convert to torch tensor\n",
    "        points = torch.tensor(np.array(points), dtype=torch.float64, device=device)\n",
    "        points = points - torch.mean(points, dim=0)  # Center the points\n",
    "        points = points / torch.max(torch.abs(points))\n",
    "        return points\n",
    "\n",
    "    def sdf(self, sites):\n",
    "        gridsize = self.sdf_grid.shape[0]  # Assuming a cubic grid of size (N,N,N)\n",
    "        \n",
    "        # Normalize points to [0, 1] range in all dimensions\n",
    "        max_dim = torch.max(sites).floor().item()\n",
    "        sites = sites +  max_dim\n",
    "        points_normalized = sites / (2*max_dim)\n",
    "        \n",
    "        \n",
    "        # sites = sites + 2 #shift to 0-10\n",
    "        # points_normalized = sites / 4\n",
    "        \n",
    "\n",
    "        # Scale to grid coordinates\n",
    "        points_grid = points_normalized * (gridsize - 1)\n",
    "\n",
    "        # Separate grid coordinates into integer and fractional parts\n",
    "        x, y, z = points_grid[:, 0], points_grid[:, 1], points_grid[:, 2]\n",
    "        x0 = x.floor().long().clamp(0, gridsize - 1)\n",
    "        y0 = y.floor().long().clamp(0, gridsize - 1)\n",
    "        z0 = z.floor().long().clamp(0, gridsize - 1)\n",
    "        x1 = (x0 + 1).clamp(0, gridsize - 1)\n",
    "        y1 = (y0 + 1).clamp(0, gridsize - 1)\n",
    "        z1 = (z0 + 1).clamp(0, gridsize - 1)\n",
    "        dx, dy, dz = x - x0, y - y0, z - z0\n",
    "\n",
    "        # Perform trilinear interpolation\n",
    "        values = (\n",
    "            (1 - dx) * (1 - dy) * (1 - dz) * self.grid[x0, y0, z0] +\n",
    "            dx * (1 - dy) * (1 - dz) * self.grid[x1, y0, z0] +\n",
    "            (1 - dx) * dy * (1 - dz) * self.grid[x0, y1, z0] +\n",
    "            dx * dy * (1 - dz) * self.grid[x1, y1, z0] +\n",
    "            (1 - dx) * (1 - dy) * dz * self.grid[x0, y0, z1] +\n",
    "            dx * (1 - dy) * dz * self.grid[x1, y0, z1] +\n",
    "            (1 - dx) * dy * dz * self.grid[x0, y1, z1] +\n",
    "            dx * dy * dz * self.grid[x1, y1, z1]\n",
    "        )\n",
    "\n",
    "        return values\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.filename} with shape {self.sdf_grid.shape}\"\n",
    "\n",
    "model = SDFGrid(mesh[1])\n",
    "sdf_values = model.sdf(sites)\n",
    "print(model)\n",
    "print(sdf_values.shape)\n",
    "# ps_cloud = ps.register_point_cloud(\"sites_w_sdf\", sites.detach().cpu().numpy())\n",
    "# ps_cloud.add_scalar_quantity(\"sdf_values\", sdf_values.detach().cpu().numpy(), enabled=True)\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208675/3413510709.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sites = torch.tensor(sites, device=device, dtype=torch.double).requires_grad_(True)\n"
     ]
    }
   ],
   "source": [
    "near_target_pc = model.pc + torch.randn_like(model.pc) * 0.1\n",
    "sites = torch.cat((sites, near_target_pc), dim=0)\n",
    "\n",
    "near_target_pc = model.pc + torch.randn_like(model.pc) * 0.1\n",
    "sites = torch.cat((sites, near_target_pc), dim=0)\n",
    "\n",
    "sites = torch.tensor(sites, device=device, dtype=torch.double).requires_grad_(True)\n",
    "\n",
    "sdf_values_trilinear = model.sdf(sites)\n",
    "\n",
    "#ps.register_point_cloud(\"model_pc\", model.pc.detach().cpu().numpy())\n",
    "\n",
    "#use torch.nn.functionnal.grid_sample\n",
    "#df_Values_gridsample = torch.nn.functional.grid_sample(sites, torch.tensor(sdf_points, device=device), mode='bilinear', padding_mode='border')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_crossing_mesh_3d(sites, model):\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    vor = Voronoi(sites_np)  # Compute 3D Voronoi diagram\n",
    "\n",
    "    sdf_values = model.sdf(sites).detach().cpu().numpy()  # Compute SDF values\n",
    "\n",
    "    valid_faces = []  # List of polygonal faces\n",
    "    used_vertices = set()  # Set of indices for valid vertices\n",
    "\n",
    "    for (point1, point2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:\n",
    "            continue  # Skip infinite ridges\n",
    "\n",
    "        # Check if SDF changes sign across this ridge\n",
    "        if np.sign(sdf_values[point1]) != np.sign(sdf_values[point2]):\n",
    "            valid_faces.append(ridge_vertices)\n",
    "            used_vertices.update(ridge_vertices)\n",
    "\n",
    "    # **Filter Voronoi vertices**\n",
    "    used_vertices = sorted(used_vertices)  # Keep unique, sorted indices\n",
    "    vertex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(used_vertices)}\n",
    "    filtered_vertices = vor.vertices[used_vertices]\n",
    "\n",
    "    # **Re-index faces to match the new filtered vertex list**\n",
    "    filtered_faces = [[vertex_map[v] for v in face] for face in valid_faces]\n",
    "\n",
    "    return filtered_vertices, filtered_faces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polyscope.surface_mesh.SurfaceMesh at 0x7c6c08077280>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ps_cloud = ps.register_point_cloud(\"sites_w_sdf\", sites.detach().cpu().numpy())\n",
    "# ps_cloud.add_scalar_quantity(\"sdf_values_trilinear\", sdf_values_trilinear.detach().cpu().numpy(), enabled=True)\n",
    "# # # ps_cloud.add_scalar_quantity(\"sdf_Values_gridsample\", sdf_Values_gridsample.detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "\n",
    "# #ps.register_point_cloud(\"sites_w_zeroes_sdf_trilinear\", sites.detach().cpu().numpy()[sdf_values_trilinear.detach().cpu().numpy()[:] <= 0])\n",
    "# # # ps.register_point_cloud(\"sites_w_zeroes_sdf_gridsample\", sites.detach().cpu().numpy()[sdf_Values_gridsample.detach().cpu().numpy()[:] <= 0])\n",
    "\n",
    "initial_mesh = get_zero_crossing_mesh_3d(sites, model)\n",
    "ps.register_surface_mesh(\"initial_mesh Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "# # ps.register_point_cloud(\"initial_mesh vertices\", initial_mesh[0])\n",
    "\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "edge_smoothing_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "zero_target_points_loss_values = []\n",
    "sdf_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.5, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_sdf = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    lambda_laplace = lambda_weights[3]\n",
    "    lambda_chamfer = lambda_weights[4]\n",
    "    lambda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    lambda_target_points = lambda_weights[7]\n",
    "    \n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        sites_np = sites.detach().cpu().numpy()\n",
    "        \n",
    "        d3dsimplices = diffvoronoi.get_delaunay_simplices(sites_np.reshape(input_dims*sites_np.shape[0]))\n",
    "        d3dsimplices = np.array(d3dsimplices)\n",
    "        print(\"d3dsimplices shape: \", d3dsimplices.shape)\n",
    "\n",
    "        vertices_to_compute, bisectors_to_compute = su.compute_zero_crossing_vertices_3d(sites, None, None, d3dsimplices, model)\n",
    "        \n",
    "        vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        #combine vertices and bisectors to one tensor for chamfer\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "        from pytorch3d.loss import chamfer_distance\n",
    "        chamfer_loss, _ = chamfer_distance(model.pc.unsqueeze(0).detach(), ((points-torch.mean(points, dim=0))/ torch.max(torch.abs(points))).unsqueeze(0))\n",
    "#        print(\"chamfer_loss: \", chamfer_loss.item())\n",
    "\n",
    "        # Compute losses       \n",
    "        cvt_loss = lf.compute_cvt_loss_vectorized_delaunay(sites, None, d3dsimplices)\n",
    "        \n",
    "        sdf_loss = torch.mean(model.sdf(points)**2)        \n",
    "        \n",
    "        #laplacian_loss = lf.compute_ridge_smoothing_loss(bisectors_to_compute, sites, model)\n",
    "        #laplacian_loss = mean_curvature_loss(ridge_vertices_pairs, sites)\n",
    "\n",
    "        # Track raw losses (unweighted)\n",
    "        cvt_loss_values.append(cvt_loss.item())\n",
    "        sdf_loss_values.append(sdf_loss.item())\n",
    "        #edge_smoothing_loss_values.append(laplacian_loss.item())\n",
    "  \n",
    "        loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            lambda_sdf * sdf_loss +\n",
    "            lambda_chamfer * chamfer_loss \n",
    "            #lambda_laplace * laplacian_loss\n",
    "        )\n",
    "        loss_values.append(loss.item())\n",
    "        #print(f\"cvt_loss: {cvt_loss}, laplace_loss: {laplacian_loss}, \")\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            if upsampled > 0:\n",
    "                print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        if epoch/max_iter > (0.5)*(upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            \n",
    "            #new_sites = su.upsampling_inside(best_sites, model)\n",
    "            #new_sites = su.adaptive_density_upsampling(best_sites, model)\n",
    "            \n",
    "            #sites = su.add_upsampled_sites(best_sites, new_sites)\n",
    "            \n",
    "            \n",
    "            sites = su.upsampling_vectorized(sites, tri=None, vor=None, simplices=d3dsimplices, model=model)\n",
    "            \n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            #print(\"upsampled sites length: \",len(sites))\n",
    "            \n",
    "            #best_sites = sites.clone()\n",
    "            #best_sites.best_loss = best_loss\n",
    "            \n",
    "            optimizer = torch.optim.Adam([#{'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "                                          {'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}.pth'\n",
    "            torch.save(sites, site_file_path)\n",
    "        \n",
    "        epoch += 1           \n",
    "        \n",
    "    return best_sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d3dsimplices shape:  (464452, 4)\n"
     ]
    }
   ],
   "source": [
    "lambda_weights = [20.01,2,0,0,2,0,0,0]\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lamda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_target_points = lambda_weights[7]\n",
    "max_iter = 100\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}3d_sites_{num_centroids}.npy'\n",
    "#check if optimized sites file exists\n",
    "if not os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)    \n",
    "else:\n",
    "    # import cProfile, pstats\n",
    "    # import time\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=0, lambda_weights=lambda_weights)\n",
    "    \n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    # stats.print_stats()\n",
    "    # stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "#plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'destination' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m site_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmesh[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmax_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_3d_sites_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_centroids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m sites \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(site_file_path)\n\u001b[1;32m      5\u001b[0m sites_np \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'destination' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}.pth'\n",
    "sites = torch.load(site_file_path)\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "#\n",
    "print(\"sites\", site_file_path)\n",
    "sdf_values_trilinear = model.sdf(sites)\n",
    "\n",
    "ps.register_point_cloud(\"final_sites_w_zeroes_sdf_trilinear\", sites.detach().cpu().numpy()[sdf_values_trilinear.detach().cpu().numpy()[:] <= 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# tri = Delaunay(sites_np)\n",
    "# delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "# sdf_values = model.sdf(sites)\n",
    "\n",
    "\n",
    "# # Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "# sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# # Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "# sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "#marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "# vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "# vertices = vertices_list[0]\n",
    "# faces = faces_list[0]\n",
    "# vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "# faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "# ps.register_surface_mesh(\"Marching Tetrahedra Mesh\", vertices_np, faces_np)\n",
    "\n",
    "\n",
    "zero_crossing_final_mesh = get_zero_crossing_mesh_3d(sites, model)\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces\", zero_crossing_final_mesh[0], zero_crossing_final_mesh[1])\n",
    "#ps.register_point_cloud(\"Mesh vertices\", zero_crossing_final_mesh[0])\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
