{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polyscope as ps\n",
    "import interactive_polyscope\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d, Delaunay\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import sdfpred_utils.sdfpred_utils as su\n",
    "import sdfpred_utils.sdf_MLP as mlp\n",
    "import sdfpred_utils.sdf_functions as sdf\n",
    "import sdfpred_utils.loss_functions as lf\n",
    "import trimesh\n",
    "\n",
    "\n",
    "#cuda devices\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Using device: \", torch.cuda.get_device_name(device))\n",
    "\n",
    "#default tensor types\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "multires = 2\n",
    "input_dims = 3\n",
    "lr_sites = 0.03\n",
    "lr_model = 0.0003\n",
    "iterations = 5000\n",
    "save_every = 100\n",
    "max_iter = 100\n",
    "#learning_rate = 0.03\n",
    "destination = \"./images/autograd/3D/TrueSDF/\"\n",
    "mesh = [\"chair\", \"./Resources/chair_low.obj\"]\n",
    "mesh = [\"chair\", \"./Resources/chair_low.obj\"]\n",
    "#mesh = [\"bunny\", \"./Resources/stanford-bunny.obj\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toobig for createCVTgrid\n",
      "Meshgrid shape: torch.Size([16581375, 3])\n",
      "Meshgrid 1st 5: tensor([[-4.9000, -4.9000, -4.9000],\n",
      "        [-4.9000, -4.9000, -4.8614],\n",
      "        [-4.9000, -4.9000, -4.8228],\n",
      "        [-4.9000, -4.9000, -4.7843],\n",
      "        [-4.9000, -4.9000, -4.7457]])\n",
      "Meshgrid 1st 5: tensor([[-4.9014, -4.9803, -4.8741],\n",
      "        [-4.7217, -4.9711, -4.7129],\n",
      "        [-4.9745, -4.8654, -4.9572],\n",
      "        [-4.8902, -4.8421, -4.6739],\n",
      "        [-4.9502, -4.8006, -4.8620]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wylliam/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#currently sites are between -5 and 5 in all 3 dimensions\n",
    "# check if sites exists\n",
    "#num_centroids = 16*16*16\n",
    "#num_centroids = 24**3\n",
    "num_centroids = 20**3\n",
    "site_fp = f'sites_{num_centroids}_{input_dims}.pt'\n",
    "\n",
    "if os.path.exists(site_fp):\n",
    "    sites = torch.load(site_fp)\n",
    "    print(\"Sites loaded:\", sites.shape)\n",
    "elif num_centroids > 32*32*32:\n",
    "    print(\"toobig for createCVTgrid\")\n",
    "    #create meshgrid between -5 and 5 in 3D\n",
    "    num_centroids = int(num_centroids**(1/3))\n",
    "    domain = 4.9\n",
    "    x = torch.linspace(-domain, domain, num_centroids)\n",
    "    y = torch.linspace(-domain, domain, num_centroids)\n",
    "    z = torch.linspace(-domain, domain, num_centroids)\n",
    "    meshgrid = torch.meshgrid(x, y, z)\n",
    "    meshgrid = torch.stack(meshgrid, dim=3).view(-1, 3)\n",
    "    print(\"Meshgrid shape:\", meshgrid.shape)\n",
    "    print(\"Meshgrid 1st 5:\", meshgrid[:5])\n",
    "    #add noise to meshgrid\n",
    "    meshgrid += torch.randn_like(meshgrid) * 0.1\n",
    "    print(\"Meshgrid 1st 5:\", meshgrid[:5])\n",
    "\n",
    "    sites = meshgrid.to(device, dtype=torch.double).requires_grad_(True)\n",
    "else:\n",
    "    print(\"Creating new sites\")\n",
    "    sites = su.createCVTgrid(num_centroids=num_centroids, dimensionality=input_dims)\n",
    "    #save the initial sites torch tensor\n",
    "    torch.save(sites, site_fp)\n",
    "\n",
    "\n",
    "def plot_voronoi_3d(sites, xlim=5, ylim=5, zlim=5):\n",
    "    import numpy as np\n",
    "    import pyvoro\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "    # initialize random number generator\n",
    "    rng = np.random.default_rng(11)\n",
    "    # create a set of points in 3D\n",
    "    points = sites.detach().cpu().numpy()\n",
    "\n",
    "    # use pyvoro to compute the Voronoi tessellation\n",
    "    # the second argument gives the the axis limits in x,y and z direction\n",
    "    # in this case all between 0 and 1.\n",
    "    # the third argument gives \"dispersion = max distance between two points\n",
    "    # that might be adjacent\" (not sure how exactly this works)\n",
    "    voronoi = pyvoro.compute_voronoi(points,[[-xlim,xlim],[-ylim,ylim],[-zlim,zlim]],1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # for each Voronoi cell, plot all the faces of the corresponding polygon\n",
    "    for vnoicell in voronoi:\n",
    "        faces = []\n",
    "        # the vertices are the corner points of the Voronoi cell\n",
    "        vertices = np.array(vnoicell['vertices'])\n",
    "        # cycle through all faces of the polygon\n",
    "        for face in vnoicell['faces']:\n",
    "            faces.append(vertices[np.array(face['vertices'])])\n",
    "            \n",
    "        # join the faces into a 3D polygon\n",
    "        polygon = Poly3DCollection(faces, alpha=0.5, \n",
    "                                facecolors=rng.uniform(0,1,3),\n",
    "                                linewidths=0.5,edgecolors='black')\n",
    "        ax.add_collection3d(polygon)\n",
    "    \n",
    "    ax.set_xlim([-xlim,xlim])\n",
    "    ax.set_ylim([-ylim,ylim])\n",
    "    ax.set_zlim([-zlim,zlim])\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "#plot_voronoi_3d(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 3.3.0 NVIDIA 570.124.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<polyscope.point_cloud.PointCloud at 0x768d34108d60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.init()\n",
    "ps.register_point_cloud(\"initial_cvt_grid\",sites.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Resources/chair_low.obj with shape (128, 128, 128)\n",
      "torch.Size([16581375])\n"
     ]
    }
   ],
   "source": [
    "class SDFGrid:\n",
    "    def __init__(self, filename=\"./Resources/dolphin.obj\"):\n",
    "        self.filename = filename\n",
    "        self.sdf_grid = np.load(filename[:-4] + '.npy')\n",
    "        self.grid = torch.tensor(self.sdf_grid, device=device)\n",
    "\n",
    "    def sdf(self, sites):\n",
    "        gridsize = self.sdf_grid.shape[0]  # Assuming a cubic grid of size (N,N,N)\n",
    "        \n",
    "        # Normalize points to [0, 1] range in all dimensions\n",
    "        sites = sites + 5.0 #shift to 0-10\n",
    "        points_normalized = sites / 10.0\n",
    "\n",
    "        # Scale to grid coordinates\n",
    "        points_grid = points_normalized * (gridsize - 1)\n",
    "\n",
    "        # Separate grid coordinates into integer and fractional parts\n",
    "        x, y, z = points_grid[:, 0], points_grid[:, 1], points_grid[:, 2]\n",
    "        x0 = x.floor().long().clamp(0, gridsize - 1)\n",
    "        y0 = y.floor().long().clamp(0, gridsize - 1)\n",
    "        z0 = z.floor().long().clamp(0, gridsize - 1)\n",
    "        x1 = (x0 + 1).clamp(0, gridsize - 1)\n",
    "        y1 = (y0 + 1).clamp(0, gridsize - 1)\n",
    "        z1 = (z0 + 1).clamp(0, gridsize - 1)\n",
    "        dx, dy, dz = x - x0, y - y0, z - z0\n",
    "\n",
    "        # Perform trilinear interpolation\n",
    "        values = (\n",
    "            (1 - dx) * (1 - dy) * (1 - dz) * self.grid[x0, y0, z0] +\n",
    "            dx * (1 - dy) * (1 - dz) * self.grid[x1, y0, z0] +\n",
    "            (1 - dx) * dy * (1 - dz) * self.grid[x0, y1, z0] +\n",
    "            dx * dy * (1 - dz) * self.grid[x1, y1, z0] +\n",
    "            (1 - dx) * (1 - dy) * dz * self.grid[x0, y0, z1] +\n",
    "            dx * (1 - dy) * dz * self.grid[x1, y0, z1] +\n",
    "            (1 - dx) * dy * dz * self.grid[x0, y1, z1] +\n",
    "            dx * dy * dz * self.grid[x1, y1, z1]\n",
    "        )\n",
    "\n",
    "        return values\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.filename} with shape {self.sdf_grid.shape}\"\n",
    "    \n",
    "    \n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# class SDFGrid:\n",
    "#     def __init__(self, filename=\"./Resources/dolphin.obj\", device='cuda'):\n",
    "#         self.filename = filename\n",
    "#         self.sdf_grid = np.load(filename[:-4] + '.npy')  # Load precomputed SDF grid\n",
    "#         self.device = device\n",
    "        \n",
    "#         # Convert grid to a proper tensor shape (1, 1, D, H, W) for grid_sample\n",
    "#         self.grid = torch.tensor(self.sdf_grid, dtype=torch.float64, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#     def sdf(self, sites):\n",
    "#         gridsize = self.sdf_grid.shape[0]  # Assuming (N, N, N)\n",
    "\n",
    "#         # Normalize points from world space to [-1, 1] for grid_sample\n",
    "#         sites = sites + 5.0  # Shift to [0, 10] range\n",
    "#         points_normalized = (2.0 * (sites / 10.0)) - 1  # Normalize to [-1, 1]\n",
    "\n",
    "#         # Reshape for grid_sample (batch_size=1)\n",
    "#         points_grid = points_normalized.view(1, 1, -1, 1, 3)  # Shape: (1, 1, N, 1, 3)\n",
    "\n",
    "#         # Perform differentiable trilinear interpolation\n",
    "#         sdf_values = F.grid_sample(self.grid, points_grid, mode='bilinear', align_corners=True, padding_mode='border')\n",
    "        \n",
    "#         return sdf_values.view(-1)  # Reshape to (N,)\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return f\"{self.filename} with shape {self.sdf_grid.shape}\"\n",
    "\n",
    "\n",
    "\n",
    "model = SDFGrid(mesh[1])\n",
    "sdf_values = model.sdf(sites)\n",
    "print(model)\n",
    "print(sdf_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2097152, 3)\n"
     ]
    }
   ],
   "source": [
    "#render sdf grid as a point cloud\n",
    "#create a 128x128s128 grid\n",
    "\n",
    "x = np.linspace(-1, 1, 128)\n",
    "y = np.linspace(-1, 1, 128)\n",
    "z = np.linspace(-1, 1, 128)\n",
    "X, Y, Z = np.meshgrid(x, y, z)\n",
    "points = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T\n",
    "print(points.shape)\n",
    "\n",
    "#coordinate points with the sdf values\n",
    "sdf_points = np.zeros((points.shape[0], 4))\n",
    "sdf_points[:, :3] = points\n",
    "sdf_points[:, 3] = model.sdf_grid.ravel()\n",
    "\n",
    "\n",
    "\n",
    "# ps.register_point_cloud(\"sdf_points\", sdf_points[sdf_points[:, 3] < 0][:, :3])\n",
    "# ps.register_point_cloud(\"sdf_points_pos\", sdf_points[sdf_points[:, 3] >= 0][:, :3])\n",
    "# ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sdf_values_trilinear = model.sdf(sites)\n",
    "\n",
    "\n",
    "#use torch.nn.functionnal.grid_sample\n",
    "#df_Values_gridsample = torch.nn.functional.grid_sample(sites, torch.tensor(sdf_points, device=device), mode='bilinear', padding_mode='border')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_crossing_mesh_3d(sites, model):\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    vor = Voronoi(sites_np)  # Compute 3D Voronoi diagram\n",
    "\n",
    "    sdf_values = model.sdf(sites).detach().cpu().numpy()  # Compute SDF values\n",
    "\n",
    "    valid_faces = []  # List of polygonal faces\n",
    "    used_vertices = set()  # Set of indices for valid vertices\n",
    "\n",
    "    for (point1, point2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:\n",
    "            continue  # Skip infinite ridges\n",
    "\n",
    "        # Check if SDF changes sign across this ridge\n",
    "        if np.sign(sdf_values[point1]) != np.sign(sdf_values[point2]):\n",
    "            valid_faces.append(ridge_vertices)\n",
    "            used_vertices.update(ridge_vertices)\n",
    "\n",
    "    # **Filter Voronoi vertices**\n",
    "    used_vertices = sorted(used_vertices)  # Keep unique, sorted indices\n",
    "    vertex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(used_vertices)}\n",
    "    filtered_vertices = vor.vertices[used_vertices]\n",
    "\n",
    "    # **Re-index faces to match the new filtered vertex list**\n",
    "    filtered_faces = [[vertex_map[v] for v in face] for face in valid_faces]\n",
    "\n",
    "    return filtered_vertices, filtered_faces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_cloud = ps.register_point_cloud(\"sites_w_sdf\", sites.detach().cpu().numpy())\n",
    "# ps_cloud.add_scalar_quantity(\"sdf_values_trilinear\", sdf_values_trilinear.detach().cpu().numpy(), enabled=True)\n",
    "# # ps_cloud.add_scalar_quantity(\"sdf_Values_gridsample\", sdf_Values_gridsample.detach().cpu().numpy(), enabled=True)\n",
    "\n",
    "\n",
    "# ps.register_point_cloud(\"sites_w_zeroes_sdf_trilinear\", sites.detach().cpu().numpy()[sdf_values_trilinear.detach().cpu().numpy()[:] <= 0])\n",
    "# # ps.register_point_cloud(\"sites_w_zeroes_sdf_gridsample\", sites.detach().cpu().numpy()[sdf_Values_gridsample.detach().cpu().numpy()[:] <= 0])\n",
    "\n",
    "# initial_mesh = get_zero_crossing_mesh_3d(sites, model)\n",
    "\n",
    "# ps.register_surface_mesh(\"initial_mesh Zero-Crossing faces\", initial_mesh[0], initial_mesh[1])\n",
    "# ps.register_point_cloud(\"initial_mesh vertices\", initial_mesh[0])\n",
    "\n",
    "#ps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_vectorized(sites, model):\n",
    "    sdf_values = model.sdf(sites)\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    # Compute Voronoi diagram\n",
    "    vor = Voronoi(sites_np)\n",
    "    \n",
    "    neighbors = torch.tensor(np.array(vor.ridge_points), device=device)\n",
    "    \n",
    "    # Extract the SDF values for each site in the pair\n",
    "    sdf_i = sdf_values[neighbors[:, 0]]  # First site in each pair\n",
    "    sdf_j = sdf_values[neighbors[:, 1]]  # Second site in each pair\n",
    "    # Find the indices where SDF values have opposing signs or one is zero\n",
    "    mask_zero_crossing_sites = (sdf_i * sdf_j <= 0).squeeze()\n",
    "    sites_to_upsample = torch.unique(neighbors[mask_zero_crossing_sites].view(-1))\n",
    "    \n",
    "    print(\"Sites to upsample \",sites_to_upsample.shape)\n",
    "    \n",
    "    tet_centroids = sites[sites_to_upsample]\n",
    "\n",
    "    # Tetrahedron relative positions (unit tetrahedron)\n",
    "    basic_tet_1 = torch.tensor([[1, 1, 1]], device=device, dtype=torch.float64)\n",
    "    basic_tet_1 = basic_tet_1.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_2 = torch.tensor([-1, -1, 1], device=device, dtype=torch.float64)    \n",
    "    basic_tet_2 = basic_tet_2.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_3 = torch.tensor([-1, 1, -1], device=device, dtype=torch.float64)    \n",
    "    basic_tet_3 = basic_tet_3.repeat(len(tet_centroids), 1)\n",
    "    basic_tet_4 = torch.tensor([1, -1, -1], device=device, dtype=torch.float64)\n",
    "    basic_tet_4 = basic_tet_4.repeat(len(tet_centroids), 1)\n",
    "\n",
    "\n",
    "    #compute scale based on cell volume\n",
    "    centroids = torch.tensor(np.array([vor.vertices[vor.regions[vor.point_region[i]]].mean(axis=0) for i in range(len(sites_np))]), device=device)\n",
    "    #centroids = torch.tensor(np.array(centroids), device=sites.device, dtype=sites.dtype)\n",
    "    cells_vertices = [vor.vertices[vor.regions[vor.point_region[i]]] for i in range(len(sites_np))]\n",
    "\n",
    "    #compute the distance between each centroid  and each vertex in cells_vertices row\n",
    "    distances = []\n",
    "    for i in range(len(cells_vertices)):\n",
    "        min_dist = 100000000000\n",
    "        for j in range(len(cells_vertices[i])):\n",
    "            dist = torch.norm(centroids[i] - torch.tensor(cells_vertices[i][j], device=device), p=2)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        distances.append(min_dist)\n",
    "    distances = torch.tensor(distances, device=device)\n",
    " \n",
    "    \n",
    "    scale = distances[sites_to_upsample] / 2\n",
    "    \n",
    "    scale = scale.unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    new_sites = torch.cat((tet_centroids + basic_tet_1 * scale, tet_centroids + basic_tet_2 * scale, tet_centroids + basic_tet_3 * scale, tet_centroids + basic_tet_4 * scale), dim=0)\n",
    "\n",
    "    updated_sites = torch.cat((sites, new_sites), dim=0)\n",
    "\n",
    "    return updated_sites\n",
    "                \n",
    "def compute_zero_crossing_vertices_3d(sites, model):\n",
    "    \"\"\"\n",
    "    Computes the indices of the sites composing vertices where neighboring sites have opposite or zero SDF values.\n",
    "\n",
    "    Args:\n",
    "        sites (torch.Tensor): (N, D) tensor of site positions.\n",
    "        model (callable): Function or neural network that computes SDF values.\n",
    "\n",
    "    Returns:\n",
    "        zero_crossing_vertices_index (list of triplets): List of sites indices (si, sj, sk) where atleast 2 sites have opposing SDF signs.\n",
    "    \"\"\"\n",
    "    # Compute Delaunay neighbors\n",
    "    # Detach and convert to NumPy for Delaunay triangulation\n",
    "    points_np = sites.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute the Delaunay tessellation\n",
    "    tri = Delaunay(points_np)\n",
    "    vor = Voronoi(points_np)\n",
    "    \n",
    "    # Compute SDF values for all sites\n",
    "    sdf_values = model.sdf(sites)  # Assuming model outputs (N, 1) or (N,) tensor\n",
    "\n",
    "    neighbors = torch.tensor(np.array(vor.ridge_points), device=device)\n",
    "    all_tetrahedra = torch.tensor(np.array(tri.simplices), device=device)\n",
    "    #all_ridge_vertices = torch.tensor(np.array(vor.ridge_vertices), device=device)\n",
    "    \n",
    "    # Extract the SDF values for each site in the pair\n",
    "    sdf_i = sdf_values[neighbors[:, 0]]  # First site in each pair\n",
    "    sdf_j = sdf_values[neighbors[:, 1]]  # Second site in each pair\n",
    "    # Find the indices where SDF values have opposing signs or one is zero\n",
    "    mask_zero_crossing_sites = (sdf_i * sdf_j <= 0).squeeze()\n",
    "    zero_crossing_pairs = neighbors[mask_zero_crossing_sites]\n",
    "    #ridge_vertices_pairs = all_ridge_vertices[mask_zero_crossing_sites]\n",
    "    #compute ridge vertices neighbors\n",
    "    \n",
    "\n",
    "    # Check if vertices has a pair of zero crossing sites\n",
    "    sdf_0 = sdf_values[all_tetrahedra[:, 0]]  # First site in each pair\n",
    "    sdf_1 = sdf_values[all_tetrahedra[:, 1]]  # Second site in each pair\n",
    "    sdf_2 = sdf_values[all_tetrahedra[:, 2]]  # Third site in each pair\n",
    "    sdf_3 = sdf_values[all_tetrahedra[:, 3]]  # Fourth site in each pair\n",
    "    mask_zero_crossing_faces = (sdf_0*sdf_1<=0).squeeze() | (sdf_0*sdf_2<=0).squeeze() | (sdf_0*sdf_3<=0).squeeze() | (sdf_1*sdf_2<=0).squeeze() | (sdf_1*sdf_3<=0).squeeze() | (sdf_2*sdf_3<=0).squeeze()\n",
    "    zero_crossing_vertices_index = all_tetrahedra[mask_zero_crossing_faces]\n",
    "    return zero_crossing_vertices_index, zero_crossing_pairs\n",
    "\n",
    "def compute_cvt_loss_vectorized(sites):\n",
    "    # Convert sites to NumPy for Voronoi computation\n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    vor = Voronoi(sites_np)\n",
    "        \n",
    "    #Todo C++ loop for this\n",
    "    # create a nested list of vertices for each site\n",
    "    centroids = [vor.vertices[vor.regions[vor.point_region[i]]].mean(axis=0) for i in range(len(sites_np)) if vor.regions[vor.point_region[i]] and -1 not in vor.regions[vor.point_region[i]]]\n",
    "    centroids = torch.tensor(np.array(centroids), device=sites.device, dtype=sites.dtype)\n",
    "    valid_indices = torch.tensor([i for i in range(len(sites_np)) if vor.regions[vor.point_region[i]] and -1 not in vor.regions[vor.point_region[i]]], device=sites.device)\n",
    "    \n",
    "    valid_sites = sites[valid_indices]\n",
    "    \n",
    "    penalties = torch.where(abs(valid_sites - centroids) < 10, valid_sites - centroids, torch.tensor(0.0, device=sites.device))\n",
    "    \n",
    "    cvt_loss = torch.mean(penalties**2)\n",
    "    \n",
    "    return cvt_loss\n",
    "\n",
    "# def mean_curvature_loss(vertices, adjacency_list):\n",
    "#     \"\"\"\n",
    "#     Computes the mean curvature loss for a given set of vertices and their adjacency list.\n",
    "\n",
    "#     Args:\n",
    "#         vertices (torch.Tensor): Tensor of shape (N, 3), where N is the number of vertices.\n",
    "#         adjacency_list (list of lists): adjacency_list[i] contains indices of neighbors of vertex i.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Scalar loss value encouraging smoother geometry.\n",
    "#     \"\"\"\n",
    "#     device = vertices.device\n",
    "#     loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "#     for i, neighbors in enumerate(adjacency_list):\n",
    "#         if len(neighbors) == 0:\n",
    "#             continue  # Skip isolated points\n",
    "        \n",
    "#         # Compute the mean of the neighboring vertices\n",
    "#         neighbor_vertices = vertices[neighbors]  # Shape (num_neighbors, 3)\n",
    "#         mean_neighbor = neighbor_vertices.mean(dim=0)  # Shape (3,)\n",
    "\n",
    "#         # Mean curvature flow loss (squared distance to neighborhood mean)\n",
    "#         loss += torch.norm(vertices[i] - mean_neighbor, p=2) ** 2\n",
    "\n",
    "#     return loss / len(adjacency_list)  # Normalize by number of vertices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_loss_values = []\n",
    "min_distance_loss_values = []\n",
    "edge_smoothing_loss_values = []\n",
    "chamfer_distance_loss_values = []\n",
    "eikonal_loss_values = []\n",
    "domain_restriction_loss_values = []\n",
    "zero_target_points_loss_values = []\n",
    "sdf_loss_values = []\n",
    "loss_values = []\n",
    "\n",
    "def autograd(sites, model, max_iter=100, stop_train_threshold=1e-6, upsampling=0, lambda_weights = [0.1,1.0,0.1,0.1,1.0,1.0,0.1]):\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': [sites], 'lr': lr_sites}\n",
    "], betas=(0.5, 0.999))\n",
    "\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_loss = float(\"inf\")\n",
    "    upsampled = 0\n",
    "    epoch = 0\n",
    "    lambda_cvt = lambda_weights[0]\n",
    "    lambda_sdf = lambda_weights[1]\n",
    "    lambda_min_distance = lambda_weights[2]\n",
    "    lambda_laplace = lambda_weights[3]\n",
    "    lamda_chamfer = lambda_weights[4]\n",
    "    lamda_eikonal = lambda_weights[5]\n",
    "    lambda_domain_restriction = lambda_weights[6]\n",
    "    lambda_target_points = lambda_weights[7]\n",
    "    \n",
    "    best_sites = sites.clone()\n",
    "    best_sites.best_loss = best_loss\n",
    "    \n",
    "    while epoch <= max_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        vertices_to_compute, bisectors_to_compute = compute_zero_crossing_vertices_3d(sites, model)\n",
    "        vertices = su.compute_vertices_3d_vectorized(sites, vertices_to_compute)    \n",
    "        bisectors = su.compute_all_bisectors_vectorized(sites, bisectors_to_compute)\n",
    "        #combine vertices and bisectors to one tensor for chamfer\n",
    "        points = torch.cat((vertices, bisectors), 0)\n",
    "\n",
    "\n",
    "        # Compute losses       \n",
    "        cvt_loss = compute_cvt_loss_vectorized(sites)\n",
    "\n",
    "        sdf_loss = torch.mean(model.sdf(points)**2)        \n",
    "        \n",
    "        laplacian_loss = lf.compute_ridge_smoothing_loss(bisectors_to_compute, sites, model)\n",
    "        #laplacian_loss = mean_curvature_loss(ridge_vertices_pairs, sites)\n",
    "\n",
    "        # Track raw losses (unweighted)\n",
    "        cvt_loss_values.append(cvt_loss.item())\n",
    "        sdf_loss_values.append(sdf_loss.item())\n",
    "        edge_smoothing_loss_values.append(laplacian_loss.item())\n",
    "  \n",
    "        loss = (\n",
    "            lambda_cvt * cvt_loss +\n",
    "            lambda_sdf * sdf_loss +\n",
    "            lambda_laplace * laplacian_loss\n",
    "        )\n",
    "        loss_values.append(loss.item())\n",
    "        print(f\"cvt_loss: {cvt_loss}, laplace_loss: {laplacian_loss}, \")\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_sites = sites.clone()\n",
    "            best_sites.best_loss = best_loss\n",
    "            if upsampled > 0:\n",
    "                print(f\"UPSAMPLED {upsampled} Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "                #return best_sites\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < stop_train_threshold:\n",
    "            print(f\"Converged at epoch {epoch} with loss {loss.item()}\")\n",
    "            #break\n",
    "        \n",
    "        prev_loss = loss.item() \n",
    "        \n",
    "        if epoch/max_iter > (0.15)*(upsampled+1)/(upsampling+1) and upsampled < upsampling:\n",
    "            print(\"sites length BEFORE UPSAMPLING: \",len(sites))\n",
    "            \n",
    "            #new_sites = su.upsampling_inside(best_sites, model)\n",
    "            #new_sites = su.adaptive_density_upsampling(best_sites, model)\n",
    "            \n",
    "            #sites = su.add_upsampled_sites(best_sites, new_sites)\n",
    "            \n",
    "            sites = upsampling_vectorized(sites, model)\n",
    "            \n",
    "            sites = sites.detach().requires_grad_(True)\n",
    "            #print(\"upsampled sites length: \",len(sites))\n",
    "            \n",
    "            #best_sites = sites.clone()\n",
    "            #best_sites.best_loss = best_loss\n",
    "            \n",
    "            optimizer = torch.optim.Adam([#{'params': [p for _, p in model.named_parameters()], 'lr': lr_model},\n",
    "                                          {'params': [sites], 'lr': lr_sites}])\n",
    "            upsampled += 1.0\n",
    "            print(\"sites length AFTER: \",len(sites))\n",
    "            \n",
    "          \n",
    "        if epoch % (max_iter/10) == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "            print(f\"Best Epoch {best_epoch}: Best loss = {best_loss}\")\n",
    "            #save model and sites\n",
    "            site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}.pth'\n",
    "            torch.save(sites, site_file_path)\n",
    "        \n",
    "        epoch += 1           \n",
    "        \n",
    "    return best_sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_weights = [0.001,1,0,0,0,0,0,0]\n",
    "\n",
    "lambda_cvt = lambda_weights[0]\n",
    "lambda_sdf = lambda_weights[1]\n",
    "lambda_min_distance = lambda_weights[2]\n",
    "lambda_laplace = lambda_weights[3]\n",
    "lamda_chamfer = lambda_weights[4]\n",
    "lambda_eikonal = lambda_weights[5]\n",
    "lambda_domain_restriction = lambda_weights[6]\n",
    "lambda_target_points = lambda_weights[7]\n",
    "max_iter = 100\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}3d_sites_{num_centroids}.npy'\n",
    "#check if optimized sites file exists\n",
    "if os.path.exists(site_file_path):\n",
    "    #import sites\n",
    "    print(\"Importing sites\")\n",
    "    sites = np.load(site_file_path)\n",
    "    sites = torch.from_numpy(sites).to(device).requires_grad_(True)    \n",
    "else:\n",
    "    import cProfile, pstats\n",
    "    import time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    sites = autograd(sites, model, max_iter=max_iter, upsampling=6, lambda_weights=lambda_weights)\n",
    "    \n",
    "    profiler.disable()\n",
    "    stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "    stats.print_stats()\n",
    "    stats.dump_stats(f'{destination}{mesh[0]}{max_iter}_3d_profile_{num_centroids}.prof')\n",
    "    \n",
    "    \n",
    "    sites_np = sites.detach().cpu().numpy()\n",
    "    np.save(site_file_path, sites_np)\n",
    "    \n",
    "\n",
    "print(\"Sites length: \", len(sites))\n",
    "print(\"min sites: \", torch.min(sites))\n",
    "print(\"max sites: \", torch.max(sites))\n",
    "ps_cloud = ps.register_point_cloud(\"best_optimized_cvt_grid\",sites.detach().cpu().numpy())\n",
    "    \n",
    "lim=torch.abs(torch.max(sites)).detach().cpu().numpy()*1.1\n",
    "#plot_voronoi_3d(sites,lim,lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites ./images/autograd/3D/TrueSDF/chair100_20_3d_sites_3375.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<polyscope.point_cloud.PointCloud at 0x7b25166ca850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "site_file_path = f'{destination}{mesh[0]}{max_iter}_{epoch}_3d_sites_{num_centroids}.pth'\n",
    "sites = torch.load(site_file_path)\n",
    "sites_np = sites.detach().cpu().numpy()\n",
    "#\n",
    "print(\"sites\", site_file_path)\n",
    "sdf_values_trilinear = model.sdf(sites)\n",
    "\n",
    "ps.register_point_cloud(\"final_sites_w_zeroes_sdf_trilinear\", sites.detach().cpu().numpy()[sdf_values_trilinear.detach().cpu().numpy()[:] <= 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tri = Delaunay(sites_np)\n",
    "# delaunay_vertices =torch.tensor(np.array(tri.simplices), device=device)\n",
    "# sdf_values = model.sdf(sites)\n",
    "\n",
    "\n",
    "# # Assuming sites is a PyTorch tensor of shape [M, 3]\n",
    "# sites = sites.unsqueeze(0)  # Now shape [1, M, 3]\n",
    "\n",
    "# # Assuming SDF_Values is a PyTorch tensor of shape [M]\n",
    "# sdf_values = sdf_values.unsqueeze(0)  # Now shape [1, M]\n",
    "\n",
    "#marching_tetrehedra_mesh = kaolin.ops.conversions.marching_tetrahedra(sites, delaunay_vertices, sdf_values, return_tet_idx=False)\n",
    "#print(marching_tetrehedra_mesh)\n",
    "# vertices_list, faces_list = marching_tetrehedra_mesh\n",
    "# vertices = vertices_list[0]\n",
    "# faces = faces_list[0]\n",
    "# vertices_np = vertices.detach().cpu().numpy()  # Shape [N, 3]\n",
    "# faces_np = faces.detach().cpu().numpy()  # Shape [M, 3] (triangles)\n",
    "# ps.register_surface_mesh(\"Marching Tetrahedra Mesh\", vertices_np, faces_np)\n",
    "\n",
    "\n",
    "zero_crossing_final_mesh = get_zero_crossing_mesh_3d(sites, model)\n",
    "ps.register_surface_mesh(\"Zero-Crossing faces\", zero_crossing_final_mesh[0], zero_crossing_final_mesh[1])\n",
    "ps.register_point_cloud(\"Mesh vertices\", zero_crossing_final_mesh[0])\n",
    "\n",
    "ps.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
